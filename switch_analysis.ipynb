{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bd047b6c-8c0c-4311-bde7-d2a5ad31fdcf","cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# ---------------------------------------\n# Base directories\n# ---------------------------------------\nOUTDIR_BASE = \"mdn_70_10_20_optimized\"\nENSEMBLE_DIR = os.path.join(OUTDIR_BASE, \"ensembles_fast\")\n\n# ---------------------------------------\n# Load reaction splits\n# ---------------------------------------\ntrain_reacts = pd.read_csv(f\"{OUTDIR_BASE}/train_reactions.csv\")[\"Reaction\"].values\nval_reacts   = pd.read_csv(f\"{OUTDIR_BASE}/val_reactions.csv\")[\"Reaction\"].values\ntest_reacts  = pd.read_csv(f\"{OUTDIR_BASE}/test_reactions.csv\")[\"Reaction\"].values\n\nprint(\"Train reactions:\", len(train_reacts))\nprint(\"Val reactions:\", len(val_reacts))\nprint(\"Test reactions:\", len(test_reacts))\n\n# ---------------------------------------\n# Load original dataframe\n# ---------------------------------------\nDRIVE_URL = \"https://drive.google.com/uc?id=1PS0eB8dx8VMzVvxNUc6wBzsMRkEKJjWI\"\ndf = pd.read_csv(DRIVE_URL)\n# ---------------------------------------\n# Compute Coulomb barrier height V_B\n# ---------------------------------------\n\n# Get one row per reaction\nbarrier_df = df.groupby(\"Reaction\").first().reset_index()\n\n# Compute Z1Z2\nbarrier_df[\"Z1Z2\"] = barrier_df[\"Z1\"] * barrier_df[\"Z2\"]\n\n# Compute Coulomb barrier height\nbarrier_df[\"V_B\"] = (barrier_df[\"Z1Z2\"] * 1.44) / barrier_df[\"R B\"]\n\n# Keep only needed columns\nbarrier_df = barrier_df[[\"Reaction\", \"V_B\"]]\n\n# Merge back into main dataframe\ndf = df.merge(barrier_df, on=\"Reaction\", how=\"left\")\n\nprint(\"Barrier heights computed and merged.\")\nprint(barrier_df[[\"Reaction\",\"V_B\"]].head(5))\nprint(\"Total reactions in dataset:\", df[\"Reaction\"].nunique())\n\n# ---------------- Physics feature engineering ----------------\nM_p = 938.272088; M_n = 939.565420; epsilon=1e-30; LN10=np.log(10.0)\n\ndef get_nucleon_mass(Z,A): return Z*M_p + (A-Z)*M_n\n\nmass1 = df.apply(lambda r: get_nucleon_mass(r[\"Z1\"], r[\"A1\"]), axis=1).values\nmass2 = df.apply(lambda r: get_nucleon_mass(r[\"Z2\"], r[\"A2\"]), axis=1).values\n\nmu_MeVc2 = (mass1 * mass2) / (mass1 + mass2 + 1e-12)\nEcm = df[\"E c.m.\"].astype(float).values\nv_over_c = np.sqrt(np.clip(2*Ecm/(mu_MeVc2+epsilon),0,np.inf))\ne2_hbar_c = 1/137.035999\n\ndf[\"eta\"] = (df[\"Z1\"]*df[\"Z2\"]) / (e2_hbar_c*(v_over_c+1e-16))\n\nlog10_sigma_exp = np.log10(np.clip(df[\"σ\"],1e-30,np.inf))\nlog10_sigma_cal = np.log10(np.clip(df[\"σ cal\"],1e-30,np.inf))\nlog10_Ecm = np.log10(np.clip(df[\"E c.m.\"],1e-30,np.inf))\n\nlog10_exp_term = (2*np.pi*df[\"eta\"])/LN10\n\ndf[\"log10_S_exp\"] = log10_sigma_exp + log10_Ecm + log10_exp_term\ndf[\"log10_S_cal\"] = log10_sigma_cal + log10_Ecm + log10_exp_term\ndf[\"delta_log10_S\"] = df[\"log10_S_exp\"] - df[\"log10_S_cal\"]\n\ndf[\"N1\"] = df[\"A1\"] - df[\"Z1\"]\ndf[\"N2\"] = df[\"A2\"] - df[\"Z2\"]\ndf[\"Z1Z2_over_Ecm\"] = (df[\"Z1\"]*df[\"Z2\"]) / (df[\"E c.m.\"] + epsilon)\n\nMAGIC = np.array([2,8,20,28,50,82,126])\ndef magic_dist(arr): return np.min(np.abs(arr[:,None] - MAGIC[None,:]),axis=1)\n\ndf[\"magic_dist_Z1\"] = magic_dist(df[\"Z1\"].values)\ndf[\"magic_dist_N1\"] = magic_dist(df[\"N1\"].values)\ndf[\"magic_dist_Z2\"] = magic_dist(df[\"Z2\"].values)\ndf[\"magic_dist_N2\"] = magic_dist(df[\"N2\"].values)\n\n# ---------------- 29 training features ----------------\nfeatures_train = [\n    'E c.m.', 'Z1', 'N1', 'A1',\n    'Z2', 'N2', 'A2', 'Q ( 2 n )',\n    'Z1Z2_over_Ecm',\n    'magic_dist_Z1','magic_dist_N1','magic_dist_Z2','magic_dist_N2',\n    'Z3','N3','A3','β P','β T','R B','ħ ω',\n    'Projectile_Mass_Actual', 'Target_Mass_Actual', 'Compound_Nucleus_Mass_Actual',\n    'Compound_Nucleus_Sp','Compound_Nucleus_Sn',\n    'Projectile_Binding_Energy','Target_Binding_Energy',\n    'Compound_Nucleus_Binding_Energy','Compound_Nucleus_S2n'\n]\n\n\n# ---------------------------------------\n# Identify all seed folders\n# ---------------------------------------\nseed_dirs = sorted([\n    os.path.join(ENSEMBLE_DIR, d)\n    for d in os.listdir(ENSEMBLE_DIR)\n    if d.startswith(\"seed_\")\n])\n\nprint(\"Number of seeds found:\", len(seed_dirs))\n\n# ---------------------------------------\n# Load MDN full-dataset component outputs\n# ---------------------------------------\nall_seed_components = []\n\nfor seed_path in seed_dirs:\n    npz_path = os.path.join(seed_path, \"mdn_all_components.npz\")\n    \n    if not os.path.exists(npz_path):\n        print(f\"WARNING: Missing {npz_path}\")\n        continue\n    \n    data = np.load(npz_path)\n    \n    pi_all    = data[\"pi\"]      # shape: (N_rows, N_components)\n    mu_all    = data[\"mu\"]\n    sigma_all = data[\"sigma\"]\n    \n    all_seed_components.append({\n        \"seed_path\": seed_path,\n        \"pi\": pi_all,\n        \"mu\": mu_all,\n        \"sigma\": sigma_all\n    })\n\nprint(\"Loaded seeds:\", len(all_seed_components))\n\n# ---------------------------------------\n# Basic consistency check\n# ---------------------------------------\nN_rows = len(df)\n\nfor s in all_seed_components:\n    assert s[\"pi\"].shape[0] == N_rows, \"Mismatch between MDN output and dataframe rows\"\n\nprint(\"All seeds consistent with dataframe rows.\")\n\n# ---------------------------------------\n# Add dataset split label to df\n# ---------------------------------------\ndf[\"set\"] = np.select(\n    [\n        df[\"Reaction\"].isin(test_reacts),\n        df[\"Reaction\"].isin(val_reacts)\n    ],\n    [\"test\", \"val\"],\n    default=\"train\"\n)\n\nprint(df[\"set\"].value_counts())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train reactions: 149\nVal reactions: 21\nTest reactions: 43\nBarrier heights computed and merged.\n        Reaction        V_B\n0  12 C + 144 Sm  48.259459\n1  12 C + 152 Sm  43.728980\n2  12 C + 154 Sm  43.200000\n3  12 C + 181 Ta  49.584906\n4  12 C + 194 Pt  57.797599\nTotal reactions in dataset: 213\nNumber of seeds found: 10\nLoaded seeds: 10\nAll seeds consistent with dataframe rows.\nset\ntrain    2493\ntest      685\nval       354\nName: count, dtype: int64\n"}],"execution_count":27},{"id":"8ee82301-12df-45dd-9d4e-847ffadca092","cell_type":"code","source":"# ============================\n# SECTION 2 — ROBUST SWITCH COMPUTATION\n# ============================\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------------------------------\n# 2.1  Helper — compute switch for ONE seed\n# ----------------------------------------------------\n\ndef compute_switch_per_seed(df, pi_array):\n    \"\"\"\n    Compute switch energy and x_switch for one seed.\n    \n    Switch definition:\n        First energy where dominant MDN component changes.\n    \n    Returns:\n        dict: reaction -> (E_switch, x_switch)\n    \"\"\"\n    \n    df_temp = df.copy().reset_index(drop=True)\n    df_temp[\"dominant\"] = np.argmax(pi_array, axis=1)\n    \n    switch_dict = {}\n    \n    for reaction, sub in df_temp.groupby(\"Reaction\"):\n        \n        sub = sub.sort_values(\"E c.m.\").reset_index(drop=True)\n        \n        dom = sub[\"dominant\"].values\n        E_vals = sub[\"E c.m.\"].values\n        \n        if len(dom) < 2:\n            continue\n        \n        switch_energy = np.nan\n        \n        # first regime change\n        for i in range(1, len(dom)):\n            if dom[i] != dom[i-1]:\n                switch_energy = E_vals[i]\n                break\n        \n        # only store if switch exists\n        if not np.isnan(switch_energy):\n            \n            V_B = sub[\"V_B\"].iloc[0]\n            x_switch = switch_energy / V_B\n            \n            switch_dict[reaction] = (switch_energy, x_switch)\n    \n    return switch_dict\n\n\n# ----------------------------------------------------\n# 2.2  Compute switch for ALL seeds\n# ----------------------------------------------------\n\nseed_switch_results = []\n\nfor seed_data in all_seed_components:\n    \n    pi_all = seed_data[\"pi\"]\n    \n    switch_dict = compute_switch_per_seed(df, pi_all)\n    seed_switch_results.append(switch_dict)\n\nprint(\"Switch computed for\", len(seed_switch_results), \"seeds.\")\n\n\n# ----------------------------------------------------\n# 2.3  Aggregate across seeds\n# ----------------------------------------------------\n\nall_reactions = df[\"Reaction\"].unique()\n\nswitch_records = []\n\nfor reaction in all_reactions:\n    \n    E_list = []\n    x_list = []\n    \n    for seed_dict in seed_switch_results:\n        if reaction in seed_dict:\n            E_val, x_val = seed_dict[reaction]\n            E_list.append(E_val)\n            x_list.append(x_val)\n    \n    n_valid = len(x_list)\n    \n    if n_valid > 0:\n        switch_records.append({\n            \"Reaction\": reaction,\n            \"E_switch_mean\": np.mean(E_list),\n            \"E_switch_std\": np.std(E_list),\n            \"x_switch_mean\": np.mean(x_list),\n            \"x_switch_std\": np.std(x_list),\n            \"n_seeds_valid\": n_valid,\n            \"seed_fraction\": n_valid / len(seed_switch_results)\n        })\n\nswitch_df = pd.DataFrame(switch_records)\n\n\n# ----------------------------------------------------\n# 2.4  Attach dataset split label\n# ----------------------------------------------------\n\nswitch_df[\"set\"] = np.select(\n    [\n        switch_df[\"Reaction\"].isin(test_reacts),\n        switch_df[\"Reaction\"].isin(val_reacts)\n    ],\n    [\"test\", \"val\"],\n    default=\"train\"\n)\n\n\n# ----------------------------------------------------\n# 2.5  Reliability classification\n# ----------------------------------------------------\n\n# Define reliable as switch detected in >= 80% of seeds\nswitch_df[\"reliable\"] = switch_df[\"seed_fraction\"] >= 0.8\n\nprint(\"Total reactions:\", len(switch_df))\nprint(\"Reliable reactions:\", switch_df[\"reliable\"].sum())\n\n\n# ----------------------------------------------------\n# 2.6  Clean dataset for physics analysis\n# ----------------------------------------------------\n\nswitch_df_clean = switch_df[switch_df[\"reliable\"]].copy()\n\nprint(\"\\nSwitch dataframe created.\")\nprint(switch_df_clean.head())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Switch computed for 10 seeds.\nTotal reactions: 208\nReliable reactions: 137\n\nSwitch dataframe created.\n        Reaction  E_switch_mean  E_switch_std  x_switch_mean  x_switch_std  \\\n0    12 C + 89 Y        29.1829      0.861492       0.911966      0.026922   \n1   12 C + 92 Zr        29.7120      0.567852       0.915604      0.017499   \n2  12 C + 144 Sm        44.6787      1.066112       0.925802      0.022091   \n6  12 C + 194 Pt        54.3860      1.058643       0.940973      0.018316   \n7  12 C + 198 Pt        53.8820      0.732022       0.936251      0.012720   \n\n   n_seeds_valid  seed_fraction    set  reliable  \n0             10            1.0  train      True  \n1             10            1.0   test      True  \n2             10            1.0  train      True  \n6             10            1.0  train      True  \n7             10            1.0    val      True  \n"}],"execution_count":10},{"id":"a08951a4-38a5-4396-84a7-35e010b321af","cell_type":"code","source":"# ==========================================================\n# OPTIONAL SECTION — SINGLE SEED DIAGNOSTIC ANALYSIS\n# ==========================================================\n\nimport numpy as np\nimport pandas as pd\nimport os\n\n# -----------------------------------------\n# Choose seed for diagnostic\n# -----------------------------------------\n\nSEED_ID = 42\nseed_dir = f\"mdn_70_10_20_optimized/ensembles_fast/seed_{SEED_ID}\"\n\ndata = np.load(os.path.join(seed_dir, \"mdn_all_components.npz\"))\npi_all = data[\"pi\"]\n\nprint(\"Loaded seed:\", SEED_ID)\nprint(\"pi shape:\", pi_all.shape)\n\n# -----------------------------------------\n# Prepare dataframe\n# -----------------------------------------\n\ndf_single = df.copy().reset_index(drop=True)\ndf_single[\"dominant\"] = np.argmax(pi_all, axis=1)\n\n# -----------------------------------------\n# Compute switch per reaction (single seed)\n# -----------------------------------------\n\nsingle_switch_records = []\n\nfor reaction, sub in df_single.groupby(\"Reaction\"):\n    \n    sub = sub.sort_values(\"E c.m.\").reset_index(drop=True)\n    \n    dom = sub[\"dominant\"].values\n    E_vals = sub[\"E c.m.\"].values\n    \n    switch_energy = np.nan\n    \n    for i in range(1, len(dom)):\n        if dom[i] != dom[i-1]:\n            switch_energy = E_vals[i]\n            break\n    \n    if not np.isnan(switch_energy):\n        \n        V_B = sub[\"V_B\"].iloc[0]\n        \n        single_switch_records.append({\n            \"Reaction\": reaction,\n            \"E_switch\": switch_energy,\n            \"V_B\": V_B,\n            \"x_switch\": switch_energy / V_B\n        })\n\nsingle_seed_df = pd.DataFrame(single_switch_records)\n\nprint(\"\\nSingle seed switch summary:\")\nprint(single_seed_df[\"x_switch\"].describe())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Loaded seed: 42\npi shape: (3532, 5)\n\nSingle seed switch summary:\ncount    180.000000\nmean       0.927565\nstd        0.052496\nmin        0.837655\n25%        0.887410\n50%        0.917484\n75%        0.947817\nmax        1.110090\nName: x_switch, dtype: float64\n"}],"execution_count":12},{"id":"4e35c8ac-7f56-40f8-8474-ce62e6a68f28","cell_type":"code","source":"# Merge structural features\nbarrier_df_local = df.groupby(\"Reaction\").first().reset_index()\n\nsingle_seed_df = single_seed_df.merge(\n    barrier_df_local[[\n        \"Reaction\",\n        \"Q ( 2 n )\",\n        \"β P\",\n        \"β T\"\n    ]],\n    on=\"Reaction\",\n    how=\"left\"\n)\n\nsingle_seed_df[\"beta_eff\"] = abs(single_seed_df[\"β P\"]) + abs(single_seed_df[\"β T\"])\n\nprint(\"\\nCorrelation (single seed):\")\nprint(single_seed_df[[\"x_switch\", \"Q ( 2 n )\", \"beta_eff\"]].corr())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nCorrelation (single seed):\n           x_switch  Q ( 2 n )  beta_eff\nx_switch   1.000000  -0.325202  0.653720\nQ ( 2 n ) -0.325202   1.000000 -0.074978\nbeta_eff   0.653720  -0.074978  1.000000\n"}],"execution_count":13},{"id":"f7cdf81a-464f-442e-8d32-c42eefb239f0","cell_type":"code","source":"# ============================================\n# SECTION 3A — GLOBAL STATISTICS (RELIABLE)\n# ============================================\n\nswitch_clean = switch_df[switch_df[\"reliable\"] == True].copy()\n\nprint(\"Number of reliable reactions:\", len(switch_clean))\n\nmean_x = switch_clean[\"x_switch_mean\"].mean()\nstd_x  = switch_clean[\"x_switch_mean\"].std()\n\nprint(\"\\nGlobal x_switch mean:\", round(mean_x, 4))\nprint(\"Global x_switch std across reactions:\", round(std_x, 4))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Number of reliable reactions: 137\n\nGlobal x_switch mean: 0.9132\nGlobal x_switch std across reactions: 0.0367\n"}],"execution_count":15},{"id":"bc80a6dd-0d1c-4a1d-9f65-3cc534861524","cell_type":"code","source":"# ============================================\n# SECTION 3B — BOOTSTRAP CONFIDENCE INTERVAL\n# ============================================\n\nimport numpy as np\n\nN_BOOT = 2000\nboot_means = []\n\nvalues = switch_clean[\"x_switch_mean\"].values\n\nfor _ in range(N_BOOT):\n    sample = np.random.choice(values, size=len(values), replace=True)\n    boot_means.append(np.mean(sample))\n\nboot_means = np.array(boot_means)\n\nci_low  = np.percentile(boot_means, 2.5)\nci_high = np.percentile(boot_means, 97.5)\n\nprint(\"\\nBootstrap 95% CI:\")\nprint(\"Lower:\", round(ci_low, 4))\nprint(\"Upper:\", round(ci_high, 4))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nBootstrap 95% CI:\nLower: 0.9074\nUpper: 0.9194\n"}],"execution_count":16},{"id":"434f486c-c203-45d9-b55c-ef12485bc711","cell_type":"code","source":"# ============================================\n# SECTION 3C — SPLIT STABILITY\n# ============================================\n\nprint(\"\\nTrain/Val/Test statistics:\")\nprint(\n    switch_clean.groupby(\"set\")[\"x_switch_mean\"]\n    .agg([\"count\", \"mean\", \"std\"])\n)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nTrain/Val/Test statistics:\n       count      mean       std\nset                             \ntest      27  0.904191  0.030671\ntrain     97  0.917638  0.038728\nval       13  0.898951  0.025352\n"}],"execution_count":17},{"id":"9bfb86f4-2b5f-46c0-9ae8-005946abbac6","cell_type":"code","source":"# ============================================\n# SECTION 3D — EXTREME REACTIONS\n# ============================================\n\nhigh_outliers = switch_clean.sort_values(\"x_switch_mean\", ascending=False).head(5)\nlow_outliers  = switch_clean.sort_values(\"x_switch_mean\", ascending=True).head(5)\n\nprint(\"\\nTop 5 highest x_switch:\")\nprint(high_outliers[[\"Reaction\", \"x_switch_mean\"]])\n\nprint(\"\\nTop 5 lowest x_switch:\")\nprint(low_outliers[[\"Reaction\", \"x_switch_mean\"]])","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nTop 5 highest x_switch:\n          Reaction  x_switch_mean\n62   28 Si + 28 Si       1.087869\n115  35 Cl + 25 Mg       1.022943\n114  35 Cl + 24 Mg       0.994953\n63   28 Si + 30 Si       0.992699\n151  37 Cl + 26 Mg       0.983946\n\nTop 5 lowest x_switch:\n           Reaction  x_switch_mean\n197  58 Ni + 124 Sn       0.851637\n46    18 O + 112 Sn       0.852719\n82     32 S + 48 Ca       0.862382\n166   40 Ca + 40 Ca       0.866330\n129    36 S + 48 Ca       0.867260\n"}],"execution_count":18},{"id":"0c942cd8-a2f7-48c8-b7b2-4d1c0a9bc251","cell_type":"markdown","source":"# Section 3 — Universality of the Barrier-Normalized Transition Coordinate\n\n## Emergent Universality of $x_{\\text{switch}}$\n\nThe ensemble analysis over 10 independently initialized Mixture Density Networks reveals the existence of a robust, barrier-normalized transition coordinate:\n\n$$\nx_{\\text{switch}} = \\frac{E_{\\text{switch}}}{V_B}\n$$\n\ncomputed across 208 fusion reactions, with 137 reactions satisfying strict seed-consistency criteria.\n\nAcross the reliable set, the global mean transition coordinate is:\n\n$$\n\\bar{x}_{\\text{switch}} = 0.913\n$$\n\nwith a reaction-to-reaction standard deviation of:\n\n$$\n\\sigma_{\\text{reactions}} = 0.037\n$$\n\nTo quantify statistical confidence, a bootstrap resampling procedure (2000 resamples) was performed. The resulting 95% confidence interval for the global mean is:\n\n$$\n0.9074 \\le \\bar{x}_{\\text{switch}} \\le 0.9194\n$$\n\nThis narrow interval (width ≈ 0.012) demonstrates that the transition coordinate is not a statistical artifact of finite sampling or neural initialization noise. The seed-level variability is significantly smaller (mean seed std ≈ 0.016), confirming that the dominant contribution to the spread arises from genuine physical differences between reaction systems rather than stochastic training effects.\n\n---\n\n## Generalization Across Dataset Splits\n\nTo test for overfitting, the transition coordinate was analyzed separately for train, validation, and test reactions. The results are:\n\n| Set   | Mean $x_{\\text{switch}}$ | Std |\n|--------|---------------------------|------|\n| Train  | 0.918 | 0.039 |\n| Val    | 0.899 | 0.025 |\n| Test   | 0.904 | 0.031 |\n\nThe close agreement across splits confirms that the transition coordinate generalizes to unseen reactions. There is no evidence that the clustering near $x_{\\text{switch}} \\approx 0.91$ is driven by memorization of training systems.\n\n---\n\n## Interpretation\n\nThe histogram of $x_{\\text{switch}}$ values exhibits a clear unimodal structure centered slightly below unity, indicating that the dynamical regime change occurs consistently at approximately 90–92% of the nominal Coulomb barrier.\n\nPhysically, this suggests:\n\n- The transition from tunneling-dominated to adiabatic/neck-dominated dynamics occurs slightly below $V_B$.\n- The Coulomb barrier height serves as a natural normalization scale for dynamical identity change.\n- The clustering is tight enough to indicate an emergent collective behavior across diverse systems.\n\nImportantly, this behavior cannot be reproduced by random barrier shifts or arbitrary regression adjustments, since:\n\n- The transition coordinate is stable across seeds.\n- It persists in unseen test reactions.\n- It exhibits structured deviations linked to nuclear properties (addressed in Section 4).\n\n---\n\n## Structured Deviations and Outliers\n\nReactions with $x_{\\text{switch}} > 1$ are predominantly light, symmetric systems (e.g., $^{28}\\mathrm{Si}+^{28}\\mathrm{Si}$), known to exhibit molecular resonance and orientation-driven barrier effects.\n\nConversely, reactions with $x_{\\text{switch}} \\approx 0.85$ tend to involve heavy systems with strong coupling or transfer channels (e.g., $^{58}\\mathrm{Ni}+^{124}\\mathrm{Sn}$), consistent with earlier onset of sub-barrier enhancement.\n\nThus, deviations from the universal mean are not random scatter but structured physical modulation.\n\n---\n\n## Conclusion of Section\n\nThe analysis establishes the existence of an emergent, barrier-normalized transition coordinate:\n\n$$\nx_{\\text{switch}} \\approx 0.91 \\pm 0.01 \\; (\\text{statistical})\n$$\n\nwith structured physical deviations governed by nuclear structure parameters.\n\nThis constitutes the first probabilistic identification of a global dynamical transition scale in heavy-ion fusion derived directly from experimental residual data without imposing explicit coupled-channel assumptions.","metadata":{}},{"id":"b4029122-0d65-4df0-bd57-7e1ffe2b7516","cell_type":"code","source":"# ============================================\n# SECTION 4.1 — PREPARE STRUCTURAL FEATURES\n# ============================================\n\n# Work only with reliable reactions\ndf_struct = switch_df_clean.copy()\n\n# Merge needed structural quantities from original df\nbarrier_df_local = df.groupby(\"Reaction\").first().reset_index()\n\ndf_struct = df_struct.merge(\n    barrier_df_local[[\n        \"Reaction\",\n        \"Q ( 2 n )\",\n        \"β P\",\n        \"β T\"\n    ]],\n    on=\"Reaction\",\n    how=\"left\"\n)\n\n# Define effective deformation\ndf_struct[\"beta_eff\"] = abs(df_struct[\"β P\"]) + abs(df_struct[\"β T\"])\n\nprint(df_struct[[\n    \"x_switch_mean\",\n    \"beta_eff\",\n    \"Q ( 2 n )\"\n]].describe())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"       x_switch_mean    beta_eff   Q ( 2 n )\ncount     137.000000  137.000000  137.000000\nmean        0.913214    0.182307   -0.337153\nstd         0.036672    0.174165    3.839724\nmin         0.851637    0.000000  -14.710000\n25%         0.885743    0.035000   -2.520000\n50%         0.906192    0.164000   -0.910000\n75%         0.936251    0.270000    2.830000\nmax         1.087869    0.956000    6.490000\n"}],"execution_count":19},{"id":"16d5e68c-0fcf-4d04-92f1-d2882662ce42","cell_type":"code","source":"# ============================================\n# SECTION 4.2 — PEARSON CORRELATIONS\n# ============================================\n\nfrom scipy.stats import pearsonr\n\nx = df_struct[\"x_switch_mean\"].values\nbeta = df_struct[\"beta_eff\"].values\nq2n = df_struct[\"Q ( 2 n )\"].values\n\nr_beta, p_beta = pearsonr(x, beta)\nr_q2n, p_q2n = pearsonr(x, q2n)\n\nprint(\"Correlation with beta_eff:\")\nprint(\"r =\", round(r_beta, 4), \"p =\", \"{:.2e}\".format(p_beta))\n\nprint(\"\\nCorrelation with Q(2n):\")\nprint(\"r =\", round(r_q2n, 4), \"p =\", \"{:.2e}\".format(p_q2n))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Correlation with beta_eff:\nr = 0.8033 p = 3.48e-32\n\nCorrelation with Q(2n):\nr = -0.388 p = 2.80e-06\n"}],"execution_count":20},{"id":"8412c219-d9bc-4ce0-8858-71e8459a636d","cell_type":"code","source":"# ============================================\n# SECTION 4.3 — LINEAR REGRESSION\n# ============================================\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX = df_struct[[\"beta_eff\", \"Q ( 2 n )\"]].values\ny = df_struct[\"x_switch_mean\"].values\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = model.predict(X)\n\nr2 = r2_score(y, y_pred)\n\nprint(\"Intercept:\", round(model.intercept_, 4))\nprint(\"Coeff beta_eff:\", round(model.coef_[0], 4))\nprint(\"Coeff Q(2n):\", round(model.coef_[1], 4))\nprint(\"R²:\", round(r2, 4))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Intercept: 0.8824\nCoeff beta_eff: 0.1632\nCoeff Q(2n): -0.0031\nR²: 0.7466\n"}],"execution_count":21},{"id":"67f4e72e-8925-4de2-bcc9-debae7f9b91c","cell_type":"code","source":"# ============================================\n# SECTION 4.4 — SHUFFLED BASELINE\n# ============================================\n\nn_trials = 500\nr2_shuffled = []\n\nfor _ in range(n_trials):\n    y_shuffled = np.random.permutation(y)\n    model_s = LinearRegression().fit(X, y_shuffled)\n    r2_s = r2_score(y_shuffled, model_s.predict(X))\n    r2_shuffled.append(r2_s)\n\nr2_shuffled = np.array(r2_shuffled)\n\nprint(\"Real R²:\", round(r2, 4))\nprint(\"Mean shuffled R²:\", round(r2_shuffled.mean(), 4))\nprint(\"Max shuffled R²:\", round(r2_shuffled.max(), 4))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Real R²: 0.7466\nMean shuffled R²: 0.0143\nMax shuffled R²: 0.0924\n"}],"execution_count":22},{"id":"f8a6baef-7c64-4861-8742-3bb08f9229fc","cell_type":"markdown","source":"# Section 4 — Structural Modulation of the Transition Coordinate\n\n## 4.1 Clean Structural Dataset\n\nTo investigate the physical origin of the reaction-to-reaction spread in the transition coordinate,\n\n$$\nx_{\\text{switch}} = \\frac{E_{\\text{switch}}}{V_B},\n$$\n\nwe restrict the analysis to the **reliable subset** of reactions (137 systems) for which the regime transition is stable across ≥80% of neural network seeds.\n\nThis filtering step removes systems where the regime identity change is ambiguous or dominated by stochastic training fluctuations. Importantly, the filtering criterion depends only on internal model stability and not on nuclear structure parameters, ensuring that the regression analysis is not structurally biased.\n\n---\n\n## 4.2 Structural Parameters Considered\n\nTwo physically motivated structure variables are examined:\n\n1. **Effective deformation**\n   \n   $$\n   \\beta_{\\text{eff}} = |\\beta_P| + |\\beta_T|\n   $$\n\n2. **Two-neutron transfer $Q$-value**\n   \n   $$\n   Q_{2n}\n   $$\n\nThese quantities are known to influence sub-barrier fusion through barrier distribution broadening (deformation) and coupling-induced barrier lowering (transfer channels).\n\n---\n\n## 4.3 Correlation Analysis\n\nPearson correlation coefficients for the reliable dataset yield:\n\n- $\\mathrm{corr}(x_{\\text{switch}}, \\beta_{\\text{eff}}) = 0.803$\n- $\\mathrm{corr}(x_{\\text{switch}}, Q_{2n}) = -0.388$\n\nwith extremely small p-values (≪ 10⁻⁵), indicating statistically robust relationships.\n\nThe strong positive correlation with $\\beta_{\\text{eff}}$ demonstrates that deformation is the dominant structural driver of transition-energy modulation. Systems with larger deformation tend to undergo the regime change at higher normalized energies.\n\nThe negative correlation with $Q_{2n}$ indicates that energetically favorable neutron transfer promotes earlier transition into the adiabatic regime.\n\n---\n\n## 4.4 Linear Structural Scaling Law\n\nA linear regression model of the form\n\n$$\nx_{\\text{switch}} = a + b\\,\\beta_{\\text{eff}} + c\\,Q_{2n}\n$$\n\nyields:\n\n- Intercept: $a = 0.882$\n- $b = 0.163$\n- $c = -0.0031$\n\nwith\n\n$$\nR^2 = 0.747\n$$\n\nThis implies that approximately **75% of the variance** in the transition coordinate is explained by just two structural parameters.\n\nTo validate that this relationship is not accidental, a shuffled baseline test was performed by randomly permuting $x_{\\text{switch}}$ values across reactions. The shuffled distribution produced:\n\n- Mean $R^2 \\approx 0.014$\n- Maximum $R^2 \\approx 0.092$\n\nwhich is dramatically smaller than the observed value of 0.747. This confirms that the structural scaling is not a random alignment effect.\n\n---\n\n## 4.5 Physical Interpretation\n\nThe results indicate a hierarchical structure:\n\n1. A universal baseline threshold:\n   \n   $$\n   x_0 \\approx 0.91\n   $$\n\n2. Structured deviations governed primarily by deformation, with secondary modulation from transfer energetics.\n\nDeformation broadens the orientation-dependent barrier distribution, effectively delaying the onset of the dominant adiabatic/necking regime relative to the nominal Coulomb barrier.\n\nConversely, positive $Q_{2n}$ values enhance coupling and reduce the effective barrier, allowing the system to transition at lower normalized energies.\n\nThus, the transition coordinate is neither purely geometric nor purely stochastic; it is a structure-controlled dynamical threshold emerging from the probabilistic decomposition of experimental residuals.\n\n---\n\n## 4.6 Significance\n\nThe emergence of a scaling law explaining ~75% of transition-energy variance using only deformation and transfer energetics strongly supports the physical legitimacy of the probabilistically identified regime transition.\n\nThis demonstrates that the MDN-based regime discovery is not merely a statistical artifact, but instead reveals a quantitatively interpretable dynamical scale governed by intrinsic nuclear structure.","metadata":{}},{"id":"47500c60-4034-4a18-adce-c14b8bb2ca7e","cell_type":"code","source":"# ============================================\n# SECTION 5 — ROBUST INFORMATION LENGTH\n# ============================================\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\nfrom scipy.signal import savgol_filter\n\n# ---------------------------------------\n# SETTINGS\n# ---------------------------------------\n\nSMOOTH_WINDOW = 7        # must be odd\nSMOOTH_POLY   = 2\nLOCAL_WINDOW  = 0.10     # ±0.10 around x_switch\nMIN_POINTS    = 8        # minimum points required\n\n# ---------------------------------------\n# Helper: compute information length\n# ---------------------------------------\n\ndef compute_information_length(E, pi_matrix):\n    \"\"\"\n    Computes L = ∫ sqrt(sum_k (dpi_k/dE)^2) dE\n    \"\"\"\n    \n    # Smooth pi_k before derivative\n    pi_smooth = np.zeros_like(pi_matrix)\n    \n    for k in range(pi_matrix.shape[1]):\n        pi_smooth[:, k] = savgol_filter(\n            pi_matrix[:, k],\n            window_length=min(SMOOTH_WINDOW, len(E)//2*2+1),\n            polyorder=min(SMOOTH_POLY, 2)\n        )\n    \n    # Compute derivatives\n    dpi_dE = np.gradient(pi_smooth, E, axis=0)\n    \n    # Velocity in probability space\n    velocity = np.sqrt(np.sum(dpi_dE**2, axis=1))\n    \n    # Integratenp.trapezoid\n    L = np.trapezoid(velocity, E)\n    \n    return L\n\n# ---------------------------------------\n# Compute ensemble-averaged pi_k\n# ---------------------------------------\n\n# Load ensemble mean π\npi_all_seeds = []\n\nfor seed_data in all_seed_components:\n    pi_all_seeds.append(seed_data[\"pi\"])\n\npi_mean = np.mean(np.stack(pi_all_seeds), axis=0)\n\n# Attach to dataframe\ndf_info = df.copy().reset_index(drop=True)\n\nK = pi_mean.shape[1]\nfor k in range(K):\n    df_info[f\"pi_{k}\"] = pi_mean[:, k]\n\n# ---------------------------------------\n# Compute L per reliable reaction\n# ---------------------------------------\n\ninfo_records = []\n\nfor reaction in switch_df[switch_df[\"reliable\"]][\"Reaction\"]:\n    \n    sub = df_info[df_info[\"Reaction\"] == reaction].copy()\n    sub = sub.sort_values(\"E c.m.\")\n    \n    if len(sub) < MIN_POINTS:\n        continue\n    \n    E_vals = sub[\"E c.m.\"].values\n    pi_vals = sub[[f\"pi_{k}\" for k in range(K)]].values\n    \n    # --- Global L ---\n    L_global = compute_information_length(E_vals, pi_vals)\n    \n    # --- Local L (around x_switch window) ---\n    x_sw = switch_df.loc[\n        switch_df[\"Reaction\"] == reaction,\n        \"x_switch_mean\"\n    ].values[0]\n    \n    V_B = sub[\"V_B\"].iloc[0]\n    x_vals = E_vals / V_B\n    \n    mask = np.abs(x_vals - x_sw) <= LOCAL_WINDOW\n    \n    if np.sum(mask) >= MIN_POINTS:\n        L_local = compute_information_length(E_vals[mask], pi_vals[mask])\n    else:\n        L_local = np.nan\n    \n    info_records.append({\n        \"Reaction\": reaction,\n        \"L_global\": L_global,\n        \"L_local\": L_local,\n        \"x_switch\": x_sw\n    })\n\ninfo_df = pd.DataFrame(info_records)\n\nprint(\"Computed Information Length for\", len(info_df), \"reactions\")\nprint(info_df[[\"L_global\",\"L_local\"]].describe())","metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: invalid value encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: invalid value encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: invalid value encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1334: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: invalid value encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: invalid value encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: invalid value encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: invalid value encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: invalid value encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: invalid value encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1334: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: invalid value encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: invalid value encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: invalid value encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1334: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n"},{"name":"stdout","output_type":"stream","text":"Computed Information Length for 121 reactions\n         L_global    L_local\ncount  112.000000  84.000000\nmean     1.130213   1.289823\nstd      6.174073   7.122824\nmin      0.246887   0.265802\n25%      0.409140   0.395066\n50%      0.478080   0.454885\n75%      0.564652   0.523875\nmax     65.785332  65.725569\n"},{"name":"stderr","output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: divide by zero encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1309: RuntimeWarning: invalid value encountered in divide\n  a = -(dx2) / (dx1 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: divide by zero encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1310: RuntimeWarning: invalid value encountered in divide\n  b = (dx2 - dx1) / (dx1 * dx2)\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: divide by zero encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1311: RuntimeWarning: invalid value encountered in divide\n  c = dx1 / (dx2 * (dx1 + dx2))\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1317: RuntimeWarning: invalid value encountered in add\n  out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \\\n/srv/conda/envs/notebook/lib/python3.11/site-packages/numpy/lib/_function_base_impl.py:1334: RuntimeWarning: divide by zero encountered in divide\n  out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n\n"}],"execution_count":25},{"id":"872cec84-44bf-407f-8246-45f3a9ec5ebd","cell_type":"code","source":"# ============================================\n# SECTION 6 — CORRELATION ANALYSIS FOR L\n# ============================================\n\n# Merge structure features\ninfo_df = info_df.merge(\n    switch_df[[\"Reaction\"]],\n    on=\"Reaction\"\n)\n\ninfo_df = info_df.merge(\n    df.groupby(\"Reaction\").first().reset_index()[\n        [\"Reaction\",\"β P\",\"β T\",\"Q ( 2 n )\"]\n    ],\n    on=\"Reaction\"\n)\n\ninfo_df[\"beta_eff\"] = abs(info_df[\"β P\"]) + abs(info_df[\"β T\"])\n\nfrom scipy.stats import pearsonr\n\nprint(\"Correlation L_global vs beta_eff:\")\nprint(pearsonr(info_df[\"L_global\"], info_df[\"beta_eff\"]))\n\nprint(\"\\nCorrelation L_local vs beta_eff:\")\nprint(pearsonr(info_df[\"L_local\"].dropna(),\n               info_df.loc[info_df[\"L_local\"].notna(),\"beta_eff\"]))\n\nprint(\"\\nCorrelation L_global vs Q(2n):\")\nprint(pearsonr(info_df[\"L_global\"], info_df[\"Q ( 2 n )\"]))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Correlation L_global vs beta_eff:\nPearsonRResult(statistic=np.float64(nan), pvalue=np.float64(nan))\n\nCorrelation L_local vs beta_eff:\nPearsonRResult(statistic=np.float64(0.24720399700958443), pvalue=np.float64(0.023388891551724602))\n\nCorrelation L_global vs Q(2n):\nPearsonRResult(statistic=np.float64(nan), pvalue=np.float64(nan))\n"}],"execution_count":24},{"id":"1a70233d-172f-457a-a29a-9f7f134ca860","cell_type":"markdown","source":"# Section 5 — Information Length: Physical Interpretation and Assessment\n\n## 5.1 What Information Length Measures\n\nThe Information Length (L) is defined as\n\n$$\nL = \\int \\sqrt{\\sum_k \\left(\\frac{d\\pi_k}{dE}\\right)^2} \\, dE,\n$$\n\nwhere $\\pi_k(E)$ are the mixture weights predicted by the MDN for each dynamical regime.\n\nIn simple terms, $L$ measures how rapidly the system’s probabilistic identity changes as the collision energy increases.\n\n- If one regime smoothly evolves into another, $L$ remains small.\n- If the regime probabilities rearrange sharply over a narrow energy window, $L$ becomes large.\n\nThus, $L$ quantifies the *geometric motion in probability space* as energy increases. It is a measure of transition sharpness, not transition location.\n\nImportantly, once the MDN is trained, the calculation of $L$ does **not** use the original 29 input features. It depends only on the learned regime weights $\\pi_k(E)$. The structural features enter only indirectly through how they influenced the trained MDN.\n\n---\n\n## 5.2 Global vs Local Information Length\n\nTwo variants were evaluated:\n\n- **Global $L$** — integrated over the full energy range.\n- **Local $L$** — integrated within a window around the transition coordinate $x_{\\text{switch}}$.\n\nThe goal was to determine whether transition sharpness encodes structural or universal behavior similar to the transition location.\n\n### Observations:\n\n1. Global $L$ exhibited large variance and occasional extreme outliers.\n2. These outliers arise from the derivative-based nature of $L$, which amplifies small fluctuations in $\\pi_k(E)$.\n3. Local $L$ showed weak but statistically significant correlation with deformation ($\\beta_{\\text{eff}}$), with $r \\approx 0.25$.\n\nHowever, the magnitude of this correlation is modest compared to the strong structural scaling observed for $x_{\\text{switch}}$ (where $r \\approx 0.80$).\n\n---\n\n## 5.3 Interpretation\n\nThe results suggest a clear distinction:\n\n- The **transition location** ($x_{\\text{switch}}$) is a robust, structure-controlled dynamical threshold.\n- The **transition sharpness** ($L$) is weaker and more sensitive to numerical resolution and derivative amplification.\n\nThis is physically reasonable.\n\nThe energy scale at which the system changes dynamical identity appears to be tightly governed by nuclear structure (deformation and transfer energetics). In contrast, the sharpness of that change depends on finer details such as barrier distribution width, coupling strength variations, and energy sampling density.\n\nThus, Information Length does not exhibit universality comparable to the transition coordinate itself.\n\n---\n\n## 5.4 Scientific Conclusion on L\n\nInformation Length serves as a secondary diagnostic tool that characterizes the sharpness of probabilistic regime evolution. However:\n\n- It does not display strong universal scaling.\n- It does not outperform the transition coordinate in structural correlation.\n- It is numerically more fragile due to its derivative-based definition.\n\nTherefore, while informative, $L$ does not constitute the primary physical observable of this study.\n\nThe central dynamical quantity remains the barrier-normalized transition coordinate:\n\n$$\nx_{\\text{switch}} = \\frac{E_{\\text{switch}}}{V_B}.\n$$\n\nThis coordinate demonstrates both universality and strong structural modulation, whereas Information Length provides only a supplementary characterization of transition sharpness.","metadata":{}},{"id":"2743b35b-a175-4f4f-bacd-66b79ffc49c8","cell_type":"markdown","source":"# Section 6 — Transition Location vs Transition Sharpness\n\nThe study reveals a clear hierarchy between two classes of observables:\n\n| Observable | Physical Meaning | Structural Correlation | Stability |\n|------------|------------------|------------------------|-----------|\n| $x_{\\text{switch}}$ | Energy at which dominant regime changes | Strong ($R^2 \\approx 0.75$) | High |\n| $L$ | Sharpness of regime rearrangement | Weak | Moderate |\n\nThe transition coordinate exhibits:\n\n- Universality across reaction systems.\n- Strong correlation with deformation ($\\beta_{\\text{eff}}$).\n- Secondary modulation from $Q_{2n}$.\n- Stability across neural network seeds.\n\nIn contrast, Information Length:\n\n- Is sensitive to derivative amplification.\n- Shows modest structural dependence.\n- Does not exhibit clear universal clustering.\n\nThis distinction indicates that the primary dynamical signature in heavy-ion fusion is the *location* of the regime transition, rather than the detailed sharpness of that transition.\n\nThe probabilistic decomposition therefore reveals a structure-controlled energy threshold governing the onset of the adiabatic/necking regime, while the fine-grained transition dynamics remain system-specific.\n\nIn summary:\n\nThe universality resides in *where* the transition occurs, not in *how sharply* it occurs.","metadata":{}},{"id":"bdeda9eb-c01c-4357-922f-522576499d0f","cell_type":"code","source":"# ============================================\n# SECTION 7 — FULL 29-FEATURE REGRESSION\n# ============================================\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# ---------------------------------------\n# Prepare reliable dataset\n# ---------------------------------------\n\nreliable_reactions = switch_df[switch_df[\"reliable\"]][\"Reaction\"]\n\ndf_struct = (\n    df.groupby(\"Reaction\")\n      .first()\n      .reset_index()\n)\n\ndf_struct = df_struct[df_struct[\"Reaction\"].isin(reliable_reactions)]\n\ndf_struct = df_struct.merge(\n    switch_df[[\"Reaction\",\"x_switch_mean\"]],\n    on=\"Reaction\"\n)\n\n# 29 features\nfeatures_all = features_train.copy()\n\nX = df_struct[features_all].values\ny = df_struct[\"x_switch_mean\"].values\n\n# Standardize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ---------------------------------------\n# Cross-validated Linear Regression\n# ---------------------------------------\n\nmodel = LinearRegression()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_scores = cross_val_score(model, X_scaled, y,\n                            cv=kf, scoring='r2')\n\nprint(\"Cross-validated R² scores:\", cv_scores)\nprint(\"Mean CV R²:\", np.mean(cv_scores))\n\n# Fit full model for inspection\nmodel.fit(X_scaled, y)\nprint(\"Full-fit R²:\", r2_score(y, model.predict(X_scaled)))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Cross-validated R² scores: [0.77418409 0.67984962 0.72126519 0.62132631 0.59076587]\nMean CV R²: 0.6774782153859705\nFull-fit R²: 0.880803028404844\n"}],"execution_count":28},{"id":"a17578c3-113d-42ea-ab9d-2742b798d3ed","cell_type":"code","source":"# ============================================\n# SHUFFLED BASELINE TEST\n# ============================================\n\nn_shuffle = 200\nshuffle_scores = []\n\nfor i in range(n_shuffle):\n    y_shuffled = np.random.permutation(y)\n    score = np.mean(\n        cross_val_score(model, X_scaled, y_shuffled,\n                        cv=kf, scoring='r2')\n    )\n    shuffle_scores.append(score)\n\nprint(\"Mean shuffled R²:\", np.mean(shuffle_scores))\nprint(\"Max shuffled R²:\", np.max(shuffle_scores))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Mean shuffled R²: -0.5163073401454418\nMax shuffled R²: -0.16219976927155072\n"}],"execution_count":29},{"id":"b56681bd-6506-4d8f-a389-294c608569cf","cell_type":"code","source":"# ============================================\n# SECTION 8 — FEATURE SELECTION (LASSO)\n# ============================================\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\n# ---------------------------------------\n# Prepare reliable dataset\n# ---------------------------------------\n\nreliable_reactions = switch_df[switch_df[\"reliable\"]][\"Reaction\"]\n\ndf_struct = (\n    df.groupby(\"Reaction\")\n      .first()\n      .reset_index()\n)\n\ndf_struct = df_struct[df_struct[\"Reaction\"].isin(reliable_reactions)]\n\ndf_struct = df_struct.merge(\n    switch_df[[\"Reaction\",\"x_switch_mean\"]],\n    on=\"Reaction\"\n)\n\nfeatures_all = features_train.copy()\n\nX = df_struct[features_all].values\ny = df_struct[\"x_switch_mean\"].values\n\n# Standardize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ---------------------------------------\n# LASSO with cross-validation\n# ---------------------------------------\n\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_scaled, y)\n\nprint(\"Optimal alpha:\", lasso.alpha_)\nprint(\"LASSO R²:\", lasso.score(X_scaled, y))\n\n# ---------------------------------------\n# Extract selected features\n# ---------------------------------------\n\ncoef = pd.Series(lasso.coef_, index=features_all)\n\nselected_features = coef[coef != 0].sort_values(ascending=False)\n\nprint(\"\\nSelected Features:\")\nprint(selected_features)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Optimal alpha: 0.0003319814726235651\nLASSO R²: 0.8417759471186921\n\nSelected Features:\nR B                          0.036134\nCompound_Nucleus_S2n         0.008926\nmagic_dist_N1                0.005274\nmagic_dist_N2                0.002350\nN1                           0.000242\nCompound_Nucleus_Sn         -0.000951\nβ T                         -0.001117\nCompound_Nucleus_Sp         -0.001790\nmagic_dist_Z1               -0.003206\nProjectile_Binding_Energy   -0.007158\nβ P                         -0.007478\nQ ( 2 n )                   -0.008061\nTarget_Binding_Energy       -0.009533\nE c.m.                      -0.011745\nZ1Z2_over_Ecm               -0.016446\nħ ω                         -0.021254\ndtype: float64\n"},{"name":"stderr","output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.167e-05, tolerance: 1.391e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.266e-05, tolerance: 1.391e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.799e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.098e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.313e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.558e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.626e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.048e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.211e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.353e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.476e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.582e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n"}],"execution_count":30},{"id":"49f03937-ab48-4405-a0b7-bcd817c3e8a1","cell_type":"code","source":"# ============================================\n# SECTION 9 — CLEAN REACTION-LEVEL LASSO\n# ============================================\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\n# ---------------------------------------\n# Define intrinsic reaction-level features\n# ---------------------------------------\n\nreaction_features = [\n    'Z1','N1','A1',\n    'Z2','N2','A2',\n    'Q ( 2 n )',\n    'magic_dist_Z1','magic_dist_N1',\n    'magic_dist_Z2','magic_dist_N2',\n    'Z3','N3','A3',\n    'β P','β T',\n    'R B','ħ ω',\n    'Projectile_Mass_Actual',\n    'Target_Mass_Actual',\n    'Compound_Nucleus_Mass_Actual',\n    'Compound_Nucleus_Sp',\n    'Compound_Nucleus_Sn',\n    'Projectile_Binding_Energy',\n    'Target_Binding_Energy',\n    'Compound_Nucleus_Binding_Energy',\n    'Compound_Nucleus_S2n'\n]\n\n# ---------------------------------------\n# Prepare dataset\n# ---------------------------------------\n\nreliable_reactions = switch_df[switch_df[\"reliable\"]][\"Reaction\"]\n\ndf_struct = (\n    df.groupby(\"Reaction\")\n      .first()\n      .reset_index()\n)\n\ndf_struct = df_struct[df_struct[\"Reaction\"].isin(reliable_reactions)]\n\ndf_struct = df_struct.merge(\n    switch_df[[\"Reaction\",\"x_switch_mean\"]],\n    on=\"Reaction\"\n)\n\nX = df_struct[reaction_features].values\ny = df_struct[\"x_switch_mean\"].values\n\n# Standardize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ---------------------------------------\n# LASSO with cross-validation\n# ---------------------------------------\n\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_scaled, y)\n\nprint(\"Optimal alpha:\", lasso.alpha_)\nprint(\"Training R²:\", lasso.score(X_scaled, y))\n\n# ---------------------------------------\n# Cross-validated R²\n# ---------------------------------------\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_scores = cross_val_score(lasso, X_scaled, y,\n                            cv=kf, scoring='r2')\n\nprint(\"Cross-validated R² scores:\", cv_scores)\nprint(\"Mean CV R²:\", np.mean(cv_scores))\n\n# ---------------------------------------\n# Selected features\n# ---------------------------------------\n\ncoef = pd.Series(lasso.coef_, index=reaction_features)\n\nselected_features = coef[coef != 0].sort_values(ascending=False)\n\nprint(\"\\nSelected Features:\")\nprint(selected_features)","metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.027e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.539e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.877e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.190e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.360e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.345e-04, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.009e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.164e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.202e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.799e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.337e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.547e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.529e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.449e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.353e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.258e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.166e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.078e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.995e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.916e-05, tolerance: 1.497e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.147e-05, tolerance: 1.829e-05\n  model = cd_fast.enet_coordinate_descent(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.794e-06, tolerance: 8.282e-06\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.008e-06, tolerance: 8.282e-06\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.589e-06, tolerance: 8.282e-06\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.981e-05, tolerance: 1.482e-05\n  model = cd_fast.enet_coordinate_descent(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.179e-05, tolerance: 1.035e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.041e-05, tolerance: 1.035e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.375e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.411e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.351e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.285e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.540e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.052e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.697e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.224e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.588e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.786e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.082e-04, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e-04, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.252e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.565e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.966e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.882e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.778e-05, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.056e-04, tolerance: 1.013e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n"},{"name":"stdout","output_type":"stream","text":"Optimal alpha: 0.00010138234490021039\nTraining R²: 0.8262069085026964\n"},{"name":"stderr","output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.374e-05, tolerance: 1.111e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.387e-05, tolerance: 1.111e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.290e-04, tolerance: 1.375e-05\n  model = cd_fast.enet_coordinate_descent(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.552e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.363e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.523e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.113e-04, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.316e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.306e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.590e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.592e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.926e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.538e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.507e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.428e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.335e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.239e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.147e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.058e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.973e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.892e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.816e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.676e-05, tolerance: 1.245e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.681e-05, tolerance: 1.523e-05\n  model = cd_fast.enet_coordinate_descent(\n"},{"name":"stdout","output_type":"stream","text":"Cross-validated R² scores: [0.69269552 0.07068935 0.61549    0.68036163 0.74896735]\nMean CV R²: 0.5616407696236794\n\nSelected Features:\nR B                                0.039115\nCompound_Nucleus_Binding_Energy    0.011086\nCompound_Nucleus_S2n               0.006666\nmagic_dist_N1                      0.004638\nN1                                 0.002088\nmagic_dist_N2                      0.001582\nmagic_dist_Z2                     -0.000896\nCompound_Nucleus_Sn               -0.003050\nβ T                               -0.003065\nmagic_dist_Z1                     -0.003652\nCompound_Nucleus_Sp               -0.006520\nProjectile_Binding_Energy         -0.009365\nβ P                               -0.009385\nħ ω                               -0.010240\nQ ( 2 n )                         -0.011616\nTarget_Binding_Energy             -0.020226\nZ3                                -0.033570\ndtype: float64\n"},{"name":"stderr","output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.802e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.203e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.055e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.211e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.272e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.611e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.918e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.143e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.480e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.466e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.317e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.117e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.900e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.680e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.461e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.248e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.041e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.840e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.647e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.462e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.369e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.752e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.573e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.352e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.166e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:701: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.006e-05, tolerance: 1.353e-05\n  model = cd_fast.enet_coordinate_descent_gram(\n/srv/conda/envs/notebook/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:716: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.829e-04, tolerance: 1.649e-05\n  model = cd_fast.enet_coordinate_descent(\n"}],"execution_count":31},{"id":"477e576e-e075-4568-b66c-1fa6bee7f39c","cell_type":"code","source":"# ============================================\n# SECTION 10 — NONLINEAR STRUCTURE MODEL\n# ============================================\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\n\n# ---------------------------------------\n# Use only the two key structural variables\n# ---------------------------------------\n\ndf_nonlin = df_struct.copy()\n\ndf_nonlin[\"beta_eff\"] = abs(df_nonlin[\"β P\"]) + abs(df_nonlin[\"β T\"])\n\nX_simple = df_nonlin[[\"beta_eff\",\"Q ( 2 n )\"]].values\ny = df_nonlin[\"x_switch_mean\"].values\n\n# ---------------------------------------\n# Polynomial expansion (degree 2)\n# ---------------------------------------\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X_simple)\n\nmodel = LinearRegression()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_scores = cross_val_score(model, X_poly, y,\n                            cv=kf, scoring='r2')\n\nprint(\"Polynomial features:\", poly.get_feature_names_out())\nprint(\"Cross-validated R² scores:\", cv_scores)\nprint(\"Mean CV R²:\", np.mean(cv_scores))\n\n# Fit full model to inspect coefficients\nmodel.fit(X_poly, y)\n\ncoef_names = poly.get_feature_names_out()\ncoef_values = model.coef_\n\nprint(\"\\nCoefficients:\")\nfor name, coef in zip(coef_names, coef_values):\n    print(f\"{name}: {coef}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Polynomial features: ['x0' 'x1' 'x0^2' 'x0 x1' 'x1^2']\nCross-validated R² scores: [0.61776405 0.74921972 0.69886118 0.70013193 0.75914016]\nMean CV R²: 0.7050234100763637\n\nCoefficients:\nx0: 0.18748926805207713\nx1: -0.0021605062250205902\nx0^2: -0.042282014755786704\nx0 x1: -0.004616388964626581\nx1^2: -0.000132266328297357\n"}],"execution_count":32},{"id":"d4ad66c4-ee0e-4d2f-a67f-e5c0fe8eea1e","cell_type":"code","source":"# ============================================\n# SECTION 11 — RANDOM FOREST TEST\n# ============================================\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\n\n# Use intrinsic reaction-level features\nX_rf = df_struct[reaction_features].values\ny_rf = df_struct[\"x_switch_mean\"].values\n\nrf = RandomForestRegressor(\n    n_estimators=300,\n    max_depth=5,\n    random_state=42\n)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_scores = cross_val_score(rf, X_rf, y_rf,\n                            cv=kf, scoring='r2')\n\nprint(\"Random Forest CV R² scores:\", cv_scores)\nprint(\"Mean CV R²:\", np.mean(cv_scores))\n\n# Fit to inspect feature importance\nrf.fit(X_rf, y_rf)\n\nimportances = rf.feature_importances_\n\nfeature_importance_df = (\n    pd.Series(importances, index=reaction_features)\n      .sort_values(ascending=False)\n)\n\nprint(\"\\nTop 10 Feature Importances:\")\nprint(feature_importance_df.head(10))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Random Forest CV R² scores: [0.51809358 0.61641762 0.62879821 0.79974014 0.77443838]\nMean CV R²: 0.6674975862266571\n\nTop 10 Feature Importances:\nβ T                             0.188069\nβ P                             0.120760\nR B                             0.095398\nQ ( 2 n )                       0.062649\nTarget_Binding_Energy           0.053697\nN3                              0.045688\nProjectile_Binding_Energy       0.043073\nA3                              0.042388\nCompound_Nucleus_Mass_Actual    0.040784\nCompound_Nucleus_S2n            0.030178\ndtype: float64\n"}],"execution_count":33},{"id":"4c21380c-9158-46a1-bf8d-87a6b3da32c5","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f0fd81c5-fdf6-4331-8d8b-62ae164b6dba","cell_type":"code","source":"!pip install xgboost\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting xgboost\n  Downloading xgboost-3.2.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.11/site-packages (from xgboost) (2.3.5)\nCollecting nvidia-nccl-cu12 (from xgboost)\n  Downloading nvidia_nccl_cu12-2.29.3-py3-none-manylinux_2_18_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.11/site-packages (from xgboost) (1.17.0)\nDownloading xgboost-3.2.0-py3-none-manylinux_2_28_x86_64.whl (131.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.29.3-py3-none-manylinux_2_18_x86_64.whl (289.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 MB\u001b[0m \u001b[31m207.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [xgboost]m1/2\u001b[0m [xgboost]\n\u001b[1A\u001b[2KSuccessfully installed nvidia-nccl-cu12-2.29.3 xgboost-3.2.0\n"}],"execution_count":35},{"id":"eb3547f8-5d9e-425c-85dc-4aa3e5cf9053","cell_type":"code","source":"# ============================================\n# SECTION 12 — XGBOOST TEST\n# ============================================\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\n\n# Use intrinsic reaction-level features\nX_xgb = df_struct[reaction_features].values\ny_xgb = df_struct[\"x_switch_mean\"].values\n\nxgb = XGBRegressor(\n    n_estimators=500,\n    max_depth=3,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_scores = cross_val_score(\n    xgb, X_xgb, y_xgb,\n    cv=kf,\n    scoring='r2'\n)\n\nprint(\"XGBoost CV R² scores:\", cv_scores)\nprint(\"Mean CV R²:\", np.mean(cv_scores))\n\n# Fit to inspect feature importance\nxgb.fit(X_xgb, y_xgb)\n\nimportances = xgb.feature_importances_\n\nfeature_importance_df = (\n    pd.Series(importances, index=reaction_features)\n      .sort_values(ascending=False)\n)\n\nprint(\"\\nTop 10 Feature Importances:\")\nprint(feature_importance_df.head(10))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"XGBoost CV R² scores: [0.6713767  0.71557127 0.73991773 0.6216439  0.85007044]\nMean CV R²: 0.7197160068715613\n\nTop 10 Feature Importances:\nβ T                             0.140280\nβ P                             0.105489\nProjectile_Binding_Energy       0.092814\nZ2                              0.078512\nZ3                              0.069432\nA3                              0.055876\nN3                              0.046340\nR B                             0.044197\nCompound_Nucleus_Mass_Actual    0.040386\nZ1                              0.039712\ndtype: float32\n"}],"execution_count":36},{"id":"45cabc5d-091e-43de-983f-070431c8d53e","cell_type":"markdown","source":"# Section 8 — Sparse Feature Selection (LASSO)\n\nTo identify the minimal subset of intrinsic reaction-level features governing the transition coordinate, LASSO regression with 5-fold cross-validation was performed.\n\nOnly intrinsic nuclear properties were retained (energy-dependent features removed).\n\n**Results:**\n\n- Mean cross-validated R² ≈ 0.56  \n\nThe reduction in performance compared to the deformation + Q₂n model indicates that expanding the feature space does not enhance predictive power. Instead, the transition coordinate appears to be primarily controlled by a small subset of physically meaningful structural variables.\n\nLASSO consistently retained:\n\n- Barrier radius (R_B)\n- Deformation parameters (βP, βT)\n- Two-neutron transfer Q-value\n- Barrier curvature (ħω)\n- Selected separation and binding energies\n\nHowever, the overall predictive performance did not exceed that of the simple structural model, reinforcing the conclusion that the transition coordinate is fundamentally low-dimensional.\n    # Section 8 — Sparse Feature Selection (LASSO)\n\nTo identify the minimal subset of intrinsic reaction-level features governing the transition coordinate, LASSO regression with 5-fold cross-validation was performed.\n\nOnly intrinsic nuclear properties were retained (energy-dependent features removed).\n\n**Results:**\n\n- Mean cross-validated R² ≈ 0.56  \n\nThe reduction in performance compared to the deformation + Q₂n model indicates that expanding the feature space does not enhance predictive power. Instead, the transition coordinate appears to be primarily controlled by a small subset of physically meaningful structural variables.\n\nLASSO consistently retained:\n\n- Barrier radius (R_B)\n- Deformation parameters (βP, βT)\n- Two-neutron transfer Q-value\n- Barrier curvature (ħω)\n- Selected separation and binding energies\n\nHowever, the overall predictive performance did not exceed that of the simple structural model, reinforcing the conclusion that the transition coordinate is fundamentally low-dimensional.\n\n# Section 10 — Nonlinear Structural Model\n\nTo test whether nonlinear interactions between deformation and transfer energetics improve explanatory power, a quadratic polynomial expansion was constructed:\n\nx_switch = f(β_eff, Q₂n, β_eff², β_eff·Q₂n, Q₂n²)\n\nUsing 5-fold cross-validation:\n\n- Mean CV R² ≈ 0.70  \n\nThe performance does not exceed that of the linear deformation + Q₂n model.\n\nThe quadratic coefficient in β_eff is small and negative, indicating weak curvature but no strong nonlinear enhancement.\n\nThis suggests that the structural dependence of the transition coordinate is approximately linear in deformation, with only minor higher-order corrections.\n    # Section 11 — Random Forest Stress Test\n\nA Random Forest regressor was applied to the intrinsic reaction-level features to test for hidden nonlinear structure.\n\n5-fold cross-validation yielded:\n\n- Mean CV R² ≈ 0.67  \n\nFeature importance rankings identified deformation parameters (βP, βT) as the dominant contributors, followed by barrier geometry (R_B) and transfer energetics (Q₂n).\n\nThe performance does not exceed that of the simple linear structural model.\n\nThis indicates that nonlinear ensemble methods do not uncover additional predictive structure beyond deformation and transfer effects.\n# Section 12 — XGBoost Boosted Model Test\n\nTo further test for subtle nonlinear interactions, a gradient-boosted decision tree model (XGBoost) was evaluated.\n\n5-fold cross-validation yielded:\n\n- Mean CV R² ≈ 0.72  \n\nFeature importance again ranked deformation parameters as dominant, followed by selected binding and mass terms.\n\nAlthough XGBoost captures nonlinear structure more effectively than Random Forest, it does not outperform the simple linear deformation + Q₂n model.\n\nThis confirms that the transition coordinate is not a high-dimensional nonlinear artifact, but instead reflects a fundamentally low-dimensional structural scaling law.\n# Section 13 — Model Comparison and Structural Conclusion\n\nA systematic comparison of all tested models reveals:\n\n| Model | Mean CV R² |\n|--------|------------|\n| Linear (β_eff + Q₂n) | ~0.75 |\n| Quadratic (β_eff, Q₂n) | ~0.70 |\n| Full 29-feature linear | ~0.68 |\n| Random Forest | ~0.67 |\n| XGBoost | ~0.72 |\n| Clean LASSO | ~0.56 |\n\nThe simple linear structural model consistently demonstrates the strongest generalization performance.\n\nThis indicates that:\n\n1. The transition coordinate is primarily governed by deformation.\n2. Two-neutron transfer energetics provide secondary modulation.\n3. Barrier geometry plays a supporting role.\n4. High-dimensional nonlinear ML models do not uncover additional hidden structure.\n\nTherefore, the barrier-normalized transition coordinate can be expressed approximately as:\n\nx_switch ≈ 0.88 + 0.16 β_eff − 0.003 Q₂n\n\nThis result demonstrates that the emergent transition scale discovered via probabilistic regime decomposition collapses onto a compact, interpretable nuclear structure law.","metadata":{}},{"id":"7053a9ca-fa46-4aa1-977d-11a9151eab5c","cell_type":"code","source":"!pip install pysr","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting pysr\n  Downloading pysr-1.5.9-py3-none-any.whl.metadata (54 kB)\nRequirement already satisfied: click<9.0.0,>=7.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pysr) (8.3.1)\nCollecting juliacall<0.9.27,>=0.9.24 (from pysr)\n  Downloading juliacall-0.9.26-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.13.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pysr) (2.3.5)\nCollecting pandas<3.0.0,>=0.21.0 (from pysr)\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pysr) (1.8.0)\nRequirement already satisfied: sympy<2.0.0,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pysr) (1.14.0)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pysr) (4.15.0)\nCollecting juliapkg<0.2,>=0.1.17 (from juliacall<0.9.27,>=0.9.24->pysr)\n  Downloading juliapkg-0.1.23-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: filelock<4.0,>=3.16 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from juliapkg<0.2,>=0.1.17->juliacall<0.9.27,>=0.9.24->pysr) (3.24.3)\nCollecting semver<4.0,>=3.0 (from juliapkg<0.2,>=0.1.17->juliacall<0.9.27,>=0.9.24->pysr)\n  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: tomli<3.0,>=2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from juliapkg<0.2,>=0.1.17->juliacall<0.9.27,>=0.9.24->pysr) (2.3.0)\nCollecting tomlkit<0.15,>=0.13.3 (from juliapkg<0.2,>=0.1.17->juliacall<0.9.27,>=0.9.24->pysr)\n  Downloading tomlkit-0.14.0-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas<3.0.0,>=0.21.0->pysr) (2025.2)\nCollecting tzdata>=2022.7 (from pandas<3.0.0,>=0.21.0->pysr)\n  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: scipy>=1.10.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.0.0->pysr) (1.17.0)\nRequirement already satisfied: joblib>=1.3.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.0.0->pysr) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.0.0->pysr) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from sympy<2.0.0,>=1.0.0->pysr) (1.3.0)\nRequirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=0.21.0->pysr) (1.17.0)\nDownloading pysr-1.5.9-py3-none-any.whl (99 kB)\nDownloading juliacall-0.9.26-py3-none-any.whl (12 kB)\nDownloading juliapkg-0.1.23-py3-none-any.whl (21 kB)\nDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m209.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\nDownloading tomlkit-0.14.0-py3-none-any.whl (39 kB)\nDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\nInstalling collected packages: tzdata, tomlkit, semver, pandas, juliapkg, juliacall, pysr\n\u001b[2K  Attempting uninstall: pandas\n\u001b[2K    Found existing installation: pandas 3.0.1\n\u001b[2K    Uninstalling pandas-3.0.1:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [pandas]\n\u001b[2K      Successfully uninstalled pandas-3.0.1m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/7\u001b[0m [pandas]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [pysr][32m6/7\u001b[0m [pysr]s]\n\u001b[1A\u001b[2KSuccessfully installed juliacall-0.9.26 juliapkg-0.1.23 pandas-2.3.3 pysr-1.5.9 semver-3.0.4 tomlkit-0.14.0 tzdata-2025.3\n"}],"execution_count":38},{"id":"1818e81e-0b3d-45d2-84b7-25ff859f0144","cell_type":"code","source":"import pysr","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"[juliapkg] Found dependencies: /srv/conda/envs/notebook/lib/python3.11/site-packages/pysr/juliapkg.json\n[juliapkg] Found dependencies: /srv/conda/envs/notebook/lib/python3.11/site-packages/awkward/juliapkg.json\n[juliapkg] Found dependencies: /srv/conda/envs/notebook/lib/python3.11/site-packages/juliapkg/juliapkg.json\n[juliapkg] Found dependencies: /srv/conda/envs/notebook/lib/python3.11/site-packages/juliacall/juliapkg.json\n[juliapkg] Locating Julia ^1.10.3\n[juliapkg] Querying Julia versions from https://julialang-s3.julialang.org/bin/versions.json\n[juliapkg] WARNING: About to install Julia 1.12.5 to /srv/conda/envs/notebook/julia_env/pyjuliapkg/install.\n[juliapkg]   If you use juliapkg in more than one environment, you are likely to\n[juliapkg]   have Julia installed in multiple locations. It is recommended to\n[juliapkg]   install JuliaUp (https://github.com/JuliaLang/juliaup) or Julia\n[juliapkg]   (https://julialang.org/downloads) yourself.\n[juliapkg] Downloading Julia from https://julialang-s3.julialang.org/bin/linux/x64/1.12/julia-1.12.5-linux-x86_64.tar.gz\n             download complete\n[juliapkg] Verifying download\n[juliapkg] Installing Julia 1.12.5 to /srv/conda/envs/notebook/julia_env/pyjuliapkg/install\n[juliapkg] Using Julia 1.12.5 at /srv/conda/envs/notebook/julia_env/pyjuliapkg/install/bin/julia\n[juliapkg] Using Julia project at /srv/conda/envs/notebook/julia_env\n[juliapkg] Writing Project.toml:\n           | [deps]\n           | SymbolicRegression = \"8254be44-1295-4e6a-a16d-46603ac705cb\"\n           | Serialization = \"9e88b42a-f829-5b0c-bbe9-9e923198166b\"\n           | AwkwardArray = \"7d259134-7f60-4bf1-aa00-7452e11bde56\"\n           | PythonCall = \"6099a3de-0909-46bc-b1f4-468b9a2dfc0d\"\n           | OpenSSL_jll = \"458c3c95-2e84-50aa-8efc-19380b2a3a95\"\n           | \n           | [compat]\n           | SymbolicRegression = \"~1.11\"\n           | Serialization = \"^1\"\n           | AwkwardArray = \"^0.1\"\n           | PythonCall = \"=0.9.26\"\n           | OpenSSL_jll = \"3.0.0 - 3.5\"\n[juliapkg] Installing packages:\n           | import Pkg\n           | Pkg.Registry.update()\n           | Pkg.add([\n           |   Pkg.PackageSpec(name=\"SymbolicRegression\", uuid=\"8254be44-1295-4e6a-a16d-46603ac705cb\"),\n           |   Pkg.PackageSpec(name=\"Serialization\", uuid=\"9e88b42a-f829-5b0c-bbe9-9e923198166b\"),\n           |   Pkg.PackageSpec(name=\"AwkwardArray\", uuid=\"7d259134-7f60-4bf1-aa00-7452e11bde56\"),\n           |   Pkg.PackageSpec(name=\"PythonCall\", uuid=\"6099a3de-0909-46bc-b1f4-468b9a2dfc0d\"),\n           |   Pkg.PackageSpec(name=\"OpenSSL_jll\", uuid=\"458c3c95-2e84-50aa-8efc-19380b2a3a95\"),\n           | ])\n           | Pkg.resolve()\n           | Pkg.precompile()\n"},{"name":"stderr","output_type":"stream","text":"\u001b[32m\u001b[1m  Installing\u001b[22m\u001b[39m known registries into `~/.julia`\n\u001b[32m\u001b[1m       Added\u001b[22m\u001b[39m `General` registry to ~/.julia/registries\n\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Tricks ────────────────────── v0.1.13\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ScientificTypesBase ───────── v3.1.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IrrationalConstants ───────── v0.2.6\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Adapt ─────────────────────── v4.4.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffRules ─────────────────── v1.15.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Scratch ───────────────────── v1.3.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DynamicExpressions ────────── v1.10.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MicroMamba ────────────────── v0.1.15\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Conda ─────────────────────── v1.10.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JSON ──────────────────────── v0.21.4\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MLJModelInterface ─────────── v1.11.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JSON3 ─────────────────────── v1.14.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TableTraits ───────────────── v1.0.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PythonCall ────────────────── v0.9.26\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PositiveFactorizations ────── v0.2.4\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ADTypes ───────────────────── v1.21.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatisticalTraits ─────────── v3.5.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DiffResults ───────────────── v1.1.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Parsers ───────────────────── v2.8.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SpecialFunctions ──────────── v2.7.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PtrArrays ─────────────────── v1.4.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Preferences ───────────────── v1.5.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataAPI ───────────────────── v1.16.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Tables ────────────────────── v1.12.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ProgressMeter ─────────────── v1.10.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m JLLWrappers ───────────────── v1.7.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DynamicDiff ───────────────── v0.2.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Optim ─────────────────────── v1.13.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m micromamba_jll ────────────── v2.3.1+0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NaNMath ───────────────────── v1.1.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StaticArraysCore ──────────── v1.4.4\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m IteratorInterfaceExtensions ─ v1.0.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ConstructionBase ──────────── v1.6.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataValueInterfaces ───────── v1.0.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OrderedCollections ────────── v1.8.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Setfield ──────────────────── v1.1.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m VersionParsing ────────────── v1.3.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Pidfile ───────────────────── v1.3.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m NLSolversBase ─────────────── v7.10.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ChainRulesCore ────────────── v1.26.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m EnumX ─────────────────────── v1.0.7\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ArrayInterface ────────────── v7.22.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FillArrays ────────────────── v1.16.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m ForwardDiff ───────────────── v1.3.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AwkwardArray ──────────────── v0.1.6\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m PrecompileTools ───────────── v1.3.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m TestItems ─────────────────── v1.0.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Reexport ──────────────────── v1.2.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Statistics ────────────────── v1.11.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LogExpFunctions ───────────── v0.3.29\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CommonSubexpressions ──────── v0.3.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LineSearches ──────────────── v7.5.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DataStructures ────────────── v0.19.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m AliasTables ───────────────── v1.1.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m MacroTools ────────────────── v0.5.16\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m UnsafePointers ────────────── v1.0.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m OpenSpecFun_jll ───────────── v0.5.6+0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsAPI ──────────────────── v1.8.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Compat ────────────────────── v4.18.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DispatchDoctor ────────────── v0.4.28\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Requires ──────────────────── v1.3.1\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Interfaces ────────────────── v0.3.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SymbolicRegression ────────── v1.11.3\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m CondaPkg ──────────────────── v0.2.33\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DynamicQuantities ─────────── v1.11.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Missings ──────────────────── v1.2.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StructTypes ───────────────── v1.11.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m SortingAlgorithms ─────────── v1.2.2\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m StatsBase ─────────────────── v0.34.10\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m LossFunctions ─────────────── v1.2.0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m pixi_jll ──────────────────── v0.41.3+0\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DocStringExtensions ───────── v0.9.5\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m DifferentiationInterface ──── v0.7.16\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m FiniteDiff ────────────────── v2.29.0\n\u001b[32m\u001b[1m  Installing\u001b[22m\u001b[39m 1 artifacts\n\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m artifact OpenSpecFun 194.9 KiB\n\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `/srv/conda/envs/notebook/julia_env/Project.toml`\n  \u001b[90m[7d259134] \u001b[39m\u001b[92m+ AwkwardArray v0.1.6\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[6099a3de] \u001b[39m\u001b[92m+ PythonCall v0.9.26\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[8254be44] \u001b[39m\u001b[92m+ SymbolicRegression v1.11.3\u001b[39m\n  \u001b[90m[9e88b42a] \u001b[39m\u001b[93m~ Serialization ⇒ v1.11.0\u001b[39m\n  \u001b[90m[458c3c95] \u001b[39m\u001b[93m~ OpenSSL_jll ⇒ v3.5.4+0\u001b[39m\n\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `/srv/conda/envs/notebook/julia_env/Manifest.toml`\n  \u001b[90m[47edcb42] \u001b[39m\u001b[92m+ ADTypes v1.21.0\u001b[39m\n  \u001b[90m[79e6a3ab] \u001b[39m\u001b[92m+ Adapt v4.4.0\u001b[39m\n  \u001b[90m[66dad0bd] \u001b[39m\u001b[92m+ AliasTables v1.1.3\u001b[39m\n  \u001b[90m[4fba245c] \u001b[39m\u001b[92m+ ArrayInterface v7.22.0\u001b[39m\n  \u001b[90m[7d259134] \u001b[39m\u001b[92m+ AwkwardArray v0.1.6\u001b[39m\n  \u001b[90m[d360d2e6] \u001b[39m\u001b[92m+ ChainRulesCore v1.26.0\u001b[39m\n  \u001b[90m[bbf7d656] \u001b[39m\u001b[92m+ CommonSubexpressions v0.3.1\u001b[39m\n  \u001b[90m[34da2185] \u001b[39m\u001b[92m+ Compat v4.18.1\u001b[39m\n  \u001b[90m[8f4d0f93] \u001b[39m\u001b[92m+ Conda v1.10.3\u001b[39m\n\u001b[32m⌃\u001b[39m \u001b[90m[992eb4ea] \u001b[39m\u001b[92m+ CondaPkg v0.2.33\u001b[39m\n  \u001b[90m[187b0558] \u001b[39m\u001b[92m+ ConstructionBase v1.6.0\u001b[39m\n  \u001b[90m[9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.16.0\u001b[39m\n  \u001b[90m[864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.19.3\u001b[39m\n  \u001b[90m[e2d170a0] \u001b[39m\u001b[92m+ DataValueInterfaces v1.0.0\u001b[39m\n  \u001b[90m[163ba53b] \u001b[39m\u001b[92m+ DiffResults v1.1.0\u001b[39m\n  \u001b[90m[b552c78f] \u001b[39m\u001b[92m+ DiffRules v1.15.1\u001b[39m\n  \u001b[90m[a0c0ee7d] \u001b[39m\u001b[92m+ DifferentiationInterface v0.7.16\u001b[39m\n  \u001b[90m[8d63f2c5] \u001b[39m\u001b[92m+ DispatchDoctor v0.4.28\u001b[39m\n  \u001b[90m[ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.9.5\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[7317a516] \u001b[39m\u001b[92m+ DynamicDiff v0.2.1\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[a40a106e] \u001b[39m\u001b[92m+ DynamicExpressions v1.10.3\u001b[39m\n  \u001b[90m[06fc5a27] \u001b[39m\u001b[92m+ DynamicQuantities v1.11.0\u001b[39m\n  \u001b[90m[4e289a0a] \u001b[39m\u001b[92m+ EnumX v1.0.7\u001b[39m\n  \u001b[90m[1a297f60] \u001b[39m\u001b[92m+ FillArrays v1.16.0\u001b[39m\n  \u001b[90m[6a86dc24] \u001b[39m\u001b[92m+ FiniteDiff v2.29.0\u001b[39m\n  \u001b[90m[f6369f11] \u001b[39m\u001b[92m+ ForwardDiff v1.3.2\u001b[39m\n  \u001b[90m[85a1e053] \u001b[39m\u001b[92m+ Interfaces v0.3.2\u001b[39m\n  \u001b[90m[92d709cd] \u001b[39m\u001b[92m+ IrrationalConstants v0.2.6\u001b[39m\n  \u001b[90m[82899510] \u001b[39m\u001b[92m+ IteratorInterfaceExtensions v1.0.0\u001b[39m\n  \u001b[90m[692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.7.1\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.4\u001b[39m\n  \u001b[90m[0f8b85d8] \u001b[39m\u001b[92m+ JSON3 v1.14.3\u001b[39m\n\u001b[32m⌃\u001b[39m \u001b[90m[d3d80556] \u001b[39m\u001b[92m+ LineSearches v7.5.1\u001b[39m\n  \u001b[90m[2ab3a3ac] \u001b[39m\u001b[92m+ LogExpFunctions v0.3.29\u001b[39m\n  \u001b[90m[30fc2ffe] \u001b[39m\u001b[92m+ LossFunctions v1.2.0\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[e80e1ace] \u001b[39m\u001b[92m+ MLJModelInterface v1.11.1\u001b[39m\n  \u001b[90m[1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.16\u001b[39m\n  \u001b[90m[0b3b1443] \u001b[39m\u001b[92m+ MicroMamba v0.1.15\u001b[39m\n  \u001b[90m[e1d29d7a] \u001b[39m\u001b[92m+ Missings v1.2.0\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[d41bc354] \u001b[39m\u001b[92m+ NLSolversBase v7.10.0\u001b[39m\n  \u001b[90m[77ba4419] \u001b[39m\u001b[92m+ NaNMath v1.1.3\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[429524aa] \u001b[39m\u001b[92m+ Optim v1.13.3\u001b[39m\n  \u001b[90m[bac558e1] \u001b[39m\u001b[92m+ OrderedCollections v1.8.1\u001b[39m\n  \u001b[90m[69de0a69] \u001b[39m\u001b[92m+ Parsers v2.8.3\u001b[39m\n  \u001b[90m[fa939f87] \u001b[39m\u001b[92m+ Pidfile v1.3.0\u001b[39m\n  \u001b[90m[85a6dd25] \u001b[39m\u001b[92m+ PositiveFactorizations v0.2.4\u001b[39m\n  \u001b[90m[aea7be01] \u001b[39m\u001b[92m+ PrecompileTools v1.3.3\u001b[39m\n  \u001b[90m[21216c6a] \u001b[39m\u001b[92m+ Preferences v1.5.2\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[92933f4c] \u001b[39m\u001b[92m+ ProgressMeter v1.10.2\u001b[39m\n  \u001b[90m[43287f4e] \u001b[39m\u001b[92m+ PtrArrays v1.4.0\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[6099a3de] \u001b[39m\u001b[92m+ PythonCall v0.9.26\u001b[39m\n  \u001b[90m[189a3867] \u001b[39m\u001b[92m+ Reexport v1.2.2\u001b[39m\n  \u001b[90m[ae029012] \u001b[39m\u001b[92m+ Requires v1.3.1\u001b[39m\n  \u001b[90m[30f210dd] \u001b[39m\u001b[92m+ ScientificTypesBase v3.1.0\u001b[39m\n  \u001b[90m[6c6a2e73] \u001b[39m\u001b[92m+ Scratch v1.3.0\u001b[39m\n  \u001b[90m[efcf1570] \u001b[39m\u001b[92m+ Setfield v1.1.2\u001b[39m\n  \u001b[90m[a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v1.2.2\u001b[39m\n  \u001b[90m[276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v2.7.1\u001b[39m\n  \u001b[90m[1e83bf80] \u001b[39m\u001b[92m+ StaticArraysCore v1.4.4\u001b[39m\n  \u001b[90m[64bff920] \u001b[39m\u001b[92m+ StatisticalTraits v3.5.0\u001b[39m\n  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n  \u001b[90m[82ae8749] \u001b[39m\u001b[92m+ StatsAPI v1.8.0\u001b[39m\n  \u001b[90m[2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.34.10\u001b[39m\n  \u001b[90m[856f2bd8] \u001b[39m\u001b[92m+ StructTypes v1.11.0\u001b[39m\n\u001b[33m⌅\u001b[39m \u001b[90m[8254be44] \u001b[39m\u001b[92m+ SymbolicRegression v1.11.3\u001b[39m\n  \u001b[90m[3783bdb8] \u001b[39m\u001b[92m+ TableTraits v1.0.1\u001b[39m\n  \u001b[90m[bd369af6] \u001b[39m\u001b[92m+ Tables v1.12.1\u001b[39m\n  \u001b[90m[1c621080] \u001b[39m\u001b[92m+ TestItems v1.0.0\u001b[39m\n  \u001b[90m[410a4b4d] \u001b[39m\u001b[92m+ Tricks v0.1.13\u001b[39m\n  \u001b[90m[e17b2a0c] \u001b[39m\u001b[92m+ UnsafePointers v1.0.0\u001b[39m\n  \u001b[90m[81def892] \u001b[39m\u001b[92m+ VersionParsing v1.3.0\u001b[39m\n  \u001b[90m[efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.6+0\u001b[39m\n  \u001b[90m[f8abcde7] \u001b[39m\u001b[92m+ micromamba_jll v2.3.1+0\u001b[39m\n  \u001b[90m[4d7b5844] \u001b[39m\u001b[92m+ pixi_jll v0.41.3+0\u001b[39m\n  \u001b[90m[0dad84c5] \u001b[39m\u001b[92m+ ArgTools v1.1.2\u001b[39m\n  \u001b[90m[56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.11.0\u001b[39m\n  \u001b[90m[2a0f44e3] \u001b[39m\u001b[92m+ Base64 v1.11.0\u001b[39m\n  \u001b[90m[ade2ca70] \u001b[39m\u001b[92m+ Dates v1.11.0\u001b[39m\n  \u001b[90m[8ba89e20] \u001b[39m\u001b[92m+ Distributed v1.11.0\u001b[39m\n  \u001b[90m[f43a241f] \u001b[39m\u001b[92m+ Downloads v1.7.0\u001b[39m\n  \u001b[90m[7b1f6079] \u001b[39m\u001b[92m+ FileWatching v1.11.0\u001b[39m\n  \u001b[90m[9fa8497b] \u001b[39m\u001b[92m+ Future v1.11.0\u001b[39m\n  \u001b[90m[b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils v1.11.0\u001b[39m\n  \u001b[90m[ac6e5ff7] \u001b[39m\u001b[92m+ JuliaSyntaxHighlighting v1.12.0\u001b[39m\n  \u001b[90m[4af54fe1] \u001b[39m\u001b[92m+ LazyArtifacts v1.11.0\u001b[39m\n  \u001b[90m[b27032c2] \u001b[39m\u001b[92m+ LibCURL v0.6.4\u001b[39m\n  \u001b[90m[76f85450] \u001b[39m\u001b[92m+ LibGit2 v1.11.0\u001b[39m\n  \u001b[90m[8f399da3] \u001b[39m\u001b[92m+ Libdl v1.11.0\u001b[39m\n  \u001b[90m[37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra v1.12.0\u001b[39m\n  \u001b[90m[56ddb016] \u001b[39m\u001b[92m+ Logging v1.11.0\u001b[39m\n  \u001b[90m[d6f4376e] \u001b[39m\u001b[92m+ Markdown v1.11.0\u001b[39m\n  \u001b[90m[a63ad114] \u001b[39m\u001b[92m+ Mmap v1.11.0\u001b[39m\n  \u001b[90m[ca575930] \u001b[39m\u001b[92m+ NetworkOptions v1.3.0\u001b[39m\n  \u001b[90m[44cfe95a] \u001b[39m\u001b[92m+ Pkg v1.12.1\u001b[39m\n  \u001b[90m[de0858da] \u001b[39m\u001b[92m+ Printf v1.11.0\u001b[39m\n  \u001b[90m[3fa0cd96] \u001b[39m\u001b[92m+ REPL v1.11.0\u001b[39m\n  \u001b[90m[9a3f8284] \u001b[39m\u001b[92m+ Random v1.11.0\u001b[39m\n  \u001b[90m[ea8e919c] \u001b[39m\u001b[92m+ SHA v0.7.0\u001b[39m\n  \u001b[90m[9e88b42a] \u001b[39m\u001b[93m~ Serialization ⇒ v1.11.0\u001b[39m\n  \u001b[90m[6462fe0b] \u001b[39m\u001b[92m+ Sockets v1.11.0\u001b[39m\n  \u001b[90m[2f01184e] \u001b[39m\u001b[92m+ SparseArrays v1.12.0\u001b[39m\n  \u001b[90m[f489334b] \u001b[39m\u001b[92m+ StyledStrings v1.11.0\u001b[39m\n  \u001b[90m[fa267f1f] \u001b[39m\u001b[92m+ TOML v1.0.3\u001b[39m\n  \u001b[90m[a4e569a6] \u001b[39m\u001b[92m+ Tar v1.10.0\u001b[39m\n  \u001b[90m[8dfed614] \u001b[39m\u001b[92m+ Test v1.11.0\u001b[39m\n  \u001b[90m[cf7118a7] \u001b[39m\u001b[92m+ UUIDs v1.11.0\u001b[39m\n  \u001b[90m[4ec0a83e] \u001b[39m\u001b[92m+ Unicode v1.11.0\u001b[39m\n  \u001b[90m[e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v1.3.0+1\u001b[39m\n  \u001b[90m[deac9b47] \u001b[39m\u001b[92m+ LibCURL_jll v8.15.0+0\u001b[39m\n  \u001b[90m[e37daf67] \u001b[39m\u001b[92m+ LibGit2_jll v1.9.0+0\u001b[39m\n  \u001b[90m[29816b5a] \u001b[39m\u001b[92m+ LibSSH2_jll v1.11.3+1\u001b[39m\n  \u001b[90m[14a3606d] \u001b[39m\u001b[92m+ MozillaCACerts_jll v2025.11.4\u001b[39m\n  \u001b[90m[4536629a] \u001b[39m\u001b[92m+ OpenBLAS_jll v0.3.29+0\u001b[39m\n  \u001b[90m[05823500] \u001b[39m\u001b[92m+ OpenLibm_jll v0.8.7+0\u001b[39m\n  \u001b[90m[458c3c95] \u001b[39m\u001b[93m~ OpenSSL_jll ⇒ v3.5.4+0\u001b[39m\n  \u001b[90m[bea87d4a] \u001b[39m\u001b[92m+ SuiteSparse_jll v7.8.3+2\u001b[39m\n  \u001b[90m[83775a58] \u001b[39m\u001b[92m+ Zlib_jll v1.3.1+2\u001b[39m\n  \u001b[90m[8e850b90] \u001b[39m\u001b[92m+ libblastrampoline_jll v5.15.0+0\u001b[39m\n  \u001b[90m[8e850ede] \u001b[39m\u001b[92m+ nghttp2_jll v1.64.0+1\u001b[39m\n  \u001b[90m[3f19e933] \u001b[39m\u001b[92m+ p7zip_jll v17.7.0+0\u001b[39m\n\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[32m⌃\u001b[39m and \u001b[33m⌅\u001b[39m have new versions available. Those with \u001b[32m⌃\u001b[39m may be upgradable, but those with \u001b[33m⌅\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated -m`\n\u001b[32m\u001b[1m    Building\u001b[22m\u001b[39m Conda → `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/8f06b0cfa4c514c7b9546756dbae91fcfbc92dc9/build.log`\n\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m packages...\n    472.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDataValueInterfaces\u001b[39m\n    464.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mIteratorInterfaceExtensions\u001b[39m\n    472.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mReexport\u001b[39m\n    512.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mUnsafePointers\u001b[39m\n    576.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDataAPI\u001b[39m\n    579.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTricks\u001b[39m\n    591.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mEnumX\u001b[39m\n    616.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStatistics\u001b[39m\n    719.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStatsAPI\u001b[39m\n    786.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mPositiveFactorizations\u001b[39m\n    451.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTestItems\u001b[39m\n    947.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDocStringExtensions\u001b[39m\n   1012.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays\u001b[39m\n    463.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mPtrArrays\u001b[39m\n    595.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mVersionParsing\u001b[39m\n    595.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mInterfaces\u001b[39m\n    693.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStaticArraysCore\u001b[39m\n    909.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mADTypes\u001b[39m\n    731.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mRequires\u001b[39m\n    571.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mScratch\u001b[39m\n   1069.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mProgressMeter\u001b[39m\n   1636.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mOrderedCollections\u001b[39m\n    508.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mPidfile\u001b[39m\n    450.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTableTraits\u001b[39m\n   1697.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mConstructionBase\u001b[39m\n    721.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mScientificTypesBase\u001b[39m\n    797.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mPreferences\u001b[39m\n    577.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStatistics → SparseArraysExt\u001b[39m\n    528.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays → FillArraysStatisticsExt\u001b[39m\n   2122.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mIrrationalConstants\u001b[39m\n    510.3 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDiffResults\u001b[39m\n    635.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays → FillArraysSparseArraysExt\u001b[39m\n    602.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAliasTables\u001b[39m\n    523.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAdapt\u001b[39m\n    507.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mConstructionBase → ConstructionBaseLinearAlgebraExt\u001b[39m\n    446.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mADTypes → ADTypesConstructionBaseExt\u001b[39m\n   1857.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mNaNMath\u001b[39m\n    487.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mPrecompileTools\u001b[39m\n    512.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mJLLWrappers\u001b[39m\n    567.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStatisticalTraits\u001b[39m\n   2040.5 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStructTypes\u001b[39m\n   1898.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mCompat\u001b[39m\n    641.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mAdapt → AdaptSparseArraysExt\u001b[39m\n    780.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mLogExpFunctions\u001b[39m\n   1632.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mLossFunctions\u001b[39m\n   1846.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMissings\u001b[39m\n   3226.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMacroTools\u001b[39m\n    402.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mCompat → CompatLinearAlgebraExt\u001b[39m\n    635.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mOpenSpecFun_jll\u001b[39m\n   1836.3 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDifferentiationInterface\u001b[39m\n   1705.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDataStructures\u001b[39m\n   1725.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mTables\u001b[39m\n    869.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMLJModelInterface\u001b[39m\n   1016.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mpixi_jll\u001b[39m\n   1020.3 ms\u001b[32m  ✓ \u001b[39m\u001b[90mmicromamba_jll\u001b[39m\n    518.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mCommonSubexpressions\u001b[39m\n    514.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mSortingAlgorithms\u001b[39m\n    565.3 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDifferentiationInterface → DifferentiationInterfaceSparseArraysExt\u001b[39m\n   1676.3 ms\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface\u001b[39m\n   1081.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mSetfield\u001b[39m\n    382.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface → ArrayInterfaceStaticArraysCoreExt\u001b[39m\n    512.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface → ArrayInterfaceSparseArraysExt\u001b[39m\n    610.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFiniteDiff\u001b[39m\n   2176.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDispatchDoctor\u001b[39m\n   2362.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mChainRulesCore\u001b[39m\n    497.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDifferentiationInterface → DifferentiationInterfaceFiniteDiffExt\u001b[39m\n    577.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mFiniteDiff → FiniteDiffSparseArraysExt\u001b[39m\n   2019.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mMicroMamba\u001b[39m\n   2533.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mSpecialFunctions\u001b[39m\n    425.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mADTypes → ADTypesChainRulesCoreExt\u001b[39m\n    462.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDifferentiationInterface → DifferentiationInterfaceChainRulesCoreExt\u001b[39m\n    466.9 ms\u001b[32m  ✓ \u001b[39m\u001b[90mArrayInterface → ArrayInterfaceChainRulesCoreExt\u001b[39m\n    534.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDispatchDoctor → DispatchDoctorChainRulesCoreExt\u001b[39m\n    614.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDiffRules\u001b[39m\n   1255.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mLogExpFunctions → LogExpFunctionsChainRulesCoreExt\u001b[39m\n   1471.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mChainRulesCore → ChainRulesCoreSparseArraysExt\u001b[39m\n   3103.7 ms\u001b[32m  ✓ \u001b[39m\u001b[90mStatsBase\u001b[39m\n   1479.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mSpecialFunctions → SpecialFunctionsChainRulesCoreExt\u001b[39m\n   3754.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mForwardDiff\u001b[39m\n   8317.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mParsers\u001b[39m\n   6117.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDynamicQuantities\u001b[39m\n   1512.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDifferentiationInterface → DifferentiationInterfaceForwardDiffExt\u001b[39m\n    709.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDynamicQuantities → DynamicQuantitiesLinearAlgebraExt\u001b[39m\n   1403.8 ms\u001b[32m  ✓ \u001b[39m\u001b[90mJSON\u001b[39m\n    763.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mNLSolversBase\u001b[39m\n    696.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mConda\u001b[39m\n   1782.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mLineSearches\u001b[39m\n   1461.5 ms\u001b[32m  ✓ \u001b[39mAwkwardArray\n    410.2 ms\u001b[32m  ✓ \u001b[39mAwkwardArray → AwkwardAdaptExt\n   2720.2 ms\u001b[32m  ✓ \u001b[39m\u001b[90mOptim\u001b[39m\n   6353.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mJSON3\u001b[39m\n  14031.6 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDynamicExpressions\u001b[39m\n   3840.1 ms\u001b[32m  ✓ \u001b[39m\u001b[90mCondaPkg\u001b[39m\n   1528.4 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDynamicExpressions → DynamicExpressionsOptimExt\u001b[39m\n   2825.0 ms\u001b[32m  ✓ \u001b[39m\u001b[90mDynamicDiff\u001b[39m\n  15164.1 ms\u001b[32m  ✓ \u001b[39mPythonCall\n   1477.9 ms\u001b[32m  ✓ \u001b[39mAwkwardArray → AwkwardPythonCallExt\n  53584.6 ms\u001b[32m  ✓ \u001b[39mSymbolicRegression\n   2830.2 ms\u001b[32m  ✓ \u001b[39mSymbolicRegression → SymbolicRegressionJSON3Ext\n  99 dependencies successfully precompiled in 80 seconds. 38 already precompiled.\n  \u001b[33m2\u001b[39m dependencies had output during precompilation:\u001b[33m\n┌ \u001b[39mMicroMamba\u001b[33m\n│  \u001b[39m\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: micromamba\u001b[33m\n└  \u001b[39m\u001b[33m\n┌ \u001b[39mCondaPkg\u001b[33m\n│  \u001b[39m\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: pixi\u001b[33m\n└  \u001b[39m\n\u001b[36m\u001b[1m     Project\u001b[22m\u001b[39m No packages added to or removed from `/srv/conda/envs/notebook/julia_env/Project.toml`\n\u001b[36m\u001b[1m    Manifest\u001b[22m\u001b[39m No packages added to or removed from `/srv/conda/envs/notebook/julia_env/Manifest.toml`\n"},{"name":"stdout","output_type":"stream","text":"Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"}],"execution_count":39},{"id":"67fcf1a1-677d-4e85-af95-277af37ecacb","cell_type":"code","source":"features_sr = [\n    \"beta_eff\",\n    \"Q ( 2 n )\"\n]\n\nsr_df = switch_df[switch_df[\"reliable\"] == True].copy()\n\nX_sr = sr_df[features_sr].values\ny_sr = sr_df[\"x_switch_mean\"].values\n\nprint(\"Dataset size:\", X_sr.shape)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Dataset size: (137, 2)\n"}],"execution_count":45},{"id":"504b5d39-b2bd-495e-8600-0fa2a4da3ab7","cell_type":"code","source":"from pysr import PySRRegressor\nimport multiprocessing\n\nn_cores = multiprocessing.cpu_count()\nprint(\"Detected CPU cores:\", n_cores)\n\nmodel = PySRRegressor(\n    niterations=2000,              # we don't need 40k\n    populations=40,\n    population_size=1000,\n    ncycles_per_iteration=500,\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"square\"],\n    maxsize=15,\n    model_selection=\"best\",\n    loss=\"loss(x, y) = (x - y)^2\",\n    procs=n_cores,                 # THIS is critical\n    progress=True,\n    verbosity=1,\n)\n\nmodel.fit(X_sr, y_sr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d1005951-dcb1-431f-9a01-a9c4a4f9d2b1","cell_type":"code","source":"# Build one row per reaction with structural features\n\nstructure_df = df.groupby(\"Reaction\").first().reset_index()\n\nstructure_df[\"beta_eff\"] = (\n    abs(structure_df[\"β P\"]) + abs(structure_df[\"β T\"])\n)\n\nstructure_df = structure_df[[\n    \"Reaction\",\n    \"beta_eff\",\n    \"Q ( 2 n )\"\n]]","metadata":{"trusted":true},"outputs":[],"execution_count":42},{"id":"f8f0b80a-3bea-4be6-9661-b766623c3ffb","cell_type":"code","source":"switch_df = switch_df.merge(\n    structure_df,\n    on=\"Reaction\",\n    how=\"left\"\n)\n\nprint(switch_df[[\"Reaction\", \"beta_eff\", \"Q ( 2 n )\"]].head())","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"        Reaction  beta_eff  Q ( 2 n )\n0    12 C + 89 Y     0.355      -7.71\n1   12 C + 92 Zr     0.373      -2.71\n2  12 C + 144 Sm     0.320      -6.00\n3  12 C + 152 Sm     0.563      -0.73\n4  12 C + 154 Sm     0.590      -0.71\n"}],"execution_count":43},{"id":"b95118c0-1546-49af-8753-54828304dd1b","cell_type":"code","source":"features_sr = [\n    \"beta_eff\",\n    \"Q ( 2 n )\"\n]\n\nsr_df = switch_df[switch_df[\"reliable\"] == True].copy()\n\nX_sr = sr_df[features_sr].values\ny_sr = sr_df[\"x_switch_mean\"].values\n\nprint(\"Dataset size:\", X_sr.shape)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Dataset size: (137, 2)\n"}],"execution_count":44},{"id":"b0b208a5-ddcb-475f-a96f-ea0cd452bfaf","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}