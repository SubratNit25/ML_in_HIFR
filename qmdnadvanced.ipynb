{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.14"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"313b86d4-f99a-4f9d-9f26-1964ba82c207","cell_type":"code","source":"import os\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom pathlib import Path\nimport pennylane as qml\nimport gc\nimport subprocess\nfrom datetime import datetime\n\n# -------------------- GPU Selection --------------------\ndef get_free_gpu():\n    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n                            capture_output=True, text=True)\n    memory = [int(x) for x in result.stdout.strip().split('\\n')]\n    print(\"GPU memory usage (MiB):\", memory)\n    best_gpu = np.argmin(memory)   # pick GPU with least used memory\n    return best_gpu\n\nGPU_ID = get_free_gpu()   # auto-select, or set manually\n# GPU_ID = 5   # <-- uncomment to set manually\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)\nprint(f\"\\nUsing GPU {GPU_ID}\")\n\n# -------------------- Device --------------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Torch Device:\", DEVICE)\n\n# -------------------- Cleanup --------------------\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"GPU memory usage (MiB): [8615, 219, 7399, 3, 3, 3, 3, 3]\n\nUsing GPU 3\nTorch Device: cuda\n"},{"data":{"text/plain":"20"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"execution_count":1},{"id":"85a98453-9404-4477-87b1-e6bba1c79691","cell_type":"code","source":"# -------------------- Load Data --------------------\nDRIVE_URL = \"https://drive.google.com/uc?id=1PS0eB8dx8VMzVvxNUc6wBzsMRkEKJjWI\"\ndf = pd.read_csv(DRIVE_URL)\nprint(\"Total rows:\", len(df))\nprint(\"Total reactions:\", df[\"Reaction\"].nunique())\n\n# -------------------- Physics Feature Engineering --------------------\nM_p = 938.272088\nM_n = 939.565420\nepsilon = 1e-30\nLN10 = np.log(10.0)\n\ndef get_nucleon_mass(Z, A):\n    return Z * M_p + (A - Z) * M_n\n\nmass1 = df.apply(lambda r: get_nucleon_mass(r[\"Z1\"], r[\"A1\"]), axis=1).values\nmass2 = df.apply(lambda r: get_nucleon_mass(r[\"Z2\"], r[\"A2\"]), axis=1).values\nmu_MeVc2 = (mass1 * mass2) / (mass1 + mass2 + 1e-12)\nEcm = df[\"E c.m.\"].astype(float).values\nv_over_c = np.sqrt(np.clip(2 * Ecm / (mu_MeVc2 + epsilon), 0, np.inf))\ne2_hbar_c = 1 / 137.035999\ndf[\"eta\"] = (df[\"Z1\"] * df[\"Z2\"]) / (e2_hbar_c * (v_over_c + 1e-16))\n\nlog10_sigma_exp = np.log10(np.clip(df[\"Ïƒ\"], 1e-30, np.inf))\nlog10_sigma_cal = np.log10(np.clip(df[\"Ïƒ cal\"], 1e-30, np.inf))\nlog10_Ecm = np.log10(np.clip(df[\"E c.m.\"], 1e-30, np.inf))\nlog10_exp_term = (2 * np.pi * df[\"eta\"]) / LN10\n\ndf[\"log10_S_exp\"] = log10_sigma_exp + log10_Ecm + log10_exp_term\ndf[\"log10_S_cal\"] = log10_sigma_cal + log10_Ecm + log10_exp_term\ndf[\"delta_log10_S\"] = df[\"log10_S_exp\"] - df[\"log10_S_cal\"]\n\ndf[\"N1\"] = df[\"A1\"] - df[\"Z1\"]\ndf[\"N2\"] = df[\"A2\"] - df[\"Z2\"]\ndf[\"Z1Z2_over_Ecm\"] = (df[\"Z1\"] * df[\"Z2\"]) / (df[\"E c.m.\"] + epsilon)\n\nMAGIC = np.array([2, 8, 20, 28, 50, 82, 126])\ndef magic_dist(arr):\n    return np.min(np.abs(arr[:, None] - MAGIC[None, :]), axis=1)\n\ndf[\"magic_dist_Z1\"] = magic_dist(df[\"Z1\"].values)\ndf[\"magic_dist_N1\"] = magic_dist(df[\"N1\"].values)\ndf[\"magic_dist_Z2\"] = magic_dist(df[\"Z2\"].values)\ndf[\"magic_dist_N2\"] = magic_dist(df[\"N2\"].values)\n\nprint(\"Feature engineering complete.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Total rows: 3532\nTotal reactions: 213\nFeature engineering complete.\n"}],"execution_count":2},{"id":"21d139c1-da5d-4bb3-bffc-f481e4889155","cell_type":"code","source":"# -------------------- 29 Features --------------------\nfeatures_train = [\n    'E c.m.', 'Z1', 'N1', 'A1',\n    'Z2', 'N2', 'A2', 'Q ( 2 n )',\n    'Z1Z2_over_Ecm',\n    'magic_dist_Z1','magic_dist_N1','magic_dist_Z2','magic_dist_N2',\n    'Z3','N3','A3','Î² P','Î² T','R B','Ä§ Ï‰',\n    'Projectile_Mass_Actual', 'Target_Mass_Actual', 'Compound_Nucleus_Mass_Actual',\n    'Compound_Nucleus_Sp','Compound_Nucleus_Sn',\n    'Projectile_Binding_Energy','Target_Binding_Energy',\n    'Compound_Nucleus_Binding_Energy','Compound_Nucleus_S2n'\n]\n\n# -------------------- Load Reaction Split --------------------\nBASE_DIR = \"mdn_70_10_20_optimized\"\ntrain_reacts = pd.read_csv(os.path.join(BASE_DIR, \"train_reactions.csv\"))[\"Reaction\"].values\nval_reacts   = pd.read_csv(os.path.join(BASE_DIR, \"val_reactions.csv\"))[\"Reaction\"].values\ntest_reacts  = pd.read_csv(os.path.join(BASE_DIR, \"test_reactions.csv\"))[\"Reaction\"].values\n\nprint(\"Train reactions:\", len(train_reacts))\nprint(\"Val reactions:\", len(val_reacts))\nprint(\"Test reactions:\", len(test_reacts))\n\ntrain_mask = df[\"Reaction\"].isin(train_reacts)\nval_mask   = df[\"Reaction\"].isin(val_reacts)\ntest_mask  = df[\"Reaction\"].isin(test_reacts)\n\n# -------------------- Prepare Arrays --------------------\nX_train_full = df.loc[train_mask | val_mask, features_train].values.astype(np.float32)\ny_train_full = df.loc[train_mask | val_mask, \"delta_log10_S\"].values.astype(np.float32).reshape(-1,1)\nX_test = df.loc[test_mask, features_train].values.astype(np.float32)\ny_test = df.loc[test_mask, \"delta_log10_S\"].values.astype(np.float32).reshape(-1,1)\n\n# -------------------- Standardize --------------------\nscaler = StandardScaler().fit(X_train_full)\nX_train_full_s = scaler.transform(X_train_full)\nX_test_s = scaler.transform(X_test)\n\nprint(\"Training samples:\", X_train_full_s.shape[0])\nprint(\"Test samples:\", X_test_s.shape[0])","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train reactions: 149\nVal reactions: 21\nTest reactions: 43\nTraining samples: 2847\nTest samples: 685\n"}],"execution_count":3},{"id":"608b8884-a5c1-4cfa-9ca2-755decf6622c","cell_type":"code","source":"# ==================================================\n# ADJUST THESE PARAMETERS FOR EACH EXPERIMENT\n# ==================================================\nn_qubits = 15\nn_layers = 14\nN_COMPONENTS = 5\nBATCH_SIZE = 16\nACCUMULATION_STEPS = 2          # effective batch = 32\nLAMBDA_ENTROPY = 0.05            # set to 0 to disable entropy\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-4\nMAX_EPOCHS = 150\nPATIENCE = 15\nSEED = 42\n\n# Effective batch size for logging\neffective_batch = BATCH_SIZE * ACCUMULATION_STEPS\nprint(f\"Qubits: {n_qubits}, Layers: {n_layers}, Components: {N_COMPONENTS}\")\nprint(f\"Physical batch: {BATCH_SIZE}, Accumulation steps: {ACCUMULATION_STEPS}\")\nprint(f\"Effective batch size: {effective_batch}\")\n\n# ---- Set seeds ----\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\n# ---- Automatic naming ----\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodel_name = f\"qmdn_q{n_qubits}_l{n_layers}_b{BATCH_SIZE}_acc{ACCUMULATION_STEPS}_s{SEED}_{timestamp}\"\nprint(\"Model name:\", model_name)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Qubits: 15, Layers: 14, Components: 5\nPhysical batch: 16, Accumulation steps: 2\nEffective batch size: 32\nModel name: qmdn_q15_l14_b16_acc2_s42_20260228_124052\n"}],"execution_count":4},{"id":"c20500a6-7423-41bb-aa9e-31386e2dfd42","cell_type":"code","source":"dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\nprint(\"Using PennyLane device: default.qubit\")\n\n@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\ndef qnode(weights, inputs):\n    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation=\"X\")\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return qml.probs(wires=range(n_qubits))   # shape (batch, 2**n_qubits)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Using PennyLane device: default.qubit\n"}],"execution_count":5},{"id":"bba2c199-a823-4e19-83b7-fc981def9043","cell_type":"code","source":"class QMDN(nn.Module):\n    def __init__(self, in_dim, n_components=N_COMPONENTS, hidden_dim=32):\n        super().__init__()\n        self.n_components = n_components\n        self.encoder = nn.Linear(in_dim, n_qubits)\n        self.weight = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, 3))\n\n        self.fc1 = nn.Linear(2**n_qubits, hidden_dim)\n        self.fc_pi = nn.Linear(hidden_dim, n_components)\n        self.fc_mu = nn.Linear(hidden_dim, n_components)\n        self.fc_sigma = nn.Linear(hidden_dim, n_components)\n\n    def forward(self, x):\n        x = x.float()\n        x_enc = torch.tanh(self.encoder(x))\n        probs = qnode(self.weight, x_enc)\n        probs = probs.to(x.dtype)          # cast to float32\n        h = torch.relu(self.fc1(probs))\n        pi_logits = self.fc_pi(h)\n        mu = self.fc_mu(h)\n        sigma_raw = self.fc_sigma(h)\n\n        pi = F.softmax(pi_logits, dim=1)\n        sigma = F.softplus(sigma_raw) + 1e-6\n        return pi, mu, sigma","metadata":{"trusted":true},"outputs":[],"execution_count":6},{"id":"359dc232-1ec0-4be3-8c20-07509846977c","cell_type":"code","source":"def mdn_loss(pi, mu, sigma, y):\n    y = y.float()\n    yexp = y.repeat(1, mu.shape[1])\n    log_gauss = (\n        -0.5 * ((yexp - mu) / sigma) ** 2\n        - torch.log(sigma)\n        - 0.5 * np.log(2 * np.pi)\n    )\n    log_mix = torch.logsumexp(torch.log(pi + 1e-12) + log_gauss, dim=1)\n    return -log_mix.mean()\n\ndef mdn_loss_entropy(pi, mu, sigma, y, lambda_entropy):\n    nll = mdn_loss(pi, mu, sigma, y)\n    entropy = -torch.sum(pi * torch.log(pi + 1e-12), dim=1).mean()\n    return nll - lambda_entropy * entropy\n\ndef make_loader(X, y, batch=BATCH_SIZE, shuffle=True):\n    return DataLoader(\n        TensorDataset(torch.tensor(X, dtype=torch.float32),\n                      torch.tensor(y, dtype=torch.float32)),\n        batch_size=batch,\n        shuffle=shuffle,\n        num_workers=0,\n        pin_memory=True if DEVICE.type == 'cuda' else False\n    )\n\n# Train/Validation split\nval_size = int(0.1 * len(X_train_full_s))\nindices = np.random.permutation(len(X_train_full_s))\ntrain_idx = indices[val_size:]\nval_idx   = indices[:val_size]\n\ntrain_loader = make_loader(X_train_full_s[train_idx], y_train_full[train_idx])\nval_loader   = make_loader(X_train_full_s[val_idx],   y_train_full[val_idx], shuffle=False)\ntest_loader  = make_loader(X_test_s, y_test, shuffle=False)\n\nprint(\"Train batches:\", len(train_loader))\nprint(\"Val batches:\", len(val_loader))\nprint(\"Test batches:\", len(test_loader))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train batches: 161\nVal batches: 18\nTest batches: 43\n"}],"execution_count":7},{"id":"68e4b0ff-9aee-405e-8eba-a3fe49a08b5b","cell_type":"code","source":"model = QMDN(in_dim=X_train_full_s.shape[1]).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\nbest_val_nll = float(\"inf\")\npatience_counter = 0\n\n# Tracking lists\nepoch_pi_means = []      # average Ï€ per epoch\nepoch_val_nll = []        # validation NLL per epoch\ntrain_losses = []         # optional\nval_losses = []           # optional","metadata":{"trusted":true},"outputs":[],"execution_count":8},{"id":"cc9af60f-b57d-4a4d-ab8f-e8928bfc8b06","cell_type":"code","source":"# =========================================================\n# Training Loop with Perâ€‘Epoch Component Monitoring (SAVED)\n# =========================================================\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\nbest_val_nll = float(\"inf\")\nbest_epoch = 0                     # <-- ADD THIS LINE\npatience_counter = 0\n\n# ---- Tracking lists ----\nepoch_pi_means = []      # store average Ï€ per epoch\nepoch_val_nll = []        # store validation NLL\ntrain_losses = []         # optional\nval_losses = []           # optional\n\nprint(f\"\\nðŸš€ Starting training: {model_name}\\n\")\nfor epoch in range(MAX_EPOCHS):\n    model.train()\n    train_loss = 0.0\n    train_nll = 0.0\n    optimizer.zero_grad()\n    start_time = time.time()\n\n    for batch_idx, (xb, yb) in enumerate(train_loader):\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n\n        pi, mu, sigma = model(xb)\n\n        if LAMBDA_ENTROPY > 0:\n            loss = mdn_loss_entropy(pi, mu, sigma, yb, LAMBDA_ENTROPY) / ACCUMULATION_STEPS\n        else:\n            loss = mdn_loss(pi, mu, sigma, yb) / ACCUMULATION_STEPS\n\n        nll = mdn_loss(pi, mu, sigma, yb) / ACCUMULATION_STEPS   # scaled for logging\n\n        loss.backward()\n\n        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        train_loss += loss.item() * xb.size(0) * ACCUMULATION_STEPS\n        train_nll  += nll.item()  * xb.size(0) * ACCUMULATION_STEPS\n\n    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n        optimizer.step()\n        optimizer.zero_grad()\n\n    train_loss /= len(train_loader.dataset)\n    train_nll  /= len(train_loader.dataset)\n    train_losses.append(train_loss)\n\n    # ---------- Validation ----------\n    model.eval()\n    val_loss = 0.0\n    val_nll = 0.0\n    pi_accum = []\n\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            pi, mu, sigma = model(xb)\n\n            if LAMBDA_ENTROPY > 0:\n                vloss = mdn_loss_entropy(pi, mu, sigma, yb, LAMBDA_ENTROPY)\n            else:\n                vloss = mdn_loss(pi, mu, sigma, yb)\n\n            val_loss += vloss.item() * xb.size(0)\n            val_nll  += mdn_loss(pi, mu, sigma, yb).item() * xb.size(0)\n            pi_accum.append(pi.cpu())\n\n    val_loss /= len(val_loader.dataset)\n    val_nll  /= len(val_loader.dataset)\n    val_losses.append(val_loss)\n\n    # Average Ï€ across validation set\n    pi_avg = torch.cat(pi_accum, dim=0).mean(dim=0).numpy()\n    epoch_pi_means.append(pi_avg)\n    epoch_val_nll.append(val_nll)\n\n    scheduler.step(val_nll)\n    epoch_time = time.time() - start_time\n    current_lr = optimizer.param_groups[0]['lr']\n\n    print(f\"E{epoch:03d} | Time: {epoch_time:.2f}s | \"\n          f\"train L={train_loss:.6f} NLL={train_nll:.6f} | \"\n          f\"val L={val_loss:.6f} NLL={val_nll:.6f} | \"\n          f\"Ï€ avg: {np.round(pi_avg,3)} | LR: {current_lr:.2e}\")\n\n    # ---------- Early Stopping ----------\n    if val_nll < best_val_nll - 1e-4:\n        best_val_nll = val_nll\n        best_epoch = epoch          # <-- record the best epoch\n        torch.save(model.state_dict(), f\"{model_name}_best.pt\")\n        patience_counter = 0\n        print(f\"  *** New best model (epoch {epoch}, val NLL: {best_val_nll:.6f}) ***\")\n    else:\n        patience_counter += 1\n        if patience_counter >= PATIENCE:\n            print(f\"ðŸ›‘ Early stopping after epoch {epoch}\")\n            break\n\nprint(\"\\nâœ… Training finished. Best val NLL:\", best_val_nll)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nðŸš€ Starting training: qmdn_q15_l14_b16_acc2_s42_20260228_124052\n\nE000 | Time: 204.74s | train L=0.434030 NLL=0.514159 | val L=0.341189 NLL=0.420613 | Ï€ avg: [0.237 0.158 0.208 0.148 0.248] | LR: 1.00e-03\n  *** New best model (val NLL: 0.420613) ***\nE001 | Time: 201.56s | train L=0.050841 NLL=0.126148 | val L=-0.071752 NLL=-0.003886 | Ï€ avg: [0.408 0.063 0.333 0.078 0.118] | LR: 1.00e-03\n  *** New best model (val NLL: -0.003886) ***\nE002 | Time: 207.79s | train L=-0.286693 NLL=-0.219086 | val L=-0.249795 NLL=-0.180798 | Ï€ avg: [0.41  0.068 0.315 0.1   0.106] | LR: 1.00e-03\n  *** New best model (val NLL: -0.180798) ***\nE003 | Time: 203.84s | train L=-0.422497 NLL=-0.352861 | val L=-0.374045 NLL=-0.305512 | Ï€ avg: [0.423 0.075 0.304 0.091 0.107] | LR: 1.00e-03\n  *** New best model (val NLL: -0.305512) ***\nE004 | Time: 201.12s | train L=-0.507093 NLL=-0.439688 | val L=-0.426090 NLL=-0.360039 | Ï€ avg: [0.449 0.071 0.304 0.077 0.099] | LR: 1.00e-03\n  *** New best model (val NLL: -0.360039) ***\nE005 | Time: 201.54s | train L=-0.552187 NLL=-0.486576 | val L=-0.452312 NLL=-0.387566 | Ï€ avg: [0.45  0.069 0.316 0.07  0.095] | LR: 1.00e-03\n  *** New best model (val NLL: -0.387566) ***\nE006 | Time: 201.42s | train L=-0.589404 NLL=-0.525306 | val L=-0.475706 NLL=-0.411601 | Ï€ avg: [0.445 0.068 0.326 0.065 0.096] | LR: 1.00e-03\n  *** New best model (val NLL: -0.411601) ***\nE007 | Time: 201.16s | train L=-0.610267 NLL=-0.547011 | val L=-0.492561 NLL=-0.429668 | Ï€ avg: [0.44  0.065 0.345 0.059 0.092] | LR: 1.00e-03\n  *** New best model (val NLL: -0.429668) ***\nE008 | Time: 201.65s | train L=-0.631345 NLL=-0.568875 | val L=-0.504736 NLL=-0.442539 | Ï€ avg: [0.449 0.064 0.34  0.056 0.091] | LR: 1.00e-03\n  *** New best model (val NLL: -0.442539) ***\nE009 | Time: 201.16s | train L=-0.652711 NLL=-0.590614 | val L=-0.500381 NLL=-0.438996 | Ï€ avg: [0.456 0.063 0.34  0.052 0.089] | LR: 1.00e-03\nE010 | Time: 201.25s | train L=-0.668617 NLL=-0.607189 | val L=-0.523270 NLL=-0.461952 | Ï€ avg: [0.46  0.064 0.334 0.05  0.091] | LR: 1.00e-03\n  *** New best model (val NLL: -0.461952) ***\nE011 | Time: 202.05s | train L=-0.689193 NLL=-0.627811 | val L=-0.518735 NLL=-0.457912 | Ï€ avg: [0.472 0.065 0.323 0.048 0.091] | LR: 1.00e-03\nE012 | Time: 200.92s | train L=-0.706828 NLL=-0.645747 | val L=-0.547579 NLL=-0.486428 | Ï€ avg: [0.475 0.069 0.312 0.048 0.096] | LR: 1.00e-03\n  *** New best model (val NLL: -0.486428) ***\nE013 | Time: 202.35s | train L=-0.723639 NLL=-0.662410 | val L=-0.540637 NLL=-0.480955 | Ï€ avg: [0.486 0.064 0.318 0.042 0.09 ] | LR: 1.00e-03\nE014 | Time: 201.67s | train L=-0.745042 NLL=-0.684456 | val L=-0.557911 NLL=-0.496541 | Ï€ avg: [0.472 0.073 0.308 0.046 0.1  ] | LR: 1.00e-03\n  *** New best model (val NLL: -0.496541) ***\nE015 | Time: 201.33s | train L=-0.760774 NLL=-0.699831 | val L=-0.571647 NLL=-0.510841 | Ï€ avg: [0.49  0.073 0.292 0.045 0.1  ] | LR: 1.00e-03\n  *** New best model (val NLL: -0.510841) ***\nE016 | Time: 201.99s | train L=-0.775776 NLL=-0.715292 | val L=-0.561193 NLL=-0.501267 | Ï€ avg: [0.503 0.073 0.283 0.042 0.099] | LR: 1.00e-03\nE017 | Time: 201.49s | train L=-0.792273 NLL=-0.731907 | val L=-0.570609 NLL=-0.511397 | Ï€ avg: [0.511 0.071 0.282 0.04  0.096] | LR: 1.00e-03\n  *** New best model (val NLL: -0.511397) ***\nE018 | Time: 201.76s | train L=-0.808382 NLL=-0.748690 | val L=-0.571449 NLL=-0.511474 | Ï€ avg: [0.504 0.076 0.277 0.043 0.101] | LR: 1.00e-03\nE019 | Time: 201.61s | train L=-0.810602 NLL=-0.751020 | val L=-0.581555 NLL=-0.522501 | Ï€ avg: [0.52  0.074 0.267 0.04  0.098] | LR: 1.00e-03\n  *** New best model (val NLL: -0.522501) ***\nE020 | Time: 201.56s | train L=-0.829757 NLL=-0.771016 | val L=-0.627968 NLL=-0.567756 | Ï€ avg: [0.501 0.078 0.275 0.043 0.103] | LR: 1.00e-03\n  *** New best model (val NLL: -0.567756) ***\nE021 | Time: 207.32s | train L=-0.847109 NLL=-0.787968 | val L=-0.624964 NLL=-0.565527 | Ï€ avg: [0.512 0.076 0.27  0.041 0.101] | LR: 1.00e-03\nE022 | Time: 205.98s | train L=-0.849010 NLL=-0.790373 | val L=-0.624402 NLL=-0.566114 | Ï€ avg: [0.53  0.074 0.26  0.039 0.097] | LR: 1.00e-03\nE023 | Time: 210.00s | train L=-0.854394 NLL=-0.795554 | val L=-0.602489 NLL=-0.544658 | Ï€ avg: [0.527 0.071 0.271 0.037 0.095] | LR: 1.00e-03\nE024 | Time: 207.85s | train L=-0.877266 NLL=-0.818985 | val L=-0.619355 NLL=-0.560289 | Ï€ avg: [0.51  0.075 0.277 0.039 0.1  ] | LR: 1.00e-03\nE025 | Time: 202.01s | train L=-0.898442 NLL=-0.839795 | val L=-0.622644 NLL=-0.564709 | Ï€ avg: [0.527 0.074 0.265 0.037 0.096] | LR: 1.00e-03\nE026 | Time: 206.08s | train L=-0.905169 NLL=-0.846760 | val L=-0.642613 NLL=-0.584689 | Ï€ avg: [0.528 0.075 0.262 0.037 0.098] | LR: 1.00e-03\n  *** New best model (val NLL: -0.584689) ***\nE027 | Time: 206.21s | train L=-0.909018 NLL=-0.850502 | val L=-0.651447 NLL=-0.593857 | Ï€ avg: [0.53  0.072 0.264 0.035 0.099] | LR: 1.00e-03\n  *** New best model (val NLL: -0.593857) ***\nE028 | Time: 206.48s | train L=-0.920082 NLL=-0.861913 | val L=-0.687376 NLL=-0.628424 | Ï€ avg: [0.513 0.077 0.267 0.038 0.105] | LR: 1.00e-03\n  *** New best model (val NLL: -0.628424) ***\nE029 | Time: 203.99s | train L=-0.931490 NLL=-0.872849 | val L=-0.676046 NLL=-0.617310 | Ï€ avg: [0.513 0.076 0.271 0.036 0.105] | LR: 1.00e-03\nE030 | Time: 213.99s | train L=-0.935136 NLL=-0.876378 | val L=-0.703892 NLL=-0.644557 | Ï€ avg: [0.507 0.08  0.267 0.038 0.108] | LR: 1.00e-03\n  *** New best model (val NLL: -0.644557) ***\nE031 | Time: 210.21s | train L=-0.946581 NLL=-0.887563 | val L=-0.676177 NLL=-0.618211 | Ï€ avg: [0.521 0.074 0.268 0.034 0.103] | LR: 1.00e-03\nE032 | Time: 206.60s | train L=-0.962908 NLL=-0.904347 | val L=-0.709250 NLL=-0.650286 | Ï€ avg: [0.508 0.078 0.271 0.037 0.106] | LR: 1.00e-03\n  *** New best model (val NLL: -0.650286) ***\nE033 | Time: 207.57s | train L=-0.973023 NLL=-0.914732 | val L=-0.678268 NLL=-0.619563 | Ï€ avg: [0.518 0.079 0.257 0.037 0.108] | LR: 1.00e-03\nE034 | Time: 201.86s | train L=-0.976703 NLL=-0.917629 | val L=-0.725087 NLL=-0.666037 | Ï€ avg: [0.511 0.081 0.261 0.037 0.11 ] | LR: 1.00e-03\n  *** New best model (val NLL: -0.666037) ***\nE035 | Time: 202.95s | train L=-0.988347 NLL=-0.929602 | val L=-0.720330 NLL=-0.660687 | Ï€ avg: [0.509 0.085 0.252 0.039 0.115] | LR: 1.00e-03\nE036 | Time: 201.60s | train L=-0.994671 NLL=-0.935914 | val L=-0.722551 NLL=-0.663397 | Ï€ avg: [0.517 0.084 0.248 0.038 0.114] | LR: 1.00e-03\nE037 | Time: 201.77s | train L=-1.007792 NLL=-0.949321 | val L=-0.677240 NLL=-0.618951 | Ï€ avg: [0.529 0.082 0.242 0.037 0.111] | LR: 1.00e-03\nE038 | Time: 201.70s | train L=-1.005247 NLL=-0.946746 | val L=-0.748102 NLL=-0.689301 | Ï€ avg: [0.517 0.083 0.254 0.038 0.109] | LR: 1.00e-03\n  *** New best model (val NLL: -0.689301) ***\nE039 | Time: 201.50s | train L=-1.007277 NLL=-0.948839 | val L=-0.764502 NLL=-0.706010 | Ï€ avg: [0.524 0.084 0.247 0.037 0.107] | LR: 1.00e-03\n  *** New best model (val NLL: -0.706010) ***\nE040 | Time: 201.65s | train L=-1.013525 NLL=-0.954886 | val L=-0.754181 NLL=-0.695620 | Ï€ avg: [0.522 0.085 0.246 0.037 0.109] | LR: 1.00e-03\nE041 | Time: 201.32s | train L=-1.028053 NLL=-0.969132 | val L=-0.771716 NLL=-0.712649 | Ï€ avg: [0.516 0.088 0.247 0.038 0.111] | LR: 1.00e-03\n  *** New best model (val NLL: -0.712649) ***\nE042 | Time: 201.37s | train L=-1.038618 NLL=-0.979336 | val L=-0.769920 NLL=-0.710721 | Ï€ avg: [0.518 0.091 0.24  0.037 0.114] | LR: 1.00e-03\nE043 | Time: 205.58s | train L=-1.038408 NLL=-0.979251 | val L=-0.713300 NLL=-0.654364 | Ï€ avg: [0.523 0.091 0.237 0.036 0.113] | LR: 1.00e-03\nE044 | Time: 210.61s | train L=-1.033611 NLL=-0.973570 | val L=-0.786895 NLL=-0.727377 | Ï€ avg: [0.516 0.093 0.24  0.036 0.115] | LR: 1.00e-03\n  *** New best model (val NLL: -0.727377) ***\nE045 | Time: 200.30s | train L=-1.057127 NLL=-0.997172 | val L=-0.749559 NLL=-0.689197 | Ï€ avg: [0.508 0.1   0.234 0.037 0.122] | LR: 1.00e-03\nE046 | Time: 198.83s | train L=-1.068726 NLL=-1.008326 | val L=-0.803760 NLL=-0.742997 | Ï€ avg: [0.506 0.107 0.224 0.039 0.123] | LR: 1.00e-03\n  *** New best model (val NLL: -0.742997) ***\nE047 | Time: 198.03s | train L=-1.073704 NLL=-1.013049 | val L=-0.805432 NLL=-0.744403 | Ï€ avg: [0.505 0.113 0.221 0.038 0.124] | LR: 1.00e-03\n  *** New best model (val NLL: -0.744403) ***\nE048 | Time: 197.95s | train L=-1.073479 NLL=-1.012363 | val L=-0.772595 NLL=-0.710592 | Ï€ avg: [0.492 0.119 0.224 0.039 0.126] | LR: 1.00e-03\nE049 | Time: 198.40s | train L=-1.073151 NLL=-1.011198 | val L=-0.819176 NLL=-0.755988 | Ï€ avg: [0.474 0.129 0.23  0.04  0.127] | LR: 1.00e-03\n  *** New best model (val NLL: -0.755988) ***\nE050 | Time: 198.02s | train L=-1.075691 NLL=-1.012759 | val L=-0.805801 NLL=-0.741569 | Ï€ avg: [0.458 0.142 0.231 0.04  0.129] | LR: 1.00e-03\nE051 | Time: 197.92s | train L=-1.097917 NLL=-1.034381 | val L=-0.809805 NLL=-0.746326 | Ï€ avg: [0.472 0.151 0.214 0.039 0.124] | LR: 1.00e-03\nE052 | Time: 197.89s | train L=-1.097505 NLL=-1.034275 | val L=-0.776174 NLL=-0.711936 | Ï€ avg: [0.468 0.163 0.202 0.04  0.127] | LR: 1.00e-03\nE053 | Time: 197.85s | train L=-1.102700 NLL=-1.038599 | val L=-0.852957 NLL=-0.787963 | Ï€ avg: [0.453 0.183 0.199 0.04  0.125] | LR: 1.00e-03\n  *** New best model (val NLL: -0.787963) ***\nE054 | Time: 197.80s | train L=-1.102852 NLL=-1.037951 | val L=-0.860797 NLL=-0.794870 | Ï€ avg: [0.434 0.196 0.203 0.042 0.125] | LR: 1.00e-03\n  *** New best model (val NLL: -0.794870) ***\nE055 | Time: 197.96s | train L=-1.129523 NLL=-1.064668 | val L=-0.867077 NLL=-0.802235 | Ï€ avg: [0.45  0.203 0.187 0.04  0.119] | LR: 1.00e-03\n  *** New best model (val NLL: -0.802235) ***\nE056 | Time: 198.14s | train L=-1.127317 NLL=-1.062378 | val L=-0.859396 NLL=-0.793578 | Ï€ avg: [0.433 0.209 0.192 0.042 0.124] | LR: 1.00e-03\nE057 | Time: 198.49s | train L=-1.138084 NLL=-1.072604 | val L=-0.833926 NLL=-0.768592 | Ï€ avg: [0.44  0.216 0.183 0.041 0.119] | LR: 1.00e-03\nE058 | Time: 198.37s | train L=-1.123153 NLL=-1.057594 | val L=-0.858725 NLL=-0.792020 | Ï€ avg: [0.414 0.227 0.192 0.043 0.123] | LR: 1.00e-03\nE059 | Time: 198.15s | train L=-1.123255 NLL=-1.056991 | val L=-0.870946 NLL=-0.804340 | Ï€ avg: [0.417 0.217 0.198 0.042 0.126] | LR: 1.00e-03\n  *** New best model (val NLL: -0.804340) ***\nE060 | Time: 197.51s | train L=-1.147297 NLL=-1.081488 | val L=-0.875987 NLL=-0.810273 | Ï€ avg: [0.43  0.225 0.184 0.041 0.12 ] | LR: 1.00e-03\n  *** New best model (val NLL: -0.810273) ***\nE061 | Time: 199.01s | train L=-1.144383 NLL=-1.078711 | val L=-0.823461 NLL=-0.757148 | Ï€ avg: [0.414 0.231 0.192 0.041 0.122] | LR: 1.00e-03\nE062 | Time: 197.75s | train L=-1.134854 NLL=-1.068884 | val L=-0.906969 NLL=-0.839534 | Ï€ avg: [0.396 0.231 0.198 0.045 0.128] | LR: 1.00e-03\n  *** New best model (val NLL: -0.839534) ***\nE063 | Time: 197.72s | train L=-1.153512 NLL=-1.087636 | val L=-0.875206 NLL=-0.809074 | Ï€ avg: [0.42  0.227 0.187 0.043 0.123] | LR: 1.00e-03\nE064 | Time: 198.04s | train L=-1.150249 NLL=-1.084356 | val L=-0.837681 NLL=-0.771345 | Ï€ avg: [0.414 0.233 0.19  0.042 0.122] | LR: 1.00e-03\nE065 | Time: 197.67s | train L=-1.155287 NLL=-1.088960 | val L=-0.891008 NLL=-0.823865 | Ï€ avg: [0.397 0.244 0.189 0.044 0.126] | LR: 1.00e-03\nE066 | Time: 197.87s | train L=-1.161531 NLL=-1.095014 | val L=-0.878936 NLL=-0.812534 | Ï€ avg: [0.41  0.245 0.18  0.042 0.123] | LR: 1.00e-03\nE067 | Time: 197.95s | train L=-1.140243 NLL=-1.073970 | val L=-0.931996 NLL=-0.864157 | Ï€ avg: [0.382 0.256 0.188 0.045 0.129] | LR: 1.00e-03\n  *** New best model (val NLL: -0.864157) ***\nE068 | Time: 198.45s | train L=-1.163810 NLL=-1.096983 | val L=-0.912887 NLL=-0.845797 | Ï€ avg: [0.392 0.26  0.178 0.045 0.125] | LR: 1.00e-03\nE069 | Time: 198.19s | train L=-1.194676 NLL=-1.128326 | val L=-0.931141 NLL=-0.864464 | Ï€ avg: [0.4   0.258 0.172 0.044 0.126] | LR: 1.00e-03\n  *** New best model (val NLL: -0.864464) ***\nE070 | Time: 198.51s | train L=-1.190221 NLL=-1.124073 | val L=-0.861609 NLL=-0.794293 | Ï€ avg: [0.382 0.274 0.17  0.046 0.129] | LR: 1.00e-03\nE071 | Time: 197.91s | train L=-1.180094 NLL=-1.113480 | val L=-0.918717 NLL=-0.851679 | Ï€ avg: [0.392 0.263 0.174 0.044 0.127] | LR: 1.00e-03\nE072 | Time: 201.17s | train L=-1.194734 NLL=-1.128179 | val L=-0.911376 NLL=-0.844314 | Ï€ avg: [0.389 0.269 0.167 0.045 0.13 ] | LR: 1.00e-03\nE073 | Time: 207.63s | train L=-1.191308 NLL=-1.124697 | val L=-0.909673 NLL=-0.843013 | Ï€ avg: [0.401 0.255 0.169 0.045 0.131] | LR: 1.00e-03\nE074 | Time: 199.89s | train L=-1.192744 NLL=-1.126128 | val L=-0.953194 NLL=-0.886758 | Ï€ avg: [0.401 0.265 0.161 0.044 0.128] | LR: 1.00e-03\n  *** New best model (val NLL: -0.886758) ***\nE075 | Time: 201.43s | train L=-1.206059 NLL=-1.140430 | val L=-0.936598 NLL=-0.869753 | Ï€ avg: [0.39  0.273 0.16  0.045 0.131] | LR: 1.00e-03\nE076 | Time: 198.57s | train L=-1.204669 NLL=-1.138339 | val L=-0.809995 NLL=-0.743628 | Ï€ avg: [0.398 0.273 0.157 0.043 0.129] | LR: 1.00e-03\nE077 | Time: 198.06s | train L=-1.199465 NLL=-1.133036 | val L=-0.886396 NLL=-0.818823 | Ï€ avg: [0.367 0.291 0.164 0.046 0.132] | LR: 1.00e-03\nE078 | Time: 198.03s | train L=-1.192498 NLL=-1.125717 | val L=-0.912387 NLL=-0.844967 | Ï€ avg: [0.365 0.3   0.159 0.046 0.13 ] | LR: 1.00e-03\nE079 | Time: 197.97s | train L=-1.179852 NLL=-1.112968 | val L=-0.888795 NLL=-0.821766 | Ï€ avg: [0.381 0.282 0.162 0.044 0.13 ] | LR: 1.00e-03\nE080 | Time: 202.15s | train L=-1.223274 NLL=-1.156902 | val L=-0.938805 NLL=-0.872168 | Ï€ avg: [0.394 0.269 0.161 0.044 0.132] | LR: 5.00e-04\nE081 | Time: 204.13s | train L=-1.244102 NLL=-1.177870 | val L=-0.940118 NLL=-0.873699 | Ï€ avg: [0.396 0.272 0.155 0.044 0.133] | LR: 5.00e-04\nE082 | Time: 199.83s | train L=-1.249637 NLL=-1.183744 | val L=-0.858659 NLL=-0.792009 | Ï€ avg: [0.39  0.277 0.152 0.046 0.136] | LR: 5.00e-04\nE083 | Time: 198.92s | train L=-1.237647 NLL=-1.171345 | val L=-0.935475 NLL=-0.868629 | Ï€ avg: [0.386 0.278 0.154 0.045 0.137] | LR: 5.00e-04\nE084 | Time: 206.75s | train L=-1.242087 NLL=-1.175842 | val L=-0.937787 NLL=-0.871294 | Ï€ avg: [0.394 0.273 0.152 0.045 0.136] | LR: 5.00e-04\nE085 | Time: 198.06s | train L=-1.233587 NLL=-1.167308 | val L=-0.961658 NLL=-0.894781 | Ï€ avg: [0.385 0.278 0.155 0.045 0.138] | LR: 5.00e-04\n  *** New best model (val NLL: -0.894781) ***\nE086 | Time: 197.91s | train L=-1.259952 NLL=-1.193544 | val L=-0.976476 NLL=-0.909521 | Ï€ avg: [0.382 0.28  0.155 0.045 0.138] | LR: 5.00e-04\n  *** New best model (val NLL: -0.909521) ***\nE087 | Time: 198.03s | train L=-1.260101 NLL=-1.193838 | val L=-0.962622 NLL=-0.896002 | Ï€ avg: [0.388 0.278 0.151 0.045 0.138] | LR: 5.00e-04\nE088 | Time: 197.83s | train L=-1.269567 NLL=-1.203257 | val L=-0.953585 NLL=-0.887204 | Ï€ avg: [0.392 0.277 0.15  0.044 0.137] | LR: 5.00e-04\nE089 | Time: 197.76s | train L=-1.229264 NLL=-1.162795 | val L=-0.953059 NLL=-0.886780 | Ï€ avg: [0.395 0.275 0.148 0.044 0.137] | LR: 5.00e-04\nE090 | Time: 197.72s | train L=-1.265973 NLL=-1.199874 | val L=-0.914848 NLL=-0.848372 | Ï€ avg: [0.387 0.283 0.147 0.045 0.138] | LR: 5.00e-04\nE091 | Time: 197.95s | train L=-1.272698 NLL=-1.206519 | val L=-0.910632 NLL=-0.844084 | Ï€ avg: [0.383 0.288 0.144 0.046 0.139] | LR: 5.00e-04\nE092 | Time: 197.83s | train L=-1.276001 NLL=-1.209838 | val L=-0.952269 NLL=-0.885808 | Ï€ avg: [0.384 0.289 0.143 0.045 0.139] | LR: 2.50e-04\nE093 | Time: 198.11s | train L=-1.282909 NLL=-1.216823 | val L=-0.959162 NLL=-0.892807 | Ï€ avg: [0.385 0.289 0.142 0.045 0.139] | LR: 2.50e-04\nE094 | Time: 198.51s | train L=-1.290466 NLL=-1.224466 | val L=-0.939453 NLL=-0.873147 | Ï€ avg: [0.384 0.291 0.141 0.045 0.139] | LR: 2.50e-04\nE095 | Time: 198.23s | train L=-1.293001 NLL=-1.226907 | val L=-0.934518 NLL=-0.868199 | Ï€ avg: [0.383 0.293 0.14  0.045 0.14 ] | LR: 2.50e-04\nE096 | Time: 197.90s | train L=-1.289499 NLL=-1.223425 | val L=-0.913685 NLL=-0.847460 | Ï€ avg: [0.386 0.291 0.139 0.045 0.14 ] | LR: 2.50e-04\nE097 | Time: 198.12s | train L=-1.298579 NLL=-1.232671 | val L=-0.964962 NLL=-0.898672 | Ï€ avg: [0.383 0.292 0.138 0.045 0.141] | LR: 2.50e-04\nE098 | Time: 198.01s | train L=-1.300098 NLL=-1.234160 | val L=-0.966944 NLL=-0.900578 | Ï€ avg: [0.381 0.295 0.137 0.046 0.142] | LR: 1.25e-04\nE099 | Time: 198.06s | train L=-1.305708 NLL=-1.239696 | val L=-0.972751 NLL=-0.906480 | Ï€ avg: [0.382 0.295 0.137 0.045 0.141] | LR: 1.25e-04\nE100 | Time: 198.45s | train L=-1.305186 NLL=-1.239213 | val L=-0.973279 NLL=-0.906997 | Ï€ avg: [0.381 0.296 0.136 0.045 0.141] | LR: 1.25e-04\nE101 | Time: 204.82s | train L=-1.303932 NLL=-1.237966 | val L=-0.976302 NLL=-0.910016 | Ï€ avg: [0.38  0.297 0.136 0.045 0.141] | LR: 1.25e-04\n  *** New best model (val NLL: -0.910016) ***\nE102 | Time: 201.51s | train L=-1.304134 NLL=-1.238153 | val L=-0.979408 NLL=-0.913166 | Ï€ avg: [0.381 0.297 0.135 0.045 0.142] | LR: 1.25e-04\n  *** New best model (val NLL: -0.913166) ***\nE103 | Time: 198.68s | train L=-1.311186 NLL=-1.245217 | val L=-0.970025 NLL=-0.903834 | Ï€ avg: [0.382 0.297 0.135 0.045 0.141] | LR: 1.25e-04\nE104 | Time: 198.64s | train L=-1.309344 NLL=-1.243409 | val L=-0.959955 NLL=-0.893744 | Ï€ avg: [0.38  0.298 0.135 0.045 0.142] | LR: 1.25e-04\nE105 | Time: 197.29s | train L=-1.310758 NLL=-1.244847 | val L=-0.965337 NLL=-0.899140 | Ï€ avg: [0.381 0.298 0.134 0.045 0.142] | LR: 1.25e-04\nE106 | Time: 198.13s | train L=-1.310091 NLL=-1.244120 | val L=-0.967212 NLL=-0.900976 | Ï€ avg: [0.379 0.3   0.134 0.046 0.142] | LR: 1.25e-04\nE107 | Time: 198.31s | train L=-1.312336 NLL=-1.246496 | val L=-0.975142 NLL=-0.908935 | Ï€ avg: [0.379 0.3   0.133 0.046 0.142] | LR: 1.25e-04\nE108 | Time: 208.34s | train L=-1.311164 NLL=-1.245243 | val L=-0.967932 NLL=-0.901790 | Ï€ avg: [0.38  0.301 0.133 0.045 0.142] | LR: 6.25e-05\nE109 | Time: 203.53s | train L=-1.315953 NLL=-1.250068 | val L=-0.973939 NLL=-0.907775 | Ï€ avg: [0.379 0.301 0.132 0.045 0.142] | LR: 6.25e-05\nE110 | Time: 198.52s | train L=-1.318123 NLL=-1.252282 | val L=-0.966548 NLL=-0.900394 | Ï€ avg: [0.379 0.301 0.132 0.045 0.142] | LR: 6.25e-05\nE111 | Time: 198.40s | train L=-1.316875 NLL=-1.251013 | val L=-0.977599 NLL=-0.911413 | Ï€ avg: [0.378 0.302 0.132 0.045 0.142] | LR: 6.25e-05\nE112 | Time: 198.25s | train L=-1.315476 NLL=-1.249603 | val L=-0.981651 NLL=-0.915461 | Ï€ avg: [0.378 0.302 0.132 0.046 0.142] | LR: 6.25e-05\n  *** New best model (val NLL: -0.915461) ***\nE113 | Time: 197.98s | train L=-1.317819 NLL=-1.251953 | val L=-0.970028 NLL=-0.903863 | Ï€ avg: [0.379 0.302 0.132 0.046 0.142] | LR: 6.25e-05\nE114 | Time: 198.04s | train L=-1.319421 NLL=-1.253566 | val L=-0.976916 NLL=-0.910755 | Ï€ avg: [0.378 0.302 0.132 0.046 0.142] | LR: 6.25e-05\nE115 | Time: 203.21s | train L=-1.318917 NLL=-1.253090 | val L=-0.976525 NLL=-0.910387 | Ï€ avg: [0.378 0.302 0.131 0.046 0.142] | LR: 6.25e-05\nE116 | Time: 198.02s | train L=-1.317189 NLL=-1.251332 | val L=-0.972252 NLL=-0.906073 | Ï€ avg: [0.377 0.303 0.131 0.046 0.143] | LR: 6.25e-05\nE117 | Time: 197.88s | train L=-1.318659 NLL=-1.252742 | val L=-0.977158 NLL=-0.910978 | Ï€ avg: [0.377 0.304 0.131 0.046 0.143] | LR: 6.25e-05\nE118 | Time: 198.11s | train L=-1.320288 NLL=-1.254420 | val L=-0.968701 NLL=-0.902541 | Ï€ avg: [0.377 0.304 0.131 0.046 0.143] | LR: 3.13e-05\nE119 | Time: 197.74s | train L=-1.322258 NLL=-1.256403 | val L=-0.970783 NLL=-0.904622 | Ï€ avg: [0.377 0.304 0.131 0.046 0.143] | LR: 3.13e-05\nE120 | Time: 197.52s | train L=-1.321269 NLL=-1.255395 | val L=-0.973837 NLL=-0.907675 | Ï€ avg: [0.377 0.304 0.131 0.046 0.143] | LR: 3.13e-05\nE121 | Time: 197.75s | train L=-1.323016 NLL=-1.257131 | val L=-0.970158 NLL=-0.904000 | Ï€ avg: [0.377 0.305 0.131 0.046 0.143] | LR: 3.13e-05\nE122 | Time: 198.05s | train L=-1.323422 NLL=-1.257574 | val L=-0.978705 NLL=-0.912550 | Ï€ avg: [0.376 0.305 0.13  0.046 0.143] | LR: 3.13e-05\nE123 | Time: 198.01s | train L=-1.322576 NLL=-1.256707 | val L=-0.972818 NLL=-0.906663 | Ï€ avg: [0.376 0.305 0.13  0.046 0.143] | LR: 3.13e-05\nE124 | Time: 198.20s | train L=-1.323531 NLL=-1.257664 | val L=-0.974363 NLL=-0.908219 | Ï€ avg: [0.377 0.305 0.13  0.046 0.143] | LR: 1.56e-05\nE125 | Time: 197.81s | train L=-1.324989 NLL=-1.259134 | val L=-0.971778 NLL=-0.905633 | Ï€ avg: [0.376 0.305 0.13  0.046 0.143] | LR: 1.56e-05\nE126 | Time: 197.66s | train L=-1.325143 NLL=-1.259282 | val L=-0.973015 NLL=-0.906873 | Ï€ avg: [0.376 0.305 0.13  0.046 0.143] | LR: 1.56e-05\nE127 | Time: 198.20s | train L=-1.325039 NLL=-1.259185 | val L=-0.973263 NLL=-0.907123 | Ï€ avg: [0.376 0.305 0.13  0.046 0.143] | LR: 1.56e-05\nðŸ›‘ Early stopping after epoch 127\n\nâœ… Training finished. Best val NLL: -0.9154605177086843\n"}],"execution_count":9},{"id":"114f53d9-b1b2-478d-8299-b3a8e952efa0","cell_type":"code","source":"# Save per-epoch data\nnp.save(f\"{model_name}_epoch_pi_means.npy\", np.array(epoch_pi_means))\nnp.save(f\"{model_name}_epoch_val_nll.npy\", np.array(epoch_val_nll))\nnp.save(f\"{model_name}_train_losses.npy\", np.array(train_losses))\nnp.save(f\"{model_name}_val_losses.npy\", np.array(val_losses))\n\n# Save hyperparameters\nwith open(f\"{model_name}_params.txt\", \"w\") as f:\n    f.write(f\"n_qubits: {n_qubits}\\n\")\n    f.write(f\"n_layers: {n_layers}\\n\")\n    f.write(f\"N_COMPONENTS: {N_COMPONENTS}\\n\")\n    f.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n    f.write(f\"ACCUMULATION_STEPS: {ACCUMULATION_STEPS}\\n\")\n    f.write(f\"effective_batch: {effective_batch}\\n\")\n    f.write(f\"LAMBDA_ENTROPY: {LAMBDA_ENTROPY}\\n\")\n    f.write(f\"LEARNING_RATE: {LEARNING_RATE}\\n\")\n    f.write(f\"WEIGHT_DECAY: {WEIGHT_DECAY}\\n\")\n    f.write(f\"MAX_EPOCHS: {MAX_EPOCHS}\\n\")\n    f.write(f\"PATIENCE: {PATIENCE}\\n\")\n    f.write(f\"SEED: {SEED}\\n\")\n    f.write(f\"best_val_nll: {best_val_nll:.6f}\\n\")\n    f.write(f\"best_epoch: {best_epoch+1}\\n\")\nprint(\"ðŸ“ Training history and parameters saved.\")","metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'best_epoch' is not defined","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSEED: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEED\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest_val_nll: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_nll\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     f.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbest_epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest_epoch\u001b[49m+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“ Training history and parameters saved.\u001b[39m\u001b[33m\"\u001b[39m)\n","\u001b[31mNameError\u001b[39m: name 'best_epoch' is not defined"]}],"execution_count":10},{"id":"a970cb99-b95a-4d02-a713-738626c469f3","cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(f\"{model_name}_best.pt\", map_location=DEVICE))\nmodel.eval()\n\ndef evaluate(loader, name):\n    preds, truths = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE)\n            pi, mu, sigma = model(xb)\n            delta_pred = torch.sum(pi * mu, dim=1, keepdim=True)\n            preds.append(delta_pred.cpu().numpy())\n            truths.append(yb.numpy())\n    preds = np.vstack(preds)\n    truths = np.vstack(truths)\n    mse = mean_squared_error(truths, preds)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(truths, preds)\n    print(f\"{name} -> MSE: {mse:.6f}, RMSE: {rmse:.6f}, RÂ²: {r2:.6f}\")\n    return preds, truths\n\ntrain_pred, train_true = evaluate(train_loader, \"TRAIN\")\nval_pred, val_true     = evaluate(val_loader, \"VALIDATION\")\ntest_pred, test_true   = evaluate(test_loader, \"TEST\")\n\n# Save predictions\nnp.savez(f\"{model_name}_predictions.npz\",\n         train_pred=train_pred, train_true=train_true,\n         val_pred=val_pred, val_true=val_true,\n         test_pred=test_pred, test_true=test_true)\nprint(\"Predictions saved.\")","metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/tmp/ipykernel_26279/3714618546.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(f\"{model_name}_best.pt\", map_location=DEVICE))\n"},{"name":"stdout","output_type":"stream","text":"TRAIN -> MSE: 0.062704, RMSE: 0.250409, RÂ²: 0.203609\nVALIDATION -> MSE: 0.101928, RMSE: 0.319262, RÂ²: 0.090631\nTEST -> MSE: 0.048081, RMSE: 0.219274, RÂ²: -0.025806\nPredictions saved.\n"}],"execution_count":11},{"id":"41bb46de-e3a4-4c0c-a240-e60bf6fe8620","cell_type":"code","source":"def get_embeddings(loader):\n    embeddings = []\n    with torch.no_grad():\n        for xb, _ in loader:\n            xb = xb.to(DEVICE)\n            x_enc = torch.tanh(model.encoder(xb))\n            probs = qnode(model.weight, x_enc)   # shape (batch, 2**n_qubits)\n            embeddings.append(probs.cpu().numpy())\n    return np.vstack(embeddings)\n\ntrain_emb = get_embeddings(train_loader)\nval_emb   = get_embeddings(val_loader)\ntest_emb  = get_embeddings(test_loader)\n\nnp.savez(f\"{model_name}_embeddings.npz\",\n         train_emb=train_emb, val_emb=val_emb, test_emb=test_emb)\nprint(\"Embeddings saved.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Embeddings saved.\n"}],"execution_count":12},{"id":"6806f087-5f1b-4480-82d9-341474130b31","cell_type":"code","source":"def get_mixture_params(loader):\n    pi_list, mu_list, sigma_list = [], [], []\n    with torch.no_grad():\n        for xb, _ in loader:\n            xb = xb.to(DEVICE)\n            pi, mu, sigma = model(xb)\n            pi_list.append(pi.cpu().numpy())\n            mu_list.append(mu.cpu().numpy())\n            sigma_list.append(sigma.cpu().numpy())\n    return (np.vstack(pi_list), np.vstack(mu_list), np.vstack(sigma_list))\n\ntrain_pi, train_mu, train_sigma = get_mixture_params(train_loader)\nval_pi, val_mu, val_sigma       = get_mixture_params(val_loader)\ntest_pi, test_mu, test_sigma     = get_mixture_params(test_loader)\n\nnp.savez(f\"{model_name}_mixtures.npz\",\n         train_pi=train_pi, train_mu=train_mu, train_sigma=train_sigma,\n         val_pi=val_pi,   val_mu=val_mu,     val_sigma=val_sigma,\n         test_pi=test_pi, test_mu=test_mu,   test_sigma=test_sigma)\nprint(\"Mixture parameters saved.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Mixture parameters saved.\n"}],"execution_count":13},{"id":"ef929706-fa60-4105-8489-6148104750b5","cell_type":"code","source":"# This cell is optional â€“ run only if you plan to compute Quantum Fisher Information.\n# It saves full statevectors for a few energy points per reaction.\nif False:   # set to True to enable\n    # Define a stateâ€‘returning QNode\n    dev_state = qml.device(\"default.qubit\", wires=n_qubits)\n    @qml.qnode(dev_state, interface=\"torch\")\n    def state_qnode(weights, x):\n        qml.templates.AngleEmbedding(x, wires=range(n_qubits), rotation=\"X\")\n        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n        return qml.state()\n\n    # Select representative samples (e.g., 5 per reaction)\n    rep_indices = []\n    for reaction in df[\"Reaction\"].unique():\n        rows = df[df[\"Reaction\"] == reaction].sort_values(\"E c.m.\")\n        if len(rows) >= 3:\n            idx = [rows.index[0], rows.index[len(rows)//2], rows.index[-1]]\n        else:\n            idx = rows.index.tolist()\n        rep_indices.extend(idx)\n\n    X_rep = X_scaled[rep_indices]\n    reactions_rep = df.iloc[rep_indices][\"Reaction\"].values\n    energy_rep = df.iloc[rep_indices][\"E c.m.\"].values\n\n    states = []\n    with torch.no_grad():\n        q_weights = model.weight.detach()\n        for i in range(len(X_rep)):\n            x_tensor = torch.tensor(X_rep[i], dtype=torch.float32).to(DEVICE)\n            state = state_qnode(q_weights, x_tensor)\n            states.append(state.cpu().numpy())\n    states = np.array(states)\n\n    np.savez(f\"{model_name}_states.npz\",\n             states=states,\n             indices=rep_indices,\n             reactions=reactions_rep,\n             energy=energy_rep)\n    print(\"Statevectors saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}