{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.14"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"4c946ebd-fb92-46f0-a1fd-40061d60a915","cell_type":"code","source":"# =============================================================================\n# COMPLETE HYPERPARAMETER SEARCH FOR 10-QUBIT QNN WITH VARYING LAYERS\n# Effective batch size = 256 (physical batch 64 + 4 accumulation steps)\n# =============================================================================\n\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport pennylane as qml\nfrom pathlib import Path\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# -------------------- Environment --------------------\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n# Use GPU 0 (you can change to another index if needed)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# -------------------- Hyperparameters for search --------------------\nn_qubits = 14\nlayer_list = [4, 6, 8, 10, 12,14,16,18]          # different layer depths\nphysical_batch = 64                      # must fit in GPU memory\naccumulation_steps = 4                    # 64 * 4 = 256 effective batch\neffective_batch = physical_batch * accumulation_steps\nlearning_rate = 1e-3\nweight_decay = 1e-4\nepochs = 100\npatience = 20\n\nprint(f\"Physical batch size: {physical_batch}, accumulation steps: {accumulation_steps}\")\nprint(f\"Effective batch size: {effective_batch}\")\n\n# -------------------- Data loading and preprocessing --------------------\nprint(\"\\nLoading and preparing data...\")\nDRIVE_URL = \"https://drive.google.com/uc?id=1PS0eB8dx8VMzVvxNUc6wBzsMRkEKJjWI\"\ndf = pd.read_csv(DRIVE_URL)\n\n# Physics feature engineering (copied from your notebook)\nM_p = 938.272088\nM_n = 939.565420\nepsilon = 1e-30\nLN10 = np.log(10.0)\n\ndef get_nucleon_mass(Z, A):\n    return Z * M_p + (A - Z) * M_n\n\nmass1 = df.apply(lambda r: get_nucleon_mass(r[\"Z1\"], r[\"A1\"]), axis=1).values\nmass2 = df.apply(lambda r: get_nucleon_mass(r[\"Z2\"], r[\"A2\"]), axis=1).values\nmu_MeVc2 = (mass1 * mass2) / (mass1 + mass2 + 1e-12)\nEcm = df[\"E c.m.\"].astype(float).values\nv_over_c = np.sqrt(np.clip(2 * Ecm / (mu_MeVc2 + epsilon), 0, np.inf))\ne2_hbar_c = 1 / 137.035999\ndf[\"eta\"] = (df[\"Z1\"] * df[\"Z2\"]) / (e2_hbar_c * (v_over_c + 1e-16))\n\nlog10_sigma_exp = np.log10(np.clip(df[\"σ\"], 1e-30, np.inf))\nlog10_sigma_cal = np.log10(np.clip(df[\"σ cal\"], 1e-30, np.inf))\nlog10_Ecm = np.log10(np.clip(df[\"E c.m.\"], 1e-30, np.inf))\nlog10_exp_term = (2 * np.pi * df[\"eta\"]) / LN10\n\ndf[\"log10_S_exp\"] = log10_sigma_exp + log10_Ecm + log10_exp_term\ndf[\"log10_S_cal\"] = log10_sigma_cal + log10_Ecm + log10_exp_term\ndf[\"delta_log10_S\"] = df[\"log10_S_exp\"] - df[\"log10_S_cal\"]\n\ndf[\"N1\"] = df[\"A1\"] - df[\"Z1\"]\ndf[\"N2\"] = df[\"A2\"] - df[\"Z2\"]\ndf[\"Z1Z2_over_Ecm\"] = (df[\"Z1\"] * df[\"Z2\"]) / (df[\"E c.m.\"] + epsilon)\n\nMAGIC = np.array([2, 8, 20, 28, 50, 82, 126])\ndef magic_dist(arr):\n    return np.min(np.abs(arr[:, None] - MAGIC[None, :]), axis=1)\n\ndf[\"magic_dist_Z1\"] = magic_dist(df[\"Z1\"].values)\ndf[\"magic_dist_N1\"] = magic_dist(df[\"N1\"].values)\ndf[\"magic_dist_Z2\"] = magic_dist(df[\"Z2\"].values)\ndf[\"magic_dist_N2\"] = magic_dist(df[\"N2\"].values)\n\n# 29 features\nfeatures_train = [\n    'E c.m.', 'Z1', 'N1', 'A1',\n    'Z2', 'N2', 'A2', 'Q ( 2 n )',\n    'Z1Z2_over_Ecm',\n    'magic_dist_Z1', 'magic_dist_N1', 'magic_dist_Z2', 'magic_dist_N2',\n    'Z3', 'N3', 'A3', 'β P', 'β T', 'R B', 'ħ ω',\n    'Projectile_Mass_Actual', 'Target_Mass_Actual', 'Compound_Nucleus_Mass_Actual',\n    'Compound_Nucleus_Sp', 'Compound_Nucleus_Sn',\n    'Projectile_Binding_Energy', 'Target_Binding_Energy',\n    'Compound_Nucleus_Binding_Energy', 'Compound_Nucleus_S2n'\n]\n\n# Reaction split\nOUTDIR_BASE = \"mdn_70_10_20_optimized\"\ntrain_file = Path(OUTDIR_BASE) / \"train_reactions.csv\"\nval_file   = Path(OUTDIR_BASE) / \"val_reactions.csv\"\ntest_file  = Path(OUTDIR_BASE) / \"test_reactions.csv\"\n\nif train_file.exists():\n    train_reacts = pd.read_csv(train_file)[\"Reaction\"].values\n    val_reacts   = pd.read_csv(val_file)[\"Reaction\"].values\n    test_reacts  = pd.read_csv(test_file)[\"Reaction\"].values\nelse:\n    reactions = df[\"Reaction\"].unique()\n    np.random.seed(42)\n    np.random.shuffle(reactions)\n    n = len(reactions)\n    train_reacts = reactions[:int(0.7 * n)]\n    val_reacts   = reactions[int(0.7 * n):int(0.8 * n)]\n    test_reacts  = reactions[int(0.8 * n):]\n    os.makedirs(OUTDIR_BASE, exist_ok=True)\n    pd.DataFrame({\"Reaction\": train_reacts}).to_csv(train_file, index=False)\n    pd.DataFrame({\"Reaction\": val_reacts}).to_csv(val_file, index=False)\n    pd.DataFrame({\"Reaction\": test_reacts}).to_csv(test_file, index=False)\n\ntrain_mask = df[\"Reaction\"].isin(train_reacts)\nval_mask   = df[\"Reaction\"].isin(val_reacts)\ntest_mask  = df[\"Reaction\"].isin(test_reacts)\n\nX = df[features_train].values.astype(np.float32)\ny = df[\"delta_log10_S\"].values.astype(np.float32).reshape(-1, 1)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train = X_scaled[train_mask]\ny_train = y[train_mask]\nX_val   = X_scaled[val_mask]\ny_val   = y[val_mask]\nX_test  = X_scaled[test_mask]\ny_test  = y[test_mask]\n\n# Convert to tensors\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train, dtype=torch.float32)\nX_val_t   = torch.tensor(X_val, dtype=torch.float32)\ny_val_t   = torch.tensor(y_val, dtype=torch.float32)\nX_test_t  = torch.tensor(X_test, dtype=torch.float32)\ny_test_t  = torch.tensor(y_test, dtype=torch.float32)\n\n# DataLoaders (with physical batch size)\ntrain_loader = DataLoader(TensorDataset(X_train_t, y_train_t),\n                          batch_size=physical_batch, shuffle=True,\n                          num_workers=0, pin_memory=True)\nval_loader   = DataLoader(TensorDataset(X_val_t, y_val_t),\n                          batch_size=physical_batch, shuffle=False,\n                          num_workers=0, pin_memory=True)\ntest_loader  = DataLoader(TensorDataset(X_test_t, y_test_t),\n                          batch_size=physical_batch, shuffle=False,\n                          num_workers=0, pin_memory=True)\n\nprint(f\"Train samples: {X_train.shape[0]}, Validation: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\nprint(f\"Train batches (physical): {len(train_loader)}\")\nprint(f\"Accumulation steps: {accumulation_steps} -> effective batches per epoch: {len(train_loader)//accumulation_steps}\")\n\n# -------------------- Quantum device and QNode --------------------\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\ndef qnode(weights, x):\n    qml.templates.AngleEmbedding(x, wires=range(n_qubits), rotation=\"X\")\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\n# -------------------- Model definition (flexible qubits & layers) --------------------\nclass QuantumRegressor(nn.Module):\n    def __init__(self, n_qubits, n_layers):\n        super().__init__()\n        self.n_qubits = n_qubits\n        self.encoder = nn.Linear(29, n_qubits)\n        self.q_weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, 3))\n        self.fc1 = nn.Linear(n_qubits, 32)\n        self.fc2 = nn.Linear(32, 1)\n\n    def forward(self, x):\n        x = x.float()\n        x_enc = torch.tanh(self.encoder(x))\n        q_out_tuple = qnode(self.q_weights, x_enc)\n        q_out = torch.stack(q_out_tuple, dim=1)\n        q_out = q_out.to(x.dtype)\n        h = torch.relu(self.fc1(q_out))\n        return self.fc2(h)\n\n# -------------------- Training function for one configuration --------------------\ndef train_one_config(layers, seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    model = QuantumRegressor(n_qubits=n_qubits, n_layers=layers).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = nn.MSELoss()\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n    best_val_loss = float('inf')\n    best_epoch = 0\n    patience_counter = 0\n    best_state = None\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        optimizer.zero_grad()   # reset at start of epoch\n\n        for batch_idx, (xb, yb) in enumerate(train_loader):\n            xb, yb = xb.to(device), yb.to(device)\n            preds = model(xb)\n            loss = criterion(preds, yb) / accumulation_steps\n            loss.backward()\n\n            if (batch_idx + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n            train_loss += loss.item() * accumulation_steps\n\n        # Handle any leftover batches\n        if (batch_idx + 1) % accumulation_steps != 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        train_loss /= len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                preds = model(xb)\n                val_loss += criterion(preds, yb).item()\n        val_loss /= len(val_loader)\n\n        scheduler.step(val_loss)\n\n        if epoch % 10 == 0:\n            print(f\"  Epoch {epoch:3d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_epoch = epoch\n            patience_counter = 0\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"    Early stopping at epoch {epoch}\")\n                break\n\n    # Load best model\n    model.load_state_dict(best_state)\n    model.to(device)\n\n    # Evaluate on test set\n    model.eval()\n    preds_list = []\n    with torch.no_grad():\n        for xb, _ in test_loader:\n            xb = xb.to(device)\n            preds = model(xb)\n            preds_list.append(preds.cpu().numpy())\n    preds_arr = np.vstack(preds_list).flatten()\n    y_true = y_test.flatten()\n\n    mse = mean_squared_error(y_true, preds_arr)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, preds_arr)\n\n    # Also compute validation metrics on best model\n    val_preds = []\n    with torch.no_grad():\n        for xb, _ in val_loader:\n            xb = xb.to(device)\n            preds = model(xb)\n            val_preds.append(preds.cpu().numpy())\n    val_preds = np.vstack(val_preds).flatten()\n    val_true = y_val.flatten()\n    val_mse = mean_squared_error(val_true, val_preds)\n    val_rmse = np.sqrt(val_mse)\n    val_r2 = r2_score(val_true, val_preds)\n\n    return {\n        'layers': layers,\n        'val_mse': val_mse,\n        'val_rmse': val_rmse,\n        'val_r2': val_r2,\n        'test_mse': mse,\n        'test_rmse': rmse,\n        'test_r2': r2,\n        'best_epoch': best_epoch\n    }\n\n# -------------------- Run hyperparameter search --------------------\nresults = []\nfor layers in layer_list:\n    print(f\"\\n{'='*60}\\nTraining with {layers} layers\\n{'='*60}\")\n    torch.cuda.empty_cache()\n    gc.collect()\n    res = train_one_config(layers, seed=42)\n    results.append(res)\n    print(f\"  --> Test R² = {res['test_r2']:.4f}\")\n\n# -------------------- Display summary --------------------\nresults_df = pd.DataFrame(results)\nprint(\"\\n\\n\" + \"=\"*70)\nprint(\"HYPERPARAMETER SEARCH SUMMARY (10 qubits, effective batch 256)\")\nprint(\"=\"*70)\nprint(results_df.to_string(index=False))\nprint(\"\\nBest test R²: {:.4f} with {} layers\".format(\n    results_df.loc[results_df['test_r2'].idxmax(), 'test_r2'],\n    results_df.loc[results_df['test_r2'].idxmax(), 'layers']\n))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Using device: cuda:0\nPhysical batch size: 64, accumulation steps: 4\nEffective batch size: 256\n\nLoading and preparing data...\nTrain samples: 2493, Validation: 354, Test: 685\nTrain batches (physical): 39\nAccumulation steps: 4 -> effective batches per epoch: 9\n\n============================================================\nTraining with 4 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.087380 | Val Loss: 0.101555\n  Epoch  10/100 | Train Loss: 0.069700 | Val Loss: 0.102930\n  Epoch  20/100 | Train Loss: 0.065974 | Val Loss: 0.105205\n    Early stopping at epoch 23\n  --> Test R² = -0.0287\n\n============================================================\nTraining with 6 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.109002 | Val Loss: 0.115668\n  Epoch  10/100 | Train Loss: 0.071582 | Val Loss: 0.105176\n  Epoch  20/100 | Train Loss: 0.067062 | Val Loss: 0.106044\n    Early stopping at epoch 23\n  --> Test R² = -0.0106\n\n============================================================\nTraining with 8 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.076948 | Val Loss: 0.106341\n  Epoch  10/100 | Train Loss: 0.064716 | Val Loss: 0.113341\n  Epoch  20/100 | Train Loss: 0.061401 | Val Loss: 0.114392\n    Early stopping at epoch 21\n  --> Test R² = -0.0321\n\n============================================================\nTraining with 10 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.092188 | Val Loss: 0.102146\n  Epoch  10/100 | Train Loss: 0.072203 | Val Loss: 0.105975\n  Epoch  20/100 | Train Loss: 0.069668 | Val Loss: 0.107610\n    Early stopping at epoch 23\n  --> Test R² = 0.0024\n\n============================================================\nTraining with 12 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.078972 | Val Loss: 0.105808\n  Epoch  10/100 | Train Loss: 0.066849 | Val Loss: 0.108429\n  Epoch  20/100 | Train Loss: 0.062181 | Val Loss: 0.106004\n    Early stopping at epoch 21\n  --> Test R² = -0.0192\n\n============================================================\nTraining with 14 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.098779 | Val Loss: 0.112498\n  Epoch  10/100 | Train Loss: 0.069927 | Val Loss: 0.111111\n  Epoch  20/100 | Train Loss: 0.066368 | Val Loss: 0.110917\n    Early stopping at epoch 23\n  --> Test R² = -0.0414\n\n============================================================\nTraining with 16 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.099458 | Val Loss: 0.109703\n  Epoch  10/100 | Train Loss: 0.074198 | Val Loss: 0.105997\n  Epoch  20/100 | Train Loss: 0.070824 | Val Loss: 0.107818\n    Early stopping at epoch 25\n  --> Test R² = -0.0176\n\n============================================================\nTraining with 18 layers\n============================================================\n  Epoch   0/100 | Train Loss: 0.079305 | Val Loss: 0.106750\n  Epoch  10/100 | Train Loss: 0.065937 | Val Loss: 0.110203\n  Epoch  20/100 | Train Loss: 0.061634 | Val Loss: 0.112635\n    Early stopping at epoch 21\n  --> Test R² = -0.0118\n\n\n======================================================================\nHYPERPARAMETER SEARCH SUMMARY (10 qubits, effective batch 256)\n======================================================================\n layers  val_mse  val_rmse    val_r2  test_mse  test_rmse   test_r2  best_epoch\n      4 0.105530  0.324853  0.024413  0.048216   0.219582 -0.028694           3\n      6 0.109880  0.331482 -0.015810  0.047369   0.217644 -0.010617           3\n      8 0.108941  0.330063 -0.007128  0.048377   0.219949 -0.032134           1\n     10 0.106618  0.326524  0.014354  0.046760   0.216241  0.002373           3\n     12 0.110001  0.331664 -0.016924  0.047770   0.218562 -0.019163           1\n     14 0.113234  0.336502 -0.046808  0.048812   0.220934 -0.041401           3\n     16 0.109619  0.331087 -0.013388  0.047697   0.218396 -0.017617           5\n     18 0.109302  0.330608 -0.010460  0.047424   0.217771 -0.011795           1\n\nBest test R²: 0.0024 with 10 layers\n"}],"execution_count":1},{"id":"9ee0557b-0571-48d5-aad3-7950a99a7e4b","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}