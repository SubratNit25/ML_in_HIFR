{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c690013d-2374-4263-964b-96f4df1c1de8","cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport pennylane as qml\nimport matplotlib.pyplot as plt\nimport gc\nimport subprocess\nfrom datetime import datetime\n\n# ---- GPU Selection ----\ndef get_free_gpu():\n    \"\"\"Return index of GPU with most free memory (simple heuristic).\"\"\"\n    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.free', '--format=csv,nounits,noheader'], \n                            capture_output=True, text=True)\n    free_memory = [int(x) for x in result.stdout.strip().split('\\n')]\n    best_gpu = np.argmax(free_memory)\n    print(f\"GPU free memory: {free_memory}\")\n    print(f\"Selected GPU {best_gpu} with {free_memory[best_gpu]} MiB free\")\n    return best_gpu\n\n# Uncomment to auto-select, or manually set GPU index:\n# gpu_index = get_free_gpu()\ngpu_index = 0   # <--- CHANGE MANUALLY IF NEEDED\nos.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device} (GPU {gpu_index})\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Using device: cuda (GPU 0)\n"}],"execution_count":1},{"id":"e6ec56b1-3e8b-4815-9e61-08029a03d155","cell_type":"code","source":"# ---- Load data ----\nDRIVE_URL = \"https://drive.google.com/uc?id=1PS0eB8dx8VMzVvxNUc6wBzsMRkEKJjWI\"\ndf = pd.read_csv(DRIVE_URL)\n\n# ---- Physics constants ----\nM_p = 938.272088\nM_n = 939.565420\nepsilon = 1e-30\nLN10 = np.log(10.0)\ne2_hbar_c = 1/137.035999\n\ndef mass(Z, A):\n    return Z*M_p + (A-Z)*M_n\n\n# ---- Feature engineering (copied from your working code) ----\ndf[\"N1\"] = df[\"A1\"] - df[\"Z1\"]\ndf[\"N2\"] = df[\"A2\"] - df[\"Z2\"]\n\ndf[\"mass1\"] = mass(df[\"Z1\"], df[\"A1\"])\ndf[\"mass2\"] = mass(df[\"Z2\"], df[\"A2\"])\n\nmu_red = (df[\"mass1\"] * df[\"mass2\"]) / (df[\"mass1\"] + df[\"mass2\"] + 1e-12)\nv_over_c = np.sqrt(np.clip(2*df[\"E c.m.\"].values/(mu_red+epsilon), 0, np.inf))\ndf[\"eta\"] = (df[\"Z1\"]*df[\"Z2\"]) / (e2_hbar_c * (v_over_c + 1e-16))\n\ndf[\"Z1Z2_over_Ecm\"] = (df[\"Z1\"]*df[\"Z2\"]) / (df[\"E c.m.\"] + epsilon)\n\nMAGIC = np.array([2,8,20,28,50,82,126])\ndf[\"magic_dist_Z1\"] = np.min(np.abs(df[\"Z1\"].values[:,None] - MAGIC), axis=1)\ndf[\"magic_dist_N1\"] = np.min(np.abs(df[\"N1\"].values[:,None] - MAGIC), axis=1)\ndf[\"magic_dist_Z2\"] = np.min(np.abs(df[\"Z2\"].values[:,None] - MAGIC), axis=1)\ndf[\"magic_dist_N2\"] = np.min(np.abs(df[\"N2\"].values[:,None] - MAGIC), axis=1)\n\n# Coulomb barrier\nbarrier_df = df.groupby(\"Reaction\").first().reset_index()\nbarrier_df[\"V_B\"] = (barrier_df[\"Z1\"]*barrier_df[\"Z2\"]*1.44) / barrier_df[\"R B\"]\ndf = df.merge(barrier_df[[\"Reaction\",\"V_B\"]], on=\"Reaction\", how=\"left\")\n\n# S-factor logs\nlog10_sigma_exp = np.log10(np.clip(df[\"œÉ\"], 1e-30, np.inf))\nlog10_sigma_cal = np.log10(np.clip(df[\"œÉ cal\"], 1e-30, np.inf))\nlog10_Ecm = np.log10(np.clip(df[\"E c.m.\"], 1e-30, np.inf))\n\ndf[\"log10_S_exp\"] = log10_sigma_exp + log10_Ecm + (2*np.pi*df[\"eta\"])/LN10\ndf[\"log10_S_cal\"] = log10_sigma_cal + log10_Ecm + (2*np.pi*df[\"eta\"])/LN10\ndf[\"delta_log10_S\"] = df[\"log10_S_exp\"] - df[\"log10_S_cal\"]\n\nprint(\"Data ready. Total rows:\", len(df))","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Data ready. Total rows: 3532\n"}],"execution_count":2},{"id":"a8349449-8f9d-4aa1-9afa-1b19b1b4c7d4","cell_type":"code","source":"features_train = [\n    'E c.m.', 'Z1', 'N1', 'A1',\n    'Z2', 'N2', 'A2', 'Q ( 2 n )',\n    'Z1Z2_over_Ecm',\n    'magic_dist_Z1','magic_dist_N1','magic_dist_Z2','magic_dist_N2',\n    'Z3','N3','A3','Œ≤ P','Œ≤ T','R B','ƒß œâ',\n    'Projectile_Mass_Actual','Target_Mass_Actual','Compound_Nucleus_Mass_Actual',\n    'Compound_Nucleus_Sp','Compound_Nucleus_Sn',\n    'Projectile_Binding_Energy','Target_Binding_Energy',\n    'Compound_Nucleus_Binding_Energy','Compound_Nucleus_S2n'\n]\n\nOUTDIR_BASE = \"mdn_70_10_20_optimized\"\ntrain_reacts = pd.read_csv(f\"{OUTDIR_BASE}/train_reactions.csv\")[\"Reaction\"].values\nval_reacts   = pd.read_csv(f\"{OUTDIR_BASE}/val_reactions.csv\")[\"Reaction\"].values\ntest_reacts  = pd.read_csv(f\"{OUTDIR_BASE}/test_reactions.csv\")[\"Reaction\"].values\n\ntrain_mask = df[\"Reaction\"].isin(train_reacts)\nval_mask   = df[\"Reaction\"].isin(val_reacts)\ntest_mask  = df[\"Reaction\"].isin(test_reacts)\n\nX = df[features_train].values.astype(np.float32)\ny = df[\"delta_log10_S\"].values.astype(np.float32).reshape(-1,1)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nX_train = X_scaled[train_mask]\ny_train = y[train_mask]\nX_val   = X_scaled[val_mask]\ny_val   = y[val_mask]\nX_test  = X_scaled[test_mask]\ny_test  = y[test_mask]\n\nprint(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train: 2493, Val: 354, Test: 685\n"}],"execution_count":3},{"id":"eeafa83a-3b3a-4049-9c93-1ea756077435","cell_type":"code","source":"# ==================================================\n# ADJUST THESE PARAMETERS FOR EACH EXPERIMENT\n# ==================================================\nn_qubits = 15\nn_layers = 14\nbatch_size = 32              # physical batch size (use smaller if OOM)\naccumulation_steps = 2        # effective batch = batch_size * accumulation_steps\nlearning_rate = 1e-3\nweight_decay = 1e-4\nepochs = 100\npatience = 10\nseed = 42                     # random seed for reproducibility\ndropout_rate = 0.2\n\n# Effective batch size for logging\neffective_batch = batch_size * accumulation_steps\nprint(f\"Qubits: {n_qubits}, Layers: {n_layers}, Batch: {batch_size} (eff. {effective_batch})\")\n\n# ---- Set seeds ----\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n# ---- Automatic naming ----\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodel_name = f\"qnn_q{n_qubits}_l{n_layers}_b{batch_size}_acc{accumulation_steps}_s{seed}_{timestamp}\"\nprint(\"Model name:\", model_name)","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Qubits: 15, Layers: 14, Batch: 32 (eff. 64)\nModel name: qnn_q15_l14_b32_acc2_s42_20260228_121144\n"}],"execution_count":4},{"id":"131f016a-d55a-4c40-89b7-09edc8abf882","cell_type":"code","source":"dev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\ndef qnode(weights, x):\n    qml.templates.AngleEmbedding(x, wires=range(n_qubits), rotation=\"X\")\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]","metadata":{"trusted":true},"outputs":[],"execution_count":5},{"id":"04e730d9-3da8-4d2e-95bc-f5adb2b7569f","cell_type":"code","source":"class QuantumRegressor(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.encoder = nn.Linear(in_dim, n_qubits)\n        self.q_weights = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, 3))\n        self.fc1 = nn.Linear(n_qubits, 16)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(16, 1)\n\n    def forward(self, x):\n        x = x.float()\n        x_enc = torch.tanh(self.encoder(x))\n        q_out_tuple = qnode(self.q_weights, x_enc)\n        q_out = torch.stack(q_out_tuple, dim=1)\n        q_out = q_out.to(x.dtype)\n        h = torch.relu(self.fc1(q_out))\n        h = self.dropout(h)\n        return self.fc2(h)","metadata":{"trusted":true},"outputs":[],"execution_count":6},{"id":"14267fa5-9fcd-49e5-abf0-7f7ef966f5f1","cell_type":"code","source":"def make_loader(X, y, batch, shuffle=True):\n    return DataLoader(\n        TensorDataset(torch.tensor(X, dtype=torch.float32),\n                      torch.tensor(y, dtype=torch.float32)),\n        batch_size=batch,\n        shuffle=shuffle,\n        num_workers=0,\n        pin_memory=True\n    )\n\ntrain_loader = make_loader(X_train, y_train, batch_size, shuffle=True)\nval_loader   = make_loader(X_val, y_val, batch_size, shuffle=False)\ntest_loader  = make_loader(X_test, y_test, batch_size, shuffle=False)\n\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train batches: 78, Val batches: 12\n"}],"execution_count":7},{"id":"1bbf8d8b-d764-4f5b-ac53-672937ff7476","cell_type":"code","source":"model = QuantumRegressor(in_dim=29).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\ncriterion = nn.MSELoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n# Tracking variables\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\npatience_counter = 0\nbest_epoch = 0","metadata":{"trusted":true},"outputs":[],"execution_count":8},{"id":"3d954334-30a2-46f8-ae9b-5c5d79e32e25","cell_type":"code","source":"import time   # make sure time is imported\n\nprint(f\"\\nüöÄ Starting training: {model_name}\\n\")\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    optimizer.zero_grad()\n    start_time = time.time()   # <-- start timer\n\n    for batch_idx, (xb, yb) in enumerate(train_loader):\n        xb, yb = xb.to(device), yb.to(device)\n        preds = model(xb)\n        loss = criterion(preds, yb) / accumulation_steps\n        loss.backward()\n\n        if (batch_idx + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n        epoch_loss += loss.item() * xb.size(0) * accumulation_steps\n\n    if (len(train_loader) % accumulation_steps) != 0:\n        optimizer.step()\n        optimizer.zero_grad()\n\n    epoch_loss /= len(train_loader.dataset)\n    train_losses.append(epoch_loss)\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            preds = model(xb)\n            val_loss += criterion(preds, yb).item() * xb.size(0)\n    val_loss /= len(val_loader.dataset)\n    val_losses.append(val_loss)\n\n    scheduler.step(val_loss)\n    epoch_time = time.time() - start_time   # <-- compute elapsed time\n    current_lr = optimizer.param_groups[0]['lr']\n\n    # Logging with time and LR\n    print(f\"Epoch {epoch+1:3d}/{epochs} | Time: {epoch_time:.2f}s | \"\n          f\"Train Loss: {epoch_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {current_lr:.2e}\")\n\n    # Early stopping and model saving\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_epoch = epoch\n        patience_counter = 0\n        torch.save(model.state_dict(), f\"{model_name}_best.pt\")\n        print(f\"  *** New best model saved (epoch {epoch+1}) ***\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nprint(\"\\n‚úÖ Training finished.\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nüöÄ Starting training: qnn_q15_l14_b32_acc2_s42_20260228_121144\n\nEpoch   1/100 | Time: 146.63s | Train Loss: 0.093895 | Val Loss: 0.106357 | LR: 1.00e-03\n  *** New best model saved (epoch 1) ***\nEpoch   2/100 | Time: 150.83s | Train Loss: 0.083180 | Val Loss: 0.106518 | LR: 1.00e-03\nEpoch   3/100 | Time: 150.81s | Train Loss: 0.077519 | Val Loss: 0.105585 | LR: 1.00e-03\n  *** New best model saved (epoch 3) ***\nEpoch   4/100 | Time: 150.69s | Train Loss: 0.075077 | Val Loss: 0.105927 | LR: 1.00e-03\nEpoch   5/100 | Time: 150.58s | Train Loss: 0.074387 | Val Loss: 0.104597 | LR: 1.00e-03\n  *** New best model saved (epoch 5) ***\nEpoch   6/100 | Time: 148.10s | Train Loss: 0.072368 | Val Loss: 0.107300 | LR: 1.00e-03\nEpoch   7/100 | Time: 148.25s | Train Loss: 0.071133 | Val Loss: 0.104925 | LR: 1.00e-03\n"}],"execution_count":null},{"id":"be56c541-67ba-47bb-af7b-f91c3b111fef","cell_type":"code","source":"# Save losses\nnp.save(f\"{model_name}_train_losses.npy\", np.array(train_losses))\nnp.save(f\"{model_name}_val_losses.npy\", np.array(val_losses))\n\n# Save hyperparameters as text\nwith open(f\"{model_name}_params.txt\", \"w\") as f:\n    f.write(f\"n_qubits: {n_qubits}\\n\")\n    f.write(f\"n_layers: {n_layers}\\n\")\n    f.write(f\"batch_size: {batch_size}\\n\")\n    f.write(f\"accumulation_steps: {accumulation_steps}\\n\")\n    f.write(f\"learning_rate: {learning_rate}\\n\")\n    f.write(f\"weight_decay: {weight_decay}\\n\")\n    f.write(f\"seed: {seed}\\n\")\n    f.write(f\"dropout_rate: {dropout_rate}\\n\")\n    f.write(f\"best_epoch: {best_epoch+1}\\n\")\n    f.write(f\"best_val_loss: {best_val_loss:.6f}\\n\")\n\nprint(\"Training history saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d1fc792e-d04d-4f93-8fb1-5c6496e2656b","cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load(f\"{model_name}_best.pt\", map_location=device))\nmodel.eval()\n\ndef evaluate(loader, name):\n    preds, truth = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(device)\n            out = model(xb)\n            preds.append(out.cpu().numpy())\n            truth.append(yb.numpy())\n    preds = np.vstack(preds)\n    truth = np.vstack(truth)\n    mse = mean_squared_error(truth, preds)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(truth, preds)\n    print(f\"{name} -> MSE: {mse:.6f}, RMSE: {rmse:.6f}, R¬≤: {r2:.6f}\")\n    return mse, rmse, r2, preds, truth\n\n# Evaluate\ntrain_mse, train_rmse, train_r2, train_pred, train_true = evaluate(train_loader, \"TRAIN\")\nval_mse, val_rmse, val_r2, val_pred, val_true = evaluate(val_loader, \"VALIDATION\")\ntest_mse, test_rmse, test_r2, test_pred, test_true = evaluate(test_loader, \"TEST\")\n\n# Save predictions\nnp.savez(f\"{model_name}_predictions.npz\",\n         train_pred=train_pred, train_true=train_true,\n         val_pred=val_pred, val_true=val_true,\n         test_pred=test_pred, test_true=test_true)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c9c2c846-821c-4424-a71e-be9e16d4ec84","cell_type":"code","source":"# This cell extracts the 14-dimensional quantum feature vector (‚ü®Z_i‚ü©) for all samples\n# Useful for PCA, regime separation, etc.\n\ndef get_embeddings(loader):\n    embeddings = []\n    with torch.no_grad():\n        for xb, _ in loader:\n            xb = xb.to(device)\n            x_enc = torch.tanh(model.encoder(xb))\n            q_out_tuple = qnode(model.q_weights, x_enc)\n            q_out = torch.stack(q_out_tuple, dim=1).cpu().numpy()\n            embeddings.append(q_out)\n    return np.vstack(embeddings)\n\ntrain_emb = get_embeddings(train_loader)\nval_emb   = get_embeddings(val_loader)\ntest_emb  = get_embeddings(test_loader)\n\nnp.savez(f\"{model_name}_embeddings.npz\",\n         train_emb=train_emb, val_emb=val_emb, test_emb=test_emb)\nprint(\"Embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6a619da3-380d-45d9-9e2e-dc72312fe40a","cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Val Loss')\nplt.axvline(x=best_epoch, color='r', linestyle='--', label='Best Model')\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.title(f'Training History: {model_name}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.savefig(f\"{model_name}_loss_curve.png\", dpi=150)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a97d3ff1-9720-4384-8ee7-5bab18a7dab1","cell_type":"code","source":"fig, axes = plt.subplots(1,3, figsize=(15,4))\ndatasets = [('Train', train_true, train_pred),\n            ('Validation', val_true, val_pred),\n            ('Test', test_true, test_pred)]\nfor ax, (name, y_true, y_pred) in zip(axes, datasets):\n    ax.scatter(y_true, y_pred, alpha=0.5)\n    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n    ax.set_xlabel('True Œîlog‚ÇÅ‚ÇÄS')\n    ax.set_ylabel('Predicted Œîlog‚ÇÅ‚ÇÄS')\n    ax.set_title(f'{name} (R¬≤ = {r2_score(y_true, y_pred):.3f})')\n    ax.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig(f\"{model_name}_scatter.png\", dpi=150)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}