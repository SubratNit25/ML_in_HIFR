{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643024d5-eeb2-4671-95c1-13f2f426d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using GPU 4\n",
      "Torch Device: cuda\n",
      "Qubits: 14, Layers: 12, Components: 5\n",
      "Physical batch: 64, Accumulation steps: 2\n",
      "Effective batch size: 128\n",
      "Using PennyLane device: default.qubit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from pathlib import Path\n",
    "import pennylane as qml\n",
    "import gc\n",
    "import subprocess\n",
    "\n",
    "# -------------------- Check GPU availability --------------------\n",
    "# def get_gpu_memory():\n",
    "#     \"\"\"Return list of GPU memory usage in MiB.\"\"\"\n",
    "#     result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'], \n",
    "#                             capture_output=True, text=True)\n",
    "#     memory = [int(x) for x in result.stdout.strip().split('\\n')]\n",
    "#     return memory\n",
    "\n",
    "# print(\"GPU memory usage:\")\n",
    "# mem = get_gpu_memory()\n",
    "# for i, m in enumerate(mem):\n",
    "#     print(f\"  GPU {i}: {m} MiB used\")\n",
    "\n",
    "# -------------------- User selects GPU --------------------\n",
    "GPU_ID = 4   # <--- CHANGE THIS to a GPU with low memory (e.g., < 500 MiB)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)\n",
    "print(f\"\\nUsing GPU {GPU_ID}\")\n",
    "\n",
    "# -------------------- Device --------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch Device:\", DEVICE)\n",
    "\n",
    "# -------------------- Cleanup --------------------\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# -------------------- Hyperparameters --------------------\n",
    "n_qubits = 14\n",
    "n_layers = 12\n",
    "N_COMPONENTS = 5\n",
    "BATCH_SIZE = 64                     # reduced physical batch size\n",
    "ACCUMULATION_STEPS = 2               # effective batch = 64\n",
    "LAMBDA_ENTROPY = 0.05                # set to 0 to disable entropy\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_EPOCHS = 150\n",
    "PATIENCE = 15\n",
    "\n",
    "print(f\"Qubits: {n_qubits}, Layers: {n_layers}, Components: {N_COMPONENTS}\")\n",
    "print(f\"Physical batch: {BATCH_SIZE}, Accumulation steps: {ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "\n",
    "# -------------------- Quantum Device --------------------\n",
    "# if DEVICE.type == \"cuda\":\n",
    "#     try:\n",
    "#         dev = qml.device(\"lightning.gpu\", wires=n_qubits, shots=None)\n",
    "#         print(\"Using PennyLane device: lightning.gpu\")\n",
    "#     except:\n",
    "#         dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
    "#         print(\"Fallback to PennyLane device: default.qubit\")\n",
    "# else:\n",
    "#     dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
    "#     print(\"Using PennyLane device: default.qubit\")\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
    "print(\"Using PennyLane device: default.qubit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5e57b8-5088-4a52-9a3f-b1f20ae74e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 3532\n",
      "Total reactions: 213\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Load Data --------------------\n",
    "DRIVE_URL = \"https://drive.google.com/uc?id=1PS0eB8dx8VMzVvxNUc6wBzsMRkEKJjWI\"\n",
    "df = pd.read_csv(DRIVE_URL)\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Total reactions:\", df[\"Reaction\"].nunique())\n",
    "\n",
    "# -------------------- Physics Feature Engineering --------------------\n",
    "M_p = 938.272088\n",
    "M_n = 939.565420\n",
    "epsilon = 1e-30\n",
    "LN10 = np.log(10.0)\n",
    "\n",
    "def get_nucleon_mass(Z, A):\n",
    "    return Z * M_p + (A - Z) * M_n\n",
    "\n",
    "mass1 = df.apply(lambda r: get_nucleon_mass(r[\"Z1\"], r[\"A1\"]), axis=1).values\n",
    "mass2 = df.apply(lambda r: get_nucleon_mass(r[\"Z2\"], r[\"A2\"]), axis=1).values\n",
    "mu_MeVc2 = (mass1 * mass2) / (mass1 + mass2 + 1e-12)\n",
    "Ecm = df[\"E c.m.\"].astype(float).values\n",
    "v_over_c = np.sqrt(np.clip(2 * Ecm / (mu_MeVc2 + epsilon), 0, np.inf))\n",
    "e2_hbar_c = 1 / 137.035999\n",
    "df[\"eta\"] = (df[\"Z1\"] * df[\"Z2\"]) / (e2_hbar_c * (v_over_c + 1e-16))\n",
    "\n",
    "log10_sigma_exp = np.log10(np.clip(df[\"œÉ\"], 1e-30, np.inf))\n",
    "log10_sigma_cal = np.log10(np.clip(df[\"œÉ cal\"], 1e-30, np.inf))\n",
    "log10_Ecm = np.log10(np.clip(df[\"E c.m.\"], 1e-30, np.inf))\n",
    "log10_exp_term = (2 * np.pi * df[\"eta\"]) / LN10\n",
    "\n",
    "df[\"log10_S_exp\"] = log10_sigma_exp + log10_Ecm + log10_exp_term\n",
    "df[\"log10_S_cal\"] = log10_sigma_cal + log10_Ecm + log10_exp_term\n",
    "df[\"delta_log10_S\"] = df[\"log10_S_exp\"] - df[\"log10_S_cal\"]\n",
    "\n",
    "df[\"N1\"] = df[\"A1\"] - df[\"Z1\"]\n",
    "df[\"N2\"] = df[\"A2\"] - df[\"Z2\"]\n",
    "df[\"Z1Z2_over_Ecm\"] = (df[\"Z1\"] * df[\"Z2\"]) / (df[\"E c.m.\"] + epsilon)\n",
    "\n",
    "MAGIC = np.array([2, 8, 20, 28, 50, 82, 126])\n",
    "def magic_dist(arr):\n",
    "    return np.min(np.abs(arr[:, None] - MAGIC[None, :]), axis=1)\n",
    "\n",
    "df[\"magic_dist_Z1\"] = magic_dist(df[\"Z1\"].values)\n",
    "df[\"magic_dist_N1\"] = magic_dist(df[\"N1\"].values)\n",
    "df[\"magic_dist_Z2\"] = magic_dist(df[\"Z2\"].values)\n",
    "df[\"magic_dist_N2\"] = magic_dist(df[\"N2\"].values)\n",
    "\n",
    "print(\"Feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adbf4f4c-93fd-459b-ba1c-9ee7dee23866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train reactions: 149\n",
      "Val reactions: 21\n",
      "Test reactions: 43\n",
      "Training samples: 2847\n",
      "Test samples: 685\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 29 Features --------------------\n",
    "features_train = [\n",
    "    'E c.m.', 'Z1', 'N1', 'A1',\n",
    "    'Z2', 'N2', 'A2', 'Q ( 2 n )',\n",
    "    'Z1Z2_over_Ecm',\n",
    "    'magic_dist_Z1','magic_dist_N1','magic_dist_Z2','magic_dist_N2',\n",
    "    'Z3','N3','A3','Œ≤ P','Œ≤ T','R B','ƒß œâ',\n",
    "    'Projectile_Mass_Actual', 'Target_Mass_Actual', 'Compound_Nucleus_Mass_Actual',\n",
    "    'Compound_Nucleus_Sp','Compound_Nucleus_Sn',\n",
    "    'Projectile_Binding_Energy','Target_Binding_Energy',\n",
    "    'Compound_Nucleus_Binding_Energy','Compound_Nucleus_S2n'\n",
    "]\n",
    "\n",
    "# -------------------- Load Reaction Split --------------------\n",
    "BASE_DIR = \"mdn_70_10_20_optimized\"\n",
    "train_reacts = pd.read_csv(os.path.join(BASE_DIR, \"train_reactions.csv\"))[\"Reaction\"].values\n",
    "val_reacts   = pd.read_csv(os.path.join(BASE_DIR, \"val_reactions.csv\"))[\"Reaction\"].values\n",
    "test_reacts  = pd.read_csv(os.path.join(BASE_DIR, \"test_reactions.csv\"))[\"Reaction\"].values\n",
    "\n",
    "print(\"Train reactions:\", len(train_reacts))\n",
    "print(\"Val reactions:\", len(val_reacts))\n",
    "print(\"Test reactions:\", len(test_reacts))\n",
    "\n",
    "train_mask = df[\"Reaction\"].isin(train_reacts)\n",
    "val_mask   = df[\"Reaction\"].isin(val_reacts)\n",
    "test_mask  = df[\"Reaction\"].isin(test_reacts)\n",
    "\n",
    "# -------------------- Prepare Arrays --------------------\n",
    "X_train_full = df.loc[train_mask | val_mask, features_train].values.astype(np.float32)\n",
    "y_train_full = df.loc[train_mask | val_mask, \"delta_log10_S\"].values.astype(np.float32).reshape(-1,1)\n",
    "X_test = df.loc[test_mask, features_train].values.astype(np.float32)\n",
    "y_test = df.loc[test_mask, \"delta_log10_S\"].values.astype(np.float32).reshape(-1,1)\n",
    "\n",
    "# -------------------- Standardize --------------------\n",
    "scaler = StandardScaler().fit(X_train_full)\n",
    "X_train_full_s = scaler.transform(X_train_full)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training samples:\", X_train_full_s.shape[0])\n",
    "print(\"Test samples:\", X_test_s.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad9233d-b15d-4573-ac80-1454656bc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Quantum Node and QMDN Model (Batched)\n",
    "# =========================================================\n",
    "\n",
    "# QNode definition (uses global n_qubits and dev)\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "def qnode(weights, inputs):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation=\"X\")\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return qml.probs(wires=range(n_qubits))   # shape (batch, 2**n_qubits)\n",
    "\n",
    "class QMDN(nn.Module):\n",
    "    def __init__(self, in_dim, n_components=N_COMPONENTS, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "        self.encoder = nn.Linear(in_dim, n_qubits)\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits, 3))\n",
    "\n",
    "        # Quantum output dimension = 2**n_qubits\n",
    "        self.fc1 = nn.Linear(2**n_qubits, hidden_dim)\n",
    "        self.fc_pi = nn.Linear(hidden_dim, n_components)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, n_components)\n",
    "        self.fc_sigma = nn.Linear(hidden_dim, n_components)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x_enc = torch.tanh(self.encoder(x))               # (batch, n_qubits)\n",
    "        probs = qnode(self.weight, x_enc)                  # (batch, 2**n_qubits)\n",
    "\n",
    "        # --- FIX: cast probs to same dtype as x (float32) ---\n",
    "        probs = probs.to(x.dtype)\n",
    "\n",
    "        h = torch.relu(self.fc1(probs))\n",
    "        pi_logits = self.fc_pi(h)\n",
    "        mu = self.fc_mu(h)\n",
    "        sigma_raw = self.fc_sigma(h)\n",
    "\n",
    "        pi = F.softmax(pi_logits, dim=1)\n",
    "        sigma = F.softplus(sigma_raw) + 1e-6\n",
    "        return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c866fe1-91e9-4a76-9d94-90c022ac632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 41\n",
      "Val batches: 5\n",
      "Test batches: 11\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Pure MDN NLL --------------------\n",
    "def mdn_loss(pi, mu, sigma, y):\n",
    "    y = y.float()\n",
    "    yexp = y.repeat(1, mu.shape[1])                     # (batch, n_components)\n",
    "    log_gauss = (\n",
    "        -0.5 * ((yexp - mu) / sigma) ** 2\n",
    "        - torch.log(sigma)\n",
    "        - 0.5 * np.log(2 * np.pi)\n",
    "    )\n",
    "    log_mix = torch.logsumexp(torch.log(pi + 1e-12) + log_gauss, dim=1)\n",
    "    return -log_mix.mean()\n",
    "\n",
    "# -------------------- Entropy Regularized Loss --------------------\n",
    "def mdn_loss_entropy(pi, mu, sigma, y, lambda_entropy):\n",
    "    nll = mdn_loss(pi, mu, sigma, y)\n",
    "    entropy = -torch.sum(pi * torch.log(pi + 1e-12), dim=1).mean()\n",
    "    return nll - lambda_entropy * entropy\n",
    "\n",
    "# -------------------- DataLoader --------------------\n",
    "def make_loader(X, y, batch=BATCH_SIZE, shuffle=True):\n",
    "    return DataLoader(\n",
    "        TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
    "                      torch.tensor(y, dtype=torch.float32)),\n",
    "        batch_size=batch,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if DEVICE.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "# -------------------- Train/Validation Split --------------------\n",
    "val_size = int(0.1 * len(X_train_full_s))\n",
    "indices = np.random.permutation(len(X_train_full_s))\n",
    "train_idx = indices[val_size:]\n",
    "val_idx   = indices[:val_size]\n",
    "\n",
    "train_loader = make_loader(X_train_full_s[train_idx], y_train_full[train_idx])\n",
    "val_loader   = make_loader(X_train_full_s[val_idx],   y_train_full[val_idx], shuffle=False)\n",
    "test_loader  = make_loader(X_test_s, y_test, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Val batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c249cff5-a253-4370-a248-0527c59c375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 525739\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Instantiate Model --------------------\n",
    "model = QMDN(in_dim=X_train_full_s.shape[1]).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# # -------------------- Test Memory with One Batch --------------------\n",
    "# print(\"\\nTesting one batch...\")\n",
    "# try:\n",
    "#     xb, yb = next(iter(train_loader))\n",
    "#     xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#     with torch.no_grad():\n",
    "#         pi, mu, sigma = model(xb)\n",
    "#     mem_used = torch.cuda.memory_allocated()/1e9 if DEVICE.type == 'cuda' else 0\n",
    "#     print(f\"‚úÖ Batch size {BATCH_SIZE} fits! Memory allocated: {mem_used:.2f} GB\")\n",
    "#     del xb, yb, pi, mu, sigma\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "# except RuntimeError as e:\n",
    "#     if \"out of memory\" in str(e):\n",
    "#         print(f\"‚ùå Batch size {BATCH_SIZE} does NOT fit. Consider reducing BATCH_SIZE or using gradient accumulation.\")\n",
    "#         print(\"You can set PHYSICAL_BATCH=32 and ACCUM_STEPS=2 to simulate batch 64.\")\n",
    "#     else:\n",
    "#         raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ad921-bbac-453a-ba41-20ef44e92f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Training (with gradient accumulation & component tracking)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Training Loop with Per‚ÄëEpoch Component Monitoring (SAVED)\n",
    "# =========================================================\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_val_nll = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "# ---- Tracking lists ----\n",
    "epoch_pi_means = []      # store average œÄ per epoch\n",
    "epoch_val_nll = []        # store validation NLL\n",
    "\n",
    "print(\"\\nüöÄ Starting Training (with gradient accumulation & component tracking)\\n\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_nll = 0.0\n",
    "    optimizer.zero_grad()   # reset gradients at epoch start\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        pi, mu, sigma = model(xb)\n",
    "\n",
    "        if LAMBDA_ENTROPY > 0:\n",
    "            loss = mdn_loss_entropy(pi, mu, sigma, yb, LAMBDA_ENTROPY) / ACCUMULATION_STEPS\n",
    "        else:\n",
    "            loss = mdn_loss(pi, mu, sigma, yb) / ACCUMULATION_STEPS\n",
    "\n",
    "        nll = mdn_loss(pi, mu, sigma, yb) / ACCUMULATION_STEPS   # scaled for logging\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights every ACCUMULATION_STEPS batches\n",
    "        if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0) * ACCUMULATION_STEPS   # rescale\n",
    "        train_nll  += nll.item()  * xb.size(0) * ACCUMULATION_STEPS\n",
    "\n",
    "    # Handle any remaining batches (if total batches not divisible by ACCUMULATION_STEPS)\n",
    "    if (len(train_loader) % ACCUMULATION_STEPS) != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_nll  /= len(train_loader.dataset)\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_nll = 0.0\n",
    "    pi_accum = []   # collect pi for this epoch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pi, mu, sigma = model(xb)\n",
    "\n",
    "            if LAMBDA_ENTROPY > 0:\n",
    "                vloss = mdn_loss_entropy(pi, mu, sigma, yb, LAMBDA_ENTROPY)\n",
    "            else:\n",
    "                vloss = mdn_loss(pi, mu, sigma, yb)\n",
    "\n",
    "            val_loss += vloss.item() * xb.size(0)\n",
    "            val_nll  += mdn_loss(pi, mu, sigma, yb).item() * xb.size(0)\n",
    "            pi_accum.append(pi.cpu())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_nll  /= len(val_loader.dataset)\n",
    "\n",
    "    # Average œÄ across validation set\n",
    "    pi_avg = torch.cat(pi_accum, dim=0).mean(dim=0).numpy()\n",
    "    epoch_pi_means.append(pi_avg)\n",
    "    epoch_val_nll.append(val_nll)\n",
    "\n",
    "    print(f\"E{epoch:03d} | train L={train_loss:.6f} NLL={train_nll:.6f} | \"\n",
    "          f\"val L={val_loss:.6f} NLL={val_nll:.6f} | \"\n",
    "          f\"œÄ avg: {np.round(pi_avg,3)} | \"\n",
    "          f\"œÄ std: {np.round(pi_avg.std(),3)}\")\n",
    "\n",
    "    # ---------- Early Stopping ----------\n",
    "    if val_nll < best_val_nll - 1e-4:\n",
    "        best_val_nll = val_nll\n",
    "        torch.save(model.state_dict(), \"qmdn_14q_12l_best.pth\")\n",
    "        patience_counter = 0\n",
    "        print(f\"  *** New best model (val NLL: {best_val_nll:.6f}) ***\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"üõë Early stopping after epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úÖ Training finished. Best val NLL:\", best_val_nll)\n",
    "\n",
    "# ---- SAVE TRACKING DATA FOR LATER ANALYSIS ----\n",
    "np.save(\"epoch_pi_means.npy\", np.array(epoch_pi_means))\n",
    "np.save(\"epoch_val_nll.npy\", np.array(epoch_val_nll))\n",
    "print(\"üìÅ Saved per‚Äëepoch component means and validation loss to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19519bb1-4db7-497e-9aa8-88274b13feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# After Training: Plot Component Evolution\n",
    "# =========================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert tracking lists to arrays\n",
    "pi_history = np.array(epoch_pi_means)\n",
    "n_components = pi_history.shape[1]\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# ---- Component evolution plot ----\n",
    "plt.subplot(1,2,1)\n",
    "for k in range(n_components):\n",
    "    plt.plot(pi_history[:, k], label=f'Component {k}', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average œÄ weight')\n",
    "plt.title('Component Evolution During Training')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# ---- Validation loss plot ----\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch_val_nll, 'r-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation NLL')\n",
    "plt.title('Validation Loss')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print final component distribution\n",
    "print(\"Final epoch average œÄ:\", np.round(pi_history[-1], 4))\n",
    "print(\"Final epoch component std:\", np.round(pi_history[-1].std(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395763d-f8d9-40b6-a7be-533c0f9c6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Load and Plot Saved Training History (Post‚ÄëTraining)\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved arrays\n",
    "pi_history = np.load(\"epoch_pi_means.npy\")\n",
    "val_nll_history = np.load(\"epoch_val_nll.npy\")\n",
    "\n",
    "print(\"Loaded pi_history shape:\", pi_history.shape)\n",
    "print(\"Loaded val_nll_history length:\", len(val_nll_history))\n",
    "\n",
    "n_components = pi_history.shape[1]\n",
    "\n",
    "# ---- Recreate the plots ----\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Component evolution\n",
    "plt.subplot(1,2,1)\n",
    "for k in range(n_components):\n",
    "    plt.plot(pi_history[:, k], label=f'Component {k}', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average œÄ weight')\n",
    "plt.title('Component Evolution During Training')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_nll_history, 'r-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation NLL')\n",
    "plt.title('Validation Loss')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: print final stats\n",
    "print(\"Final epoch average œÄ:\", np.round(pi_history[-1], 4))\n",
    "print(\"Final epoch component std:\", np.round(pi_history[-1].std(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a30bd5-9aee-4ec6-8b62-719e712eae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Load Best Model --------------------\n",
    "model.load_state_dict(torch.load(\"qmdn_14q_12l_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# -------------------- Predict Œîlog10S --------------------\n",
    "all_delta_pred = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        pi, mu, sigma = model(xb)\n",
    "        delta_pred = torch.sum(pi * mu, dim=1, keepdim=True)   # mixture mean\n",
    "        all_delta_pred.append(delta_pred.cpu().numpy())\n",
    "\n",
    "delta_pred = np.vstack(all_delta_pred).flatten()\n",
    "delta_true = y_test.flatten()\n",
    "\n",
    "print(\"Test Œîlog10S RMSE:\", np.sqrt(mean_squared_error(delta_true, delta_pred)))\n",
    "print(\"Test Œîlog10S R¬≤:\", r2_score(delta_true, delta_pred))\n",
    "\n",
    "# -------------------- Reconstruct œÉ --------------------\n",
    "test_local = df.loc[test_mask].reset_index(drop=True)\n",
    "eta_test = test_local[\"eta\"].values\n",
    "log10_S_cal_test = test_local[\"log10_S_cal\"].values\n",
    "\n",
    "log10_S_pred = log10_S_cal_test + delta_pred\n",
    "log10_sigma_pred = (\n",
    "    log10_S_pred\n",
    "    - np.log10(np.clip(test_local[\"E c.m.\"].values, 1e-30, np.inf))\n",
    "    - (2*np.pi*eta_test)/np.log(10.0)\n",
    ")\n",
    "sigma_pred = 10**log10_sigma_pred\n",
    "sigma_true = test_local[\"œÉ\"].values\n",
    "\n",
    "print(\"Test logœÉ R¬≤:\", r2_score(np.log10(sigma_true+1e-30), log10_sigma_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2c44a-25d0-4e4b-acf3-76b24d518bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Collect Component Weights for All Samples --------------------\n",
    "pi_all = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        pi, _, _ = model(xb)\n",
    "        pi_all.append(pi.cpu().numpy())\n",
    "    for xb, _ in val_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        pi, _, _ = model(xb)\n",
    "        pi_all.append(pi.cpu().numpy())\n",
    "    for xb, _ in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        pi, _, _ = model(xb)\n",
    "        pi_all.append(pi.cpu().numpy())\n",
    "\n",
    "pi_all = np.vstack(pi_all)\n",
    "print(\"œÄ array shape:\", pi_all.shape)\n",
    "print(\"Mean œÄ:\", pi_all.mean(axis=0))\n",
    "print(\"Standard deviation of œÄ:\", pi_all.std(axis=0))\n",
    "\n",
    "# -------------------- Dominant Component --------------------\n",
    "dominant = np.argmax(pi_all, axis=1)\n",
    "unique, counts = np.unique(dominant, return_counts=True)\n",
    "print(\"\\nDominant component counts:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Component {u}: {c} ({c/len(dominant)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c7b8c-b2d8-484e-9c6b-5e7b986bdafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a reaction (e.g., the one you used before)\n",
    "reaction_name = \"12 C + 194 Pt\"   # change as needed\n",
    "\n",
    "reaction_mask = df[\"Reaction\"] == reaction_name\n",
    "reaction_rows = df.loc[reaction_mask].sort_values(\"E c.m.\")\n",
    "if len(reaction_rows) == 0:\n",
    "    print(\"Reaction not found.\")\n",
    "else:\n",
    "    # Get indices in the full dataset\n",
    "    full_indices = reaction_rows.index\n",
    "    # Get the corresponding œÄ values (if you saved them per sample)\n",
    "    # Here we need to map full_indices to positions in pi_all.\n",
    "    # This mapping depends on how you constructed pi_all (concatenation order).\n",
    "    # If you want to be precise, you should extract œÄ during inference for this reaction only.\n",
    "    # Simpler: run inference again for this reaction.\n",
    "    x_reaction = scaler.transform(reaction_rows[features_train].values.astype(np.float32))\n",
    "    x_tensor = torch.tensor(x_reaction, dtype=torch.float32).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pi_reaction, _, _ = model(x_tensor)\n",
    "    pi_reaction = pi_reaction.cpu().numpy()\n",
    "    E_vals = reaction_rows[\"E c.m.\"].values\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for k in range(N_COMPONENTS):\n",
    "        plt.plot(E_vals, pi_reaction[:, k], marker='o', label=f\"Regime {k}\")\n",
    "    plt.xlabel(\"E c.m. (MeV)\")\n",
    "    plt.ylabel(\"Component weight œÄ\")\n",
    "    plt.title(f\"Regime evolution: {reaction_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
